Project Root: /Users/aaron/Desktop/app/MGMT
Version: V1.6.0
Generated at: 2025-12-10 11:31:07
==================================================


==================== START SPECIAL FILE: ui/assets/patch_notes.txt ====================
VERSION=V1.6.0

[V1.0.0] 2025-08-10
å¥ åŸºç‰ˆæœ¬å‘å¸ƒ
- æ ¸å¿ƒå·¥å…·é›†ï¼šå‘å¸ƒäº†åŸºç¡€ Python è„šæœ¬å·¥å…·åŒ…ã€‚
- åŸºç¡€åŠŸèƒ½ï¼šæ”¯æŒ SKU é”€é‡ç»Ÿè®¡ã€åº“å­˜ä¸Šä¼ ä»¥åŠåŸºç¡€äº¤æ˜“æ•°æ®çš„æ¸…æ´—ä¸å¤„ç†ã€‚

[V1.1.0] 2025-09-28
è´¢åŠ¡æ¨¡å—é›†æˆ
- å¤šæºæ•°æ®é›†æˆï¼šé›†æˆäº† Order Earning èµ„é‡‘æµæ•°æ®æºï¼Œå®ç°äº†ä¸šè´¢ä¸€ä½“åŒ–ã€‚
- é€»è¾‘ä¼˜åŒ–ï¼šåœ¨äº¤æ˜“å¤„ç†é€»è¾‘ä¸­å¼•å…¥äº†åŸºäºå“ˆå¸Œ (Hash) çš„å»é‡ç®—æ³•ï¼Œæå‡äº†æ•°æ®å¤„ç†çš„å‡†ç¡®æ€§ã€‚
- æŠ¥è¡¨ç³»ç»Ÿï¼šå‘å¸ƒäº†å¤šç»´åº¦çš„ç›ˆäºåˆ†ææŠ¥è¡¨ä½“ç³»ã€‚

[V1.2.0] 2025-11-29
æ™ºèƒ½åŒ–è¿ç»´æ¨¡å—
- æœºå™¨å­¦ä¹ é›†æˆï¼šé›†æˆäº† K-Means èšç±»å’Œ PCA é™ç»´ç®—æ³•ï¼Œå®ç°äº† SKU çš„è‡ªåŠ¨åŒ–æ‰“æ ‡ä¸åˆ†å±‚ã€‚
- é«˜çº§åˆ†æå·¥å…·ï¼šæ–°å¢äº† Listing æµé‡è¯Šæ–­å·¥å…·ä¸ Combo ç»„åˆç­–ç•¥è¯„ä¼°æ¨¡å‹ã€‚

[V1.3.0] 2025-12-02
Web åº”ç”¨åŒ–è½¬å‹
- UI/UX å…¨é¢å‡çº§ï¼šä»å‘½ä»¤è¡Œè„šæœ¬è¿ç§»è‡³ Streamlit Web æ¡†æ¶ï¼Œå®ç°äº†äº¤äº’å¼å¯è§†åŒ–æ“ä½œç•Œé¢ã€‚
- CRM å®¢æˆ·ç®¡ç†æ¨¡å—ï¼šå¼•å…¥äº†åŸºäº RFM (æœ€è¿‘ä¸€æ¬¡æ¶ˆè´¹ã€æ¶ˆè´¹é¢‘ç‡ã€æ¶ˆè´¹é‡‘é¢) æ¨¡å‹çš„å®¢æˆ·ä»·å€¼åˆ†æåŠŸèƒ½ï¼Œæ”¯æŒç²¾å‡†ç”¨æˆ·ç”»åƒã€‚

[V1.4.0] 2025-12-04
ä¼ä¸šçº§æ¶æ„é‡æ„
- è‡ªåŠ¨åŒ– ETL æµæ°´çº¿ï¼šå‘å¸ƒäº†è¦†ç›–æ•°æ®åŠ è½½ (Ingest)ã€è§£æ (Parsing)ã€è½¬æ¢ (Transformation) å’Œå½’æ¡£ (Archiving) çš„å…¨ç”Ÿå‘½å‘¨æœŸè‡ªåŠ¨åŒ–å¤„ç†æµç¨‹ã€‚
- ç¾éš¾æ¢å¤ç³»ç»Ÿï¼šå¼•å…¥äº†å…·å¤‡æ—¶é—´ç‚¹æ¢å¤èƒ½åŠ›çš„æ•°æ®åº“å¤‡ä»½ä¸è¿˜åŸä¸­å¿ƒ (Backup & Restore Center)ã€‚
- åŸºç¡€è®¾æ–½å‡çº§ï¼šå®ç°äº†é…ç½®ä¸ä»£ç çš„å®Œå…¨åˆ†ç¦» (Configuration Decoupling)ï¼›å‰ç«¯æ¶æ„å…¨é¢è¿ç§»è‡³ç»„ä»¶åŒ– UI æ¨¡å¼ (Component-Based UI)ã€‚

[V1.4.1] 2025-12-05
æ ¸å¿ƒæ•°æ®å®Œæ•´æ€§ä¸ç®—æ³•çƒ­ä¿®å¤
- ETL æ™ºèƒ½å»é‡ (Smart Upsert)ï¼šå®æ–½äº†åŸºäºæ—¶é—´çª—å£å‰ªæå’Œå››ç»´å¤åˆä¸»é”® (Seller + Order + Item + Action) åŒ¹é…çš„åˆ†åŒºå»é‡ç­–ç•¥ï¼Œå®ç°äº†æ¯«ç§’çº§ç²¾å‡†è¦†ç›–ï¼Œå½»åº•æ¶ˆé™¤äº†è¯¯åˆ å†å²æ•°æ®çš„é£é™©ã€‚
- å…¨é“¾è·¯å½’ä¸€åŒ– (Global Normalization)ï¼šåœ¨æ‰€æœ‰åˆ†æå™¨ (Sales, Profit, Ordering, Prediction) ä¸­å¼ºåˆ¶å®æ–½ä¸¥æ ¼çš„ SKU å½’ä¸€åŒ–æ ‡å‡†ï¼ˆå¤§å†™/å»ç©ºæ ¼ï¼‰ï¼Œè§£å†³äº†æŠ¥è¡¨é¦–åˆ—é‡å¤å’Œç»Ÿè®¡åå·®é—®é¢˜ã€‚
- æ•°æ®æµåŠ å›º (Data Pipeline Hardening)ï¼šä¿®å¤äº† Repository åˆ° Analyzer ä¼ è¾“è¿‡ç¨‹ä¸­çš„é™é»˜æ•°æ®ä¸¢å¤±é—®é¢˜ï¼›å¢å¼ºäº†æ•°æ®ä»“åº“ä¸­æ—¥æœŸæ ¼å¼è§£æçš„å®¹é”™èƒ½åŠ›ã€‚
- Bug ä¿®å¤ï¼šä¿®æ­£äº†é”€é‡ç»Ÿè®¡åˆ†æä¸­åº—é“ºåˆ†æ¡¶ç»Ÿè®¡ä¸¢å¤±çš„é—®é¢˜ï¼Œå¹¶è¡¥å…¨äº†æ™ºèƒ½è¡¥è´§è®¡åˆ’ä¸­çš„çŠ¶æ€å¤‡æ³¨å­—æ®µã€‚

[V1.5.0] 2025-12-05
ä¼ä¸šçº§å®‰å…¨æ¶æ„ä¸æ€§èƒ½é©å‘½
- ä¼ä¸šçº§å®‰å…¨æ¶æ„ (Enterprise Security Architecture)ï¼šå®æ–½äº† PBKDF2-SHA256 å¯†ç åŠ å¯†å­˜å‚¨ä¸é˜²æš´åŠ›ç ´è§£ç­–ç•¥ï¼›æ•æ„Ÿæ“ä½œå¼•å…¥â€œæ“ä½œå¯†ç +ç¡®è®¤çŸ­è¯­â€åŒé‡æ ¡éªŒæœºåˆ¶ï¼Œå…¨é¢æå‡ç³»ç»Ÿå®‰å…¨æ€§ã€‚
- é«˜æ€§èƒ½ ETL å¼•æ“ (High-Performance ETL Engine)ï¼šæ ¸å¿ƒè½¬æ¢é€»è¾‘å‡çº§ä¸º Pandas å‘é‡åŒ–çŸ©é˜µè¿ç®—ï¼Œæ‘’å¼ƒä½æ•ˆå¾ªç¯ï¼Œå¤„ç†é€Ÿåº¦æå‡ 20-50 å€ï¼›å¼•å…¥ Staging Table åŸå­å†™å…¥ç­–ç•¥ï¼Œç¡®ä¿å¤šåº—é“ºæ•°æ®å¹¶å‘ä¸Šä¼ æ—¶çš„ç»å¯¹ä¸€è‡´æ€§ã€‚
- æ·±åº¦å®¡è®¡ç³»ç»Ÿ (Deep Audit System)ï¼šå‘å¸ƒäº†æ˜¾å¾®é•œçº§ä¸šåŠ¡å®¡è®¡æ—¥å¿—ï¼Œæ”¯æŒæ•°æ®å˜æ›´çš„â€œä¿®æ”¹å‰ vs ä¿®æ”¹åâ€é€å­—æ®µæ¯”å¯¹ï¼Œå®ç°å…¨é“¾è·¯æ“ä½œå¯è¿½æº¯ã€‚
- ç”¨æˆ·æƒé™ä¸è¿ç»´å¢å¼º (User Access & Ops Enhancement)ï¼šæ„å»ºäº†åŸºäº RBAC çš„ç»†ç²’åº¦æƒé™æ§åˆ¶ä½“ç³»ï¼Œæ”¯æŒé’ˆå¯¹ ETLã€æŠ¥è¡¨ç­‰æ¨¡å—çš„ç‹¬ç«‹é‰´æƒï¼›æ–°å¢æ•°æ®åº“è¿ç»´â€œå±é™©åŒºåŸŸâ€ï¼Œæ”¯æŒä¸€é”®é‡ç½®æµ‹è¯•ç¯å¢ƒï¼ˆä»…æ¸…ç©ºæµæ°´ï¼Œä¿ç•™ç”¨æˆ·ä¸åŸºç¡€æ¡£æ¡ˆï¼‰ã€‚

[V1.6.0] 2025-12-10
åˆ†å¸ƒå¼å¹¶å‘æ¶æ„ä¸å®‰å…¨æƒé™é‡å¡‘ (Distributed Concurrency & Security Governance)
ç”¨æˆ·æƒé™æ¨¡å‹é‡è®¾è®¡ (RBAC Redesign)ï¼š
- åœ¨åŸæœ‰çš„ç²—ç²’åº¦ RBAC åŸºç¡€ä¸Šï¼Œå…¨é¢å‡çº§äº†å¯¹ æ¨¡å—ã€Tab é¡µé¢åŠæ“ä½œåŠ¨ä½œ çš„ä¸‰çº§æˆæƒè®¾è®¡ã€‚
- è¿™ä¸€è®¾è®¡å…è®¸ç®¡ç†å‘˜ç²¾ç¡®é…ç½®ç”¨æˆ·å¯¹æ ¸å¿ƒåŠŸèƒ½å¦‚ ETLã€æŠ¥è¡¨ç­‰æ¨¡å—çš„è®¿é—®æƒé™ï¼Œå®ç°äº†çœŸæ­£çš„ æœ€å°æƒé™åŸåˆ™ã€‚
åˆ†å¸ƒå¼å¹¶å‘æ§åˆ¶æ¶æ„ (Distributed Concurrency Control)ï¼š
- æ¶æ„æ ¸å¿ƒå¼•å…¥äº†åŸºäºæ•°æ®åº“æŒä¹…åŒ–çš„ åˆ†å¸ƒå¼é”ç®¡ç†å™¨ (LockManager) ã€‚
- å¯¹ ETL äº¤æ˜“æ•°æ®å¯¼å…¥ã€åº“å­˜åŒæ­¥ å’Œ SKU æ¡£æ¡ˆç»´æŠ¤ ç­‰æ ¸å¿ƒå†™å…¥èµ„æºå®æ–½äº†ä¸¥æ ¼çš„äº’æ–¥é”æœºåˆ¶ ã€‚
ä¼šè¯çº§å®‰å…¨å›æ»šæœºåˆ¶ (Session-Level Safety Rollback)ï¼š
- ç³»ç»Ÿç°åœ¨æ·±åº¦ç»‘å®šäº†ä¼šè¯ç”Ÿå‘½å‘¨æœŸä¸æ•°æ®å®Œæ•´æ€§ ã€‚
- ä¸€æ—¦æ£€æµ‹åˆ°ç”¨æˆ·åœ¨ ETL æµç¨‹ä¸­é€”å¼‚å¸¸é€€å‡ºæˆ–å¼ºåˆ¶ç™»å‡ºï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æ£€æŸ¥å¹¶é‡Šæ”¾å…¶æŒæœ‰çš„é”ï¼Œå¹¶ç«‹å³è§¦å‘ Data_Transaction è„æ•°æ®å›æ»š (Auto-Rollback) ç¨‹åºï¼Œæ¸…ç©ºåŸå§‹è¡¨ä¸­çš„æœªæäº¤æ•°æ®ï¼Œé˜²æ­¢æ•°æ®æ±¡æŸ“ ã€‚
æ—¥å¿—ç³»ç»Ÿæ·±åº¦å¼ºåŒ–ï¼ˆDeep Audit System Updateï¼‰ï¼š
- æ—¥å¿—è®°å½•å·²å…¨é¢ä¼˜åŒ–ï¼Œç¡®ä¿æ¯ä¸ªæ“ä½œæ—¥å¿—å‡åŒ…å«å®Œæ•´çš„ æ“ä½œäººã€æ“ä½œåŠ¨ä½œã€ç›®æ ‡è¡¨åä»¥åŠæ¥æº IP ç­‰æº¯æºä¿¡æ¯ ã€‚
- å®¡è®¡æ—¥å¿—ç°åœ¨å…·å¤‡æ›´å¼ºçš„é²æ£’æ€§ï¼Œå³ä½¿åœ¨å¤šçº¿ç¨‹/å¤šè¿›ç¨‹ç¯å¢ƒä¸‹ï¼Œä¹Ÿèƒ½é€šè¿‡ ContextFilter å‡†ç¡®æ•è·å½“å‰æ“ä½œç”¨æˆ·çš„ä¼šè¯ä¿¡æ¯ ã€‚

==================== END SPECIAL FILE: ui/assets/patch_notes.txt ====================


==================== START FILE: stop_server.sh ====================
#!/bin/bash

# ========================================================
# Eaglestar ERP - macOS å…³é—­è„šæœ¬ (å…¨æ ˆæ¸…ç†)
# ========================================================

BASE_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
cd "$BASE_DIR"

SERVER_PID_FILE="$BASE_DIR/server.pid"
TUNNEL_PID_FILE="$BASE_DIR/tunnel.pid"

echo "ğŸ›‘ æ­£åœ¨åœæ­¢æœåŠ¡..."

# 1. å…³é—­ Streamlit ä¸»ç¨‹åº
if [ -f "$SERVER_PID_FILE" ]; then
    PID=$(cat "$SERVER_PID_FILE")
    if ps -p $PID > /dev/null; then
        kill $PID
        echo " å·²ç»ˆæ­¢ ERP ä¸»è¿›ç¨‹ (PID: $PID)"
        rm "$SERVER_PID_FILE"
    else
        echo "âš ï¸  ERP è¿›ç¨‹å·²ä¸å­˜åœ¨ï¼Œæ¸…ç† PID æ–‡ä»¶ã€‚"
        rm "$SERVER_PID_FILE"
    fi
else
    echo "ï¸  æœªæ‰¾åˆ° ERP PID æ–‡ä»¶ã€‚"
fi

# 2. å…³é—­ Cloudflare Tunnel
if [ -f "$TUNNEL_PID_FILE" ]; then
    PID=$(cat "$TUNNEL_PID_FILE")
    if ps -p $PID > /dev/null; then
        kill $PID
        echo " å·²ç»ˆæ­¢ Cloudflare Tunnel (PID: $PID)"
        rm "$TUNNEL_PID_FILE"
    else
        echo "âš ï¸  Tunnel è¿›ç¨‹å·²ä¸å­˜åœ¨ï¼Œæ¸…ç† PID æ–‡ä»¶ã€‚"
        rm "$TUNNEL_PID_FILE"
    fi
else
    echo "ï¸  æœªæ‰¾åˆ° Tunnel PID æ–‡ä»¶ã€‚"
fi

# 3. å…œåº•æ¸…ç† (é˜²æ­¢è¿›ç¨‹æ®‹ç•™)
# æ¸…ç† Streamlit
LEFTOVER_ST=$(pgrep -f "streamlit run app.py")
if [ -n "$LEFTOVER_ST" ]; then
    echo "ğŸ§¹ æ¸…ç†æ®‹ç•™çš„ Streamlit è¿›ç¨‹..."
    echo "$LEFTOVER_ST" | xargs kill
fi

# æ¸…ç† Cloudflare (åªæ¸…ç†å¸¦ token å‚æ•°çš„è¿›ç¨‹ï¼Œé˜²æ­¢è¯¯æ€å…¶ä»– tunnel)
LEFTOVER_CF=$(pgrep -f "cloudflared tunnel run --token")
if [ -n "$LEFTOVER_CF" ]; then
    echo "ğŸ§¹ æ¸…ç†æ®‹ç•™çš„ Cloudflare è¿›ç¨‹..."
    echo "$LEFTOVER_CF" | xargs kill
fi

echo "ğŸ˜´ é˜²ä¼‘çœ æ¨¡å¼å·²è§£é™¤ï¼ŒæœåŠ¡å·²å…¨éƒ¨å…³é—­ã€‚"
==================== END FILE: stop_server.sh ====================


==================== START FILE: verify_infra.py ====================
# verify_inventory.py
from core.services.inventory.service import InventoryService
from core.sys.context import set_context

# 1. æ¨¡æ‹Ÿç™»å½•
set_context(username="InvTester")

print("ğŸš€ æµ‹è¯•åº“å­˜æœåŠ¡...")
svc = InventoryService()

# 2. æµ‹è¯•åŠ¨æ€ DDL (Dry Run)
# è¿™ä¸€æ­¥ä¼šæ£€æŸ¥æ•°æ®åº“é‡Œæœ‰æ²¡æœ‰ 'TEST_COL_2099'ï¼Œæ²¡æœ‰çš„è¯ä¼šè‡ªåŠ¨æ·»åŠ 
# è¿è¡Œåä½ å¯ä»¥å»æ•°æ®åº“æ£€æŸ¥ï¼Œåº”è¯¥å¤šäº†ä¸€åˆ—
print("ğŸ”§ æµ‹è¯•åŠ¨æ€è¡¨ç»“æ„æ‰©å±• (DDL)...")
try:
    svc._ensure_column_exists("TEST_COL_2099")
    print(" åˆ—æ£€æŸ¥/åˆ›å»ºæˆåŠŸ (è¯·æ£€æŸ¥ DB æ˜¯å¦å­˜åœ¨ TEST_COL_2099)")
except Exception as e:
    print(f" DDL å¤±è´¥: {e}")

print("ğŸ‰ åº“å­˜æ¨¡å—éªŒè¯å®Œæ¯•ã€‚")
==================== END FILE: verify_infra.py ====================


==================== START FILE: start_server.sh ====================
#!/bin/bash

# ========================================================
# Eaglestar ERP - macOS å¯åŠ¨è„šæœ¬ (å« Cloudflare Tunnel)
# ========================================================

# 1. ç¡®ä¿è¿›å…¥è„šæœ¬æ‰€åœ¨çš„ç›®å½• (é¡¹ç›®æ ¹ç›®å½•)
BASE_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
cd "$BASE_DIR"

# --- é…ç½®åŒºåŸŸ ---
SERVER_PID_FILE="$BASE_DIR/server.pid"
TUNNEL_PID_FILE="$BASE_DIR/tunnel.pid"
LOG_DIR="$BASE_DIR/logs"
SERVER_LOG="$LOG_DIR/server_runtime.log"
TUNNEL_LOG="$LOG_DIR/cloudflared.log"

# Cloudflare Token
CF_TOKEN="YOUR_CF_TOKEN_HERE"

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
mkdir -p "$LOG_DIR"

# 2. ç¯å¢ƒæ£€æŸ¥ä¸æ¸…ç†
if [ -f "$SERVER_PID_FILE" ]; then
    PID=$(cat "$SERVER_PID_FILE")
    if ps -p $PID > /dev/null; then
        echo " æœåŠ¡å™¨å·²ç»åœ¨è¿è¡Œä¸­ (PID: $PID)"
        exit 1
    else
        rm "$SERVER_PID_FILE"
    fi
fi

# 3. æ¿€æ´» Python ç¯å¢ƒ
echo "ğŸš€ æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒ..."
if [ -d ".venv" ]; then
    echo "ğŸŸ¢ æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (.venv)..."
    source .venv/bin/activate
elif [ -d "venv" ]; then
    echo "ğŸŸ¢ æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (venv)..."
    source venv/bin/activate
fi

# 4. å¯åŠ¨ Cloudflare Tunnel (åå°è¿è¡Œ)
echo "ğŸŒ æ­£åœ¨å¯åŠ¨ Cloudflare Tunnel..."
# ä½¿ç”¨ nohup åå°è¿è¡Œï¼Œæ—¥å¿—è¾“å‡ºåˆ° logs/cloudflared.log
nohup cloudflared tunnel run --token $CF_TOKEN > "$TUNNEL_LOG" 2>&1 &
TUNNEL_PID=$!
echo $TUNNEL_PID > "$TUNNEL_PID_FILE"
echo "   -> Tunnel PID: $TUNNEL_PID"

# 5. å¯åŠ¨ Streamlit æœåŠ¡å™¨ (æ ¸å¿ƒé˜²ä¼‘çœ )
# caffeinate -ims: åªè¦è¿™ä¸ªå‘½ä»¤åœ¨è¿è¡Œï¼Œç”µè„‘å°±ä¸ä¼šä¼‘çœ 
echo "ğŸ›¡ï¸  æ­£åœ¨å¯åŠ¨ Streamlit æœåŠ¡å™¨ (å·²å¯ç”¨ macOS é˜²ä¼‘çœ å®ˆæŠ¤)..."
nohup caffeinate -ims streamlit run app.py --server.port 8501 > "$SERVER_LOG" 2>&1 &
SERVER_PID=$!
echo $SERVER_PID > "$SERVER_PID_FILE"

# 6. è¾“å‡ºç»“æœ
echo "=================================================="
echo " æœåŠ¡å…¨æ ˆå¯åŠ¨æˆåŠŸï¼"
echo "   - ERP ä¸»ç¨‹åº PID: $SERVER_PID"
echo "   - Cloudflare PID: $TUNNEL_PID"
echo "--------------------------------------------------"
echo "ğŸ“„ ERP æ—¥å¿—: logs/server_runtime.log"
echo "ğŸ“„ CF  æ—¥å¿—: logs/cloudflared.log"
echo "ğŸ”’ ç”µè„‘å·²è¿›å…¥é˜²ä¼‘çœ æ¨¡å¼ï¼Œæ‚¨å¯ä»¥æ”¾å¿ƒåœ°é”å±ã€‚"
echo "=================================================="
==================== END FILE: start_server.sh ====================


==================== START FILE: debug_config.py ====================
# debug_config.py
import json
import os
from pathlib import Path
from config.settings import settings

print("="*50)
print(f"ğŸ” è°ƒè¯•æ¨¡å¼: æ£€æŸ¥é…ç½®åŠ è½½")
print("="*50)

print(f"1. é¡¹ç›®æ ¹ç›®å½• (BASE_DIR): {settings.BASE_DIR}")
print(f"2. é…ç½®ç›®å½• (CONFIG_DIR): {settings.CONFIG_DIR}")

json_path = settings.CONFIG_DIR / "modules.json"
print(f"3. ç›®æ ‡é…ç½®æ–‡ä»¶: {json_path}")

if not json_path.exists():
    print(" é”™è¯¯: modules.json æ–‡ä»¶ä¸å­˜åœ¨ï¼")
else:
    print(" æ–‡ä»¶å­˜åœ¨ã€‚å°è¯•è¯»å–å†…å®¹...")
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            content = f.read()
            if not content.strip():
                print(" é”™è¯¯: æ–‡ä»¶æ˜¯ç©ºçš„ï¼")
            else:
                data = json.loads(content)
                print(f" JSON è§£ææˆåŠŸï¼å…±æ‰¾åˆ° {len(data)} ä¸ªæ¨¡å—é…ç½®ã€‚")
                print("-" * 20)
                for mod in data:
                    print(f"   - [{mod.get('key')}] {mod.get('name')} (Perm: {mod.get('permission')})")
    except json.JSONDecodeError as e:
        print(f" JSON æ ¼å¼é”™è¯¯: {e}")
    except Exception as e:
        print(f" è¯»å–é”™è¯¯: {e}")

print("="*50)
==================== END FILE: debug_config.py ====================


==================== START FILE: .env ====================
# .env

# ======================================================
# 1. æ•°æ®åº“é…ç½® (Database)
# ======================================================
DB_HOST=localhost
DB_PORT=3306
DB_USER=root
# DB_PASS=***REDACTED_PASSWORD***
DB_PASS=***REDACTED_PASSWORD***
DB_NAME=MGMT
DB_CHARSET=utf8mb4

# ======================================================
# 2. é»˜è®¤è´¦å·åˆå§‹åŒ– (Bootstrap)
# ======================================================
DEFAULT_ADMIN_USERNAME=admin
DEFAULT_ADMIN_PASSWORD=1522P

DEFAULT_USER_USERNAME=user
DEFAULT_USER_PASSWORD=6M130

# ======================================================
# 3. AI æœåŠ¡ (Google Gemini)
# ======================================================
GOOGLE_API_KEY=YOUR_GOOGLE_API_KEY_HERE

# ======================================================
# 4. ç³»ç»Ÿè®¾ç½® (System)
# ======================================================
AUTO_LOGOUT_SECONDS=600
LOG_LEVEL=INFO

# ======================================================
# 5. [Architecture V5] ä¼ä¸šçº§å®‰å…¨ä¸­å°é…ç½®
# ISO 27001 Access Control Level
# ======================================================

# 1. æ•°æ®æŸ¥è¯¢å®‰ä¿ç  (ä½å±: ç”¨äºæŸ¥çœ‹æ•æ„ŸæŠ¥è¡¨ç­‰)
SEC_CODE_QUERY=7951

# 2. æ•°æ®ä¿®æ”¹å®‰ä¿ç  (ä¸­å±: ç”¨äºå•ç‚¹ä¿®æ”¹æ•°æ®)
SEC_CODE_MODIFY=1522

# 3. æ•°æ®åº“ç®¡ç†ç  (é«˜å±: ç”¨äºå¤‡ä»½æ¢å¤ã€æ‰¹é‡æ›´æ–°)
# (åŸ DB_OPERATOR_PWD)
SEC_CODE_DB=6130

# 4. å¹³å°ç³»ç»Ÿç®¡ç†ç  (æ ¸å¼¹çº§: ç”¨äºåˆ åº“ã€æ—¥å¿—é”€æ¯ã€ç³»ç»Ÿé‡ç½®)
# (åŸ PLATFORM_SEC_PWD)
SEC_CODE_SYSTEM=***REDACTED_SYSTEM_CODE***
==================== END FILE: .env ====================


==================== START FILE: app.py ====================
# app.py
"""
æ–‡ä»¶è¯´æ˜: Streamlit åº”ç”¨å…¥å£ (Entry Point)
ä¸»è¦åŠŸèƒ½:
1. é…ç½® Streamlit é¡µé¢åŸºç¡€å±æ€§ (Title, Layout)ã€‚
2. åˆå§‹åŒ–è®¤è¯æœåŠ¡ (Bootstrap Database)ã€‚
3. [New] åˆå§‹åŒ–å¹¶å‘é”ç®¡ç†å™¨ã€‚
4. å®ä¾‹åŒ–å¹¶å¯åŠ¨åŠ¨æ€è·¯ç”±å¼•æ“ (Router)ã€‚
"""

import streamlit as st
from config.settings import settings
from ui.kernel.router import Router
from core.services.auth.service import AuthService
from core.sys.lock_manager import LockManager  # [New]

# ==============================================================================
# 1. å…¨å±€é…ç½® (å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤)
# ==============================================================================
st.set_page_config(
    page_title=settings.APP_NAME,
    page_icon="ğŸ¦…",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ==============================================================================
# 2. ç³»ç»Ÿåˆå§‹åŒ– (Bootstrap)
# ==============================================================================
# ç¡®ä¿æ•°æ®åº“è¡¨ç»“æ„å­˜åœ¨ï¼Œå¹¶åˆ›å»ºé»˜è®¤è´¦å·
AuthService.initialize()

# [New] åˆå§‹åŒ–åˆ†å¸ƒå¼é”è¡¨ç»“æ„
LockManager.initialize()


# ==============================================================================
# 3. å¯åŠ¨è·¯ç”±å¼•æ“
# ==============================================================================
def main():
    """ä¸»ç¨‹åºå…¥å£"""
    try:
        # å®ä¾‹åŒ–è·¯ç”±
        app_router = Router()

        # æ¸²æŸ“ç•Œé¢
        app_router.render()

    except Exception as e:
        st.error("ğŸ”¥ System Critical Error (Main Loop)")
        st.exception(e)


if __name__ == "__main__":
    main()
==================== END FILE: app.py ====================


==================== START FILE: verify_lock_system.py ====================
# verify_lock_system.py
"""
æ–‡ä»¶è¯´æ˜: åˆ†å¸ƒå¼é”ç³»ç»ŸéªŒè¯è„šæœ¬ (Verification Suite)
è¿è¡Œæ–¹å¼: python verify_lock_system.py
ä¸»è¦åŠŸèƒ½:
1. æ¨¡æ‹Ÿå¹¶å‘ç¯å¢ƒï¼ŒéªŒè¯ LockManager çš„åŸå­æ€§å’Œéš”ç¦»æ€§ã€‚
2. éªŒè¯ 'Data_Transaction', 'Data_Inventory' ç­‰å…³é”®èµ„æºçš„äº’æ–¥é€»è¾‘ã€‚
3. éªŒè¯ç”¨æˆ·ç™»å‡ºæ—¶çš„é”è‡ªåŠ¨å›æ”¶æœºåˆ¶ã€‚
"""

import time
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Path
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root))

from core.sys.lock_manager import LockManager
from core.components.db.client import DBClient


def print_step(msg):
    print(f"\nğŸ”¹ {msg}")


def print_res(success, msg):
    icon = "" if success else ""
    print(f"   {icon} {msg}")


def run_verification():
    print("ğŸš€ å¼€å§‹éªŒè¯åˆ†å¸ƒå¼é”ç³»ç»Ÿ (ISO/IEC 25010 Reliability Test)...")

    # 0. åˆå§‹åŒ–
    LockManager.initialize()
    RESOURCE = "Data_Test_Resource"
    USER_A = "Admin_Alice"
    USER_B = "User_Bob"

    # æ¸…ç†ç¯å¢ƒ
    DBClient.execute_stmt(f"DELETE FROM System_Locks WHERE resource_key='{RESOURCE}'")

    # --- Case 1: åŸºç¡€äº’æ–¥æµ‹è¯• ---
    print_step("Case 1: äº’æ–¥æ€§æµ‹è¯• (Mutually Exclusive)")

    # A è·å–é”
    ok, msg = LockManager.acquire_lock(RESOURCE, USER_A, "Testing Module")
    print_res(ok, f"User A å°è¯•åŠ é”: {msg}")
    if not ok:
        print(" ä¸¥é‡é”™è¯¯: åˆå§‹åŠ é”å¤±è´¥")
        return

    # B å°è¯•è·å–é” (åº”å¤±è´¥)
    ok, msg = LockManager.acquire_lock(RESOURCE, USER_B, "Testing Module")
    print_res(not ok, f"User B å°è¯•åŠ é” (é¢„æœŸå¤±è´¥): {msg}")

    # --- Case 2: é”çš„æ‰€æœ‰æƒä¸åˆ·æ–° ---
    print_step("Case 2: æ‰€æœ‰æƒéªŒè¯ (Re-entrancy)")

    # A å†æ¬¡è·å–é” (åº”æˆåŠŸï¼Œåˆ·æ–°æ—¶é—´)
    ok, msg = LockManager.acquire_lock(RESOURCE, USER_A, "Testing Module")
    print_res(ok, f"User A å†æ¬¡åŠ é” (åˆ·æ–°): {msg}")

    # --- Case 3: æ­£å¸¸é‡Šæ”¾æµç¨‹ ---
    print_step("Case 3: é‡Šæ”¾ä¸æŠ¢å  (Release & Acquire)")

    # A é‡Šæ”¾é”
    ok = LockManager.release_lock(RESOURCE, USER_A)
    print_res(ok, "User A é‡Šæ”¾é”")

    # B ç°åœ¨åº”è¯¥èƒ½è·å–é”
    ok, msg = LockManager.acquire_lock(RESOURCE, USER_B, "Testing Module")
    print_res(ok, f"User B å°è¯•åŠ é” (é¢„æœŸæˆåŠŸ): {msg}")

    # --- Case 4: å¼ºåˆ¶æ¸…ç† (Session Logout) ---
    print_step("Case 4: å¼‚å¸¸ä¸­æ–­ä¸å¼ºåˆ¶æ¸…ç† (Force Release)")

    # æ¨¡æ‹Ÿ B çªç„¶æ‰çº¿ (Logout)ï¼Œç³»ç»Ÿè°ƒç”¨ release_all
    released_list = LockManager.release_all_user_locks(USER_B)
    print_res(RESOURCE in released_list, f"ç³»ç»Ÿå¼ºåˆ¶é‡Šæ”¾ User B çš„é”: {released_list}")

    # A é‡æ–°è·å–
    ok, msg = LockManager.acquire_lock(RESOURCE, USER_A, "Testing Module")
    print_res(ok, f"User A é‡æ–°æ¥ç®¡èµ„æº: {msg}")

    # --- æ¸…ç† ---
    LockManager.release_lock(RESOURCE, USER_A)
    print("\nğŸ‰ éªŒè¯å®Œæˆ: é”ç®¡ç†å™¨é€»è¾‘è¿è¡Œæ­£å¸¸ã€‚")


if __name__ == "__main__":
    run_verification()
==================== END FILE: verify_lock_system.py ====================


==================== START FILE: ui/styles.py ====================
# ui/styles.py
"""
æ–‡ä»¶è¯´æ˜: å…¨å±€æ ·å¼ç®¡ç†å™¨ (Global Styles) - V3.0 Security UI
ä¸»è¦åŠŸèƒ½:
1. å¢å¼º Streamlit ç»„ä»¶äº¤äº’ (Hover æ•ˆæœ)ã€‚
2. [New] å®‰å…¨è­¦ç¤ºæ ·å¼: ä¸ºâ€œè§£é”å®Œæ•´ä¿¡æ¯â€æ¨¡å¼æä¾›è§†è§‰åé¦ˆã€‚
"""

import streamlit as st


class StyleManager:
    _MAIN_STYLES = """
    <style>
        /* 1. éšè—å¤šä½™å…ƒç´  */
        #MainMenu {visibility: hidden;} 
        footer {visibility: hidden;}

        /* 2. ä¾§è¾¹æ èœå• (Radio) */
        div[role="radiogroup"] label { 
            transition: all 0.2s cubic-bezier(0.175, 0.885, 0.32, 1.275) !important;
            border-radius: 8px;
            padding: 8px 12px !important; 
            margin-bottom: 4px; 
            border: 1px solid transparent;
            cursor: pointer;
        }
        div[role="radiogroup"] label:hover { 
            transform: scale(1.05) translateX(10px) !important;
            background: rgba(78, 201, 176, 0.2) !important;
            border: 1px solid rgba(78, 201, 176, 0.5) !important; 
            box-shadow: 0 10px 25px rgba(0,0,0,0.5) !important;
            z-index: 999 !important; 
            color: #FFFFFF !important;
            font-weight: bold !important; 
        }
        div[role="radiogroup"] [aria-checked="true"] {
            color: #4EC9B0 !important;
            font-weight: 800 !important;
        }

        /* 3. Tab æ ‡ç­¾æ  */
        button[data-baseweb="tab"] { 
            transition: all 0.2s ease-in-out !important;
            border-radius: 4px 4px 0 0 !important;
            margin: 0 4px !important; 
            border: none !important;
            border-bottom: 2px solid transparent !important;
        }
        button[data-baseweb="tab"]:hover { 
            background-color: rgba(255, 255, 255, 0.15) !important; 
            border-bottom: 3px solid #4EC9B0 !important;
            color: #FFFFFF !important; 
            transform: none !important;
        }
        button[data-baseweb="tab"][aria-selected="true"] {
            background-color: rgba(78, 201, 176, 0.25) !important;
            border-bottom: 3px solid #4EC9B0 !important;
            font-weight: bold !important;
        }

        /* 4. [New] ä¸Šå¸æ¨¡å¼è­¦ç¤ºè¾¹æ¡† (Security Unmasked Mode) */
        @keyframes border-pulse {
            0% { border-color: rgba(255, 75, 75, 0.3); box-shadow: 0 0 10px rgba(255, 0, 0, 0.1) inset; }
            50% { border-color: rgba(255, 75, 75, 0.8); box-shadow: 0 0 30px rgba(255, 0, 0, 0.3) inset; }
            100% { border-color: rgba(255, 75, 75, 0.3); box-shadow: 0 0 10px rgba(255, 0, 0, 0.1) inset; }
        }

        .security-unmasked {
            position: fixed;
            top: 0; left: 0; width: 100vw; height: 100vh;
            border: 5px solid #FF4B4B;
            pointer-events: none; /* è®©é¼ æ ‡èƒ½ç©¿é€ */
            z-index: 999999;
            animation: border-pulse 2s infinite;
        }

    </style>
    """

    @classmethod
    def apply_global_styles(cls):
        """æ³¨å…¥å…¨å±€ CSS"""
        st.markdown(cls._MAIN_STYLES, unsafe_allow_html=True)

    @classmethod
    def apply_unmasked_style(cls):
        """[ç‰¹æƒ] æ¿€æ´»çº¢è‰²è­¦ç¤ºæ¡†"""
        st.markdown('<div class="security-unmasked"></div>', unsafe_allow_html=True)
==================== END FILE: ui/styles.py ====================


==================== START FILE: ui/components/security.py ====================
# ui/components/security.py
"""
æ–‡ä»¶è¯´æ˜: å®‰å…¨å«å£«ç»„ä»¶ (Security Gate Component) - V5.2 Four-Level Codes
ä¸»è¦åŠŸèƒ½:
1. [ç»Ÿä¸€å…¥å£] å°è£…é«˜å±æ“ä½œéªŒè¯æµç¨‹ã€‚
2. [åŠ¨æ€ç­–ç•¥] æ”¯æŒ user, query, modify, db, system äº”ç§ä»¤ç‰Œã€‚
3. [æ™ºèƒ½å®¡è®¡] ä¸¥æ ¼åˆ¤å®š: ä»…å½“ (Modify + DB + System) åŒæ—¶å¯ç”¨æ—¶ï¼Œå¼ºåˆ¶æ ‡è®°æ—¥å¿—ä¸ºä¸å¯åˆ é™¤ã€‚
4. [å®‰å…¨ç†”æ–­] è¿ç»­éªŒè¯å¤±è´¥ 5 æ¬¡ï¼Œé”å®šè´¦æˆ·å¹¶å¼ºåˆ¶ç™»å‡ºã€‚
"""

import streamlit as st
import time
from typing import List, Tuple, Literal, Optional

from config.settings import settings
from core.services.auth.service import AuthService
from ui.kernel.session import SessionManager
from core.sys.logger import get_audit_logger
from core.services.security.policy_manager import SecurityPolicyManager

# ä¸“ç”¨å®¡è®¡æ—¥å¿—å™¨
audit_logger = get_audit_logger()
IMMUTABLE_MARKER = "[[SECURITY_AUDIT]]"

# å®šä¹‰ä»¤ç‰Œç±»å‹
TokenType = Literal["user", "query", "modify", "db", "system"]


class SecurityGate:
    """
    [å®‰å…¨åˆ‡é¢] é«˜å±æ“ä½œä¿æŠ¤å™¨ (å«ç†”æ–­æœºåˆ¶)
    """

    MAX_RETRIES = 5

    @staticmethod
    def _handle_failure(username: str, action_name: str):
        """å¤„ç†éªŒè¯å¤±è´¥é€»è¾‘: è®¡æ•° -> æŠ¥è­¦ -> ç†”æ–­"""
        if "auth_fail_count" not in st.session_state:
            st.session_state["auth_fail_count"] = 0

        st.session_state["auth_fail_count"] += 1
        current_fails = st.session_state["auth_fail_count"]

        remaining = SecurityGate.MAX_RETRIES - current_fails

        # è®°å½•è­¦å‘Šæ—¥å¿—
        audit_logger.warning(
            f"éªŒè¯å¤±è´¥ ({current_fails}/{SecurityGate.MAX_RETRIES}) | Action: {action_name}",
            extra={"user": username, "action": "AUTH_FAIL"}
        )

        if remaining > 0:
            st.error(f" éªŒè¯å¤±è´¥ï¼å¯†ç é”™è¯¯æˆ–æƒé™ä¸è¶³ã€‚å‰©ä½™å°è¯•æ¬¡æ•°: {remaining}")
        else:
            # --- è§¦å‘ç†”æ–­ ---
            st.error("ğŸš« å®‰å…¨è­¦æŠ¥ï¼šè¿ç»­éªŒè¯å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œè´¦æˆ·å·²è¢«é”å®šï¼")
            AuthService.set_lock_state(username, True)

            audit_logger.critical(
                f"{IMMUTABLE_MARKER} [SECURITY LOCKOUT] User {username} locked due to max auth failures.",
                extra={"user": username, "action": "AUTO_LOCK", "func": "SecurityGate"}
            )

            time.sleep(2)
            SessionManager.logout()

    @staticmethod
    def authorize_action(
            action_key: str,
            ui_key_suffix: str = "",
            require_reason: bool = False,
            help_text: Optional[str] = None
    ) -> Tuple[bool, str]:
        """
        [ä¸»æ–¹æ³•] æ¸²æŸ“éªŒè¯ UI å¹¶è¿”å›æˆæƒç»“æœ
        """
        # 1. èº«ä»½é¢„æ£€
        user = SessionManager.get_current_user()
        if not user: return False, ""
        username = user.get("username")

        # 2. åŠ è½½ç­–ç•¥
        tokens = SecurityPolicyManager.get_required_tokens(action_key)
        meta = SecurityPolicyManager.get_action_metadata(action_key)

        action_name = meta.get("name", action_key)
        description = help_text or meta.get("description", "")

        # 3. æ¸²æŸ“ UI
        st.markdown(f"##### ğŸ” å®‰å…¨éªŒè¯: {action_name}")
        if description: st.caption(description)

        cols_count = len(tokens)
        cols = st.columns(cols_count) if cols_count > 0 else []

        inputs = {}
        unique_ui_key = f"gate_{action_key}_{ui_key_suffix}"

        # [æ ¸å¿ƒ] æ–°ç‰ˆæ ‡ç­¾æ˜ å°„
        LABEL_MAP = {
            "user": "ğŸ‘¤ å½“å‰ç”¨æˆ·å¯†ç ",
            "query": "ğŸ‘€ æ•°æ®æŸ¥è¯¢å®‰ä¿ç ",
            "modify": "âœï¸ æ•°æ®ä¿®æ”¹å®‰ä¿ç ",
            "db": "ğŸ’¾ æ•°æ®åº“ç®¡ç†ç ",
            "system": "ğŸ›¡ï¸ å¹³å°ç³»ç»Ÿç®¡ç†ç "
        }

        for idx, token_type in enumerate(tokens):
            with cols[idx]:
                # å‹å¥½æç¤ºï¼šå¦‚æœæ˜¯å¤šé‡éªŒè¯ï¼ŒåŠ åºå·
                raw_label = LABEL_MAP.get(token_type, f"ğŸ”‘ {token_type}")
                label = f"{idx + 1}. {raw_label}" if cols_count > 1 else raw_label

                inputs[token_type] = st.text_input(
                    label,
                    type="password",
                    key=f"{unique_ui_key}_pwd_{token_type}"
                )

        reason_text = ""
        if require_reason:
            reason_text = st.text_area(
                "ğŸ“ æ“ä½œå¤‡æ³¨ (å¿…å¡«)",
                placeholder="è¯·è¯¦ç»†è¯´æ˜æ“ä½œåŸå› ï¼Œæ­¤è®°å½•å°†è¢«æ°¸ä¹…ä¿å­˜ã€‚",
                key=f"{unique_ui_key}_reason"
            )

        # ç¡®è®¤æŒ‰é’®
        if st.button(f" ç¡®è®¤æ‰§è¡Œ", type="primary", use_container_width=True, key=f"{unique_ui_key}_btn"):

            # A. æ ¡éªŒå¤‡æ³¨
            if require_reason and not reason_text.strip():
                st.error(" æ‹’ç»æ‰§è¡Œï¼šå¿…é¡»å¡«å†™æ“ä½œå¤‡æ³¨ã€‚")
                return False, ""

            # B. æ ¡éªŒå¯†ç 
            all_valid = True
            failed_token = None

            for token_type, input_val in inputs.items():
                if not SecurityPolicyManager.validate_token(token_type, input_val, username):
                    all_valid = False
                    failed_token = token_type
                    break

            if not all_valid:
                SecurityGate._handle_failure(username, f"{action_key} (Token: {failed_token})")
                return False, ""

            # éªŒè¯é€šè¿‡ -> é‡ç½®è®¡æ•°å™¨
            if "auth_fail_count" in st.session_state:
                st.session_state["auth_fail_count"] = 0

            # C. å®¡è®¡è®°å½•åˆ¤å®š
            # [æ ¸å¿ƒè§„åˆ™] è‹¥ Modify, DB, System ä¸‰è€…éƒ½è¢«é€‰ä¸­ => ä¸å¯åˆ é™¤æ—¥å¿—
            required_set = set(tokens)
            critical_set = {"modify", "db", "system"}

            is_immutable = critical_set.issubset(required_set)
            prefix = IMMUTABLE_MARKER if is_immutable else ""

            log_payload = f"{prefix} æˆæƒé€šè¿‡ | Action: {action_key} | Reason: {reason_text}"
            audit_logger.critical(
                log_payload,
                extra={"user": username, "action": "AUTH_GRANT", "func": "SecurityGate"}
            )

            return True, reason_text

        return False, ""
==================== END FILE: ui/components/security.py ====================


==================== START FILE: ui/components/permission_tree.py ====================
# ui/components/permission_tree.py
"""
æ–‡ä»¶è¯´æ˜: æƒé™æ ‘æ¸²æŸ“ç»„ä»¶ (Permission Tree UI Component) - V2.1 Bi-Directional Sync
ä¸»è¦åŠŸèƒ½:
1. [å…¬æœ‰UIç±»] é€’å½’æ¸²æŸ“ Module -> Tab -> Action çš„å¤é€‰æ¡†æ ‘ã€‚
2. [åŒå‘è”åŠ¨] å®ç° "çˆ¶é€‰å­å…¨é€‰" (å‘ä¸‹) å’Œ "å­å…¨é€‰çˆ¶è‡ªåŠ¨å‹¾" (å‘ä¸Š) çš„é€»è¾‘ã€‚
3. [çŠ¶æ€ç®¡ç†] ç›´æ¥æ“ä½œ st.session_stateï¼Œä¿æŒ UI å“åº”æ€§ã€‚
"""

import streamlit as st
from typing import List, Dict


class PermissionTreeRenderer:

    def __init__(self, user_perms: Dict[str, bool], session_prefix: str = "perm_tree"):
        self.prefix = session_prefix
        self._init_state(user_perms)

    def _get_key(self, node_key: str) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„ Session Key"""
        return f"{self.prefix}_{node_key}"

    def _init_state(self, perms: Dict[str, bool]):
        """åˆå§‹åŒ–çŠ¶æ€ (ä»…ä¸€æ¬¡)"""
        if f"{self.prefix}_initialized" not in st.session_state:
            st.session_state[f"{self.prefix}_initialized"] = True
            for k, v in perms.items():
                st.session_state[self._get_key(k)] = v

    def render(self, tree_data: List[Dict]):
        """[ä¸»å…¥å£] æ¸²æŸ“æ•´æ£µæ ‘"""
        st.markdown("#### ğŸ”’ ç»†ç²’åº¦æƒé™é…ç½®")
        for node in tree_data:
            self._render_node(node, level=0)

    def _render_node(self, node: Dict, level: int):
        """é€’å½’æ¸²æŸ“èŠ‚ç‚¹ï¼ŒåŒ…å«åŒå‘è”åŠ¨é€»è¾‘"""
        key = node["key"]
        label = node["name"]
        children = node.get("children", [])
        ui_key = self._get_key(key)

        indent = "&nbsp;" * (level * 4)
        icon = "ğŸ“¦" if level == 0 else ("ğŸ“‘" if level == 1 else "")

        # 1. ç¡®ä¿ Session ä¸­æœ‰å€¼ (é»˜è®¤ False)
        if ui_key not in st.session_state:
            st.session_state[ui_key] = False

        # 2. [å…³é”®] å‘ä¸Šæ ¡æ­£é€»è¾‘ (Upward Sync)
        # åœ¨æ¸²æŸ“å‰ï¼Œæ£€æŸ¥æ‰€æœ‰å­èŠ‚ç‚¹çš„çŠ¶æ€ã€‚
        # å¦‚æœå­˜åœ¨å­èŠ‚ç‚¹ï¼Œä¸”æ‰€æœ‰å­èŠ‚ç‚¹éƒ½è¢«é€‰ä¸­ï¼Œåˆ™çˆ¶èŠ‚ç‚¹è‡ªåŠ¨é€‰ä¸­ï¼›å¦åˆ™å–æ¶ˆé€‰ä¸­ã€‚
        # æ³¨æ„ï¼šè¿™ä¿è¯äº†"è§†è§‰"ä¸Šçš„ä¸€è‡´æ€§ã€‚
        if children:
            child_keys = [self._get_key(c["key"]) for c in children]
            # ç¡®ä¿å­èŠ‚ç‚¹ key åœ¨ session ä¸­å­˜åœ¨ (é˜²æ­¢ key error)
            for k in child_keys:
                if k not in st.session_state: st.session_state[k] = False

            # è®¡ç®—ç†è®ºçŠ¶æ€
            all_children_checked = all(st.session_state[k] for k in child_keys)

            # ä»…å½“çŠ¶æ€ä¸ä¸€è‡´æ—¶æ‰æ›´æ–°ï¼Œé¿å…æ­»å¾ªç¯
            if st.session_state[ui_key] != all_children_checked:
                st.session_state[ui_key] = all_children_checked

        # 3. å®šä¹‰å‘ä¸‹ä¼ æ’­å›è°ƒ (Downward Sync)
        def on_change():
            new_val = st.session_state[ui_key]
            self._propagate_down(node, new_val)
            # è¿™é‡Œçš„æ”¹å˜ä¼šåœ¨ä¸‹ä¸€æ¬¡ Rerun æ—¶è§¦å‘å­èŠ‚ç‚¹çš„æ¸²æŸ“æ›´æ–°

        # 4. æ¸²æŸ“ Checkbox
        st.checkbox(
            f"{icon} {label}",
            key=ui_key,
            on_change=on_change,
        )

        if node.get("desc"):
            st.caption(f"{indent} â””â”€ {node['desc']}")

        # 5. é€’å½’æ¸²æŸ“å­èŠ‚ç‚¹
        if children:
            with st.container():
                # ç‰©ç†ç¼©è¿›
                _, col_content = st.columns([0.5 + (level * 0.3), 10])
                with col_content:
                    for child in children:
                        self._render_node(child, level + 1)

    def _propagate_down(self, node: Dict, new_value: bool):
        """[å‘ä¸‹è”åŠ¨] çˆ¶çº§æ”¹å˜ -> é€’å½’å¼ºåˆ¶è®¾ç½®æ‰€æœ‰å­å­™çº§"""
        children = node.get("children", [])
        for child in children:
            child_key = self._get_key(child["key"])
            st.session_state[child_key] = new_value
            # é€’å½’å¤„ç†å­™èŠ‚ç‚¹
            self._propagate_down(child, new_value)

    def get_selected_keys(self, tree_data: List[Dict]) -> List[str]:
        """è·å–å½“å‰æ‰€æœ‰è¢«å‹¾é€‰çš„ Key (ç”¨äºä¿å­˜)"""
        selected = []

        def traverse(nodes):
            for n in nodes:
                # åªä¿å­˜å‹¾é€‰çš„
                if st.session_state.get(self._get_key(n["key"]), False):
                    selected.append(n["key"])
                traverse(n.get("children", []))

        traverse(tree_data)
        return selected
==================== END FILE: ui/components/permission_tree.py ====================


==================== START FILE: ui/components/safety.py ====================
# ui/components/safety.py
"""
æ–‡ä»¶è¯´æ˜: å®‰å…¨æ ¡éªŒç»„ä»¶ (Safety Verification Component)
ä¸»è¦åŠŸèƒ½:
1. æä¾›é«˜å±æ“ä½œå‰çš„äºŒæ¬¡ç¡®è®¤ UIã€‚
2. æ ¡éªŒå½“å‰ç”¨æˆ·å¯†ç æˆ–ç³»ç»Ÿè¿ç»´å¯†ç  (DB_OPERATOR_PWD)ã€‚
3. å¼ºåˆ¶è¾“å…¥ç¡®è®¤çŸ­è¯­ (Confirmation Phrase) ä»¥é˜²æ­¢è¯¯æ“ä½œã€‚
"""

import streamlit as st
from typing import Optional, Tuple

from config.settings import settings
from core.services.auth.service import AuthService
from core.sys.context import get_current_user
from core.sys.logger import get_logger

logger = get_logger("SafetyComponent")


def confirm_dangerous_action(
        title: str,
        description: str,
        *,
        confirm_phrase: str = "CONFIRM",
        require_current_password: bool = True,
        require_operator_password: bool = False,
        key_prefix: str = "danger_confirm",
) -> Tuple[bool, Optional[str]]:
    """
    [UIç»„ä»¶] é«˜å±æ“ä½œç»Ÿä¸€ç¡®è®¤æ¡†

    Returns:
        (confirmed: bool, note: str)
    """
    username = get_current_user()

    st.markdown(f"### {title}")
    st.warning(description)

    with st.expander("ğŸ›¡ï¸ å®‰å…¨æ ¡éªŒ / Security Check", expanded=True):
        # 1. å¤‡æ³¨è®°å½•
        note = st.text_area(
            "æ“ä½œå¤‡æ³¨ (å¯é€‰):",
            help="å»ºè®®è®°å½•æ“ä½œåŸå› ï¼Œä¾‹å¦‚: 'ä¿®å¤æ•°æ®å¯¼å…¥é”™è¯¯'",
            key=f"{key_prefix}_note"
        )

        # 2. å¯†ç æ ¡éªŒçŠ¶æ€
        current_pwd_ok = True
        operator_pwd_ok = True

        current_pwd_input = ""
        operator_pwd_input = ""

        c1, c2 = st.columns(2)

        with c1:
            if require_current_password:
                current_pwd_input = st.text_input(
                    "ğŸ‘¤ éªŒè¯å½“å‰è´¦å·å¯†ç :",
                    type="password",
                    key=f"{key_prefix}_curr_pwd"
                )
                current_pwd_ok = False  # é»˜è®¤æœªé€šè¿‡ï¼Œç­‰å¾…æ ¡éªŒ

        with c2:
            if require_operator_password:
                operator_pwd_input = st.text_input(
                    "ğŸ”‘ éªŒè¯è¿ç»´æ“ä½œå¯†ç :",
                    type="password",
                    help="è¯·è¾“å…¥ç³»ç»Ÿé…ç½®çš„ DB_OPERATOR_PWD",
                    key=f"{key_prefix}_op_pwd"
                )
                operator_pwd_ok = False

        # 3. ç¡®è®¤çŸ­è¯­
        phrase_input = st.text_input(
            f"âœï¸ è¯·è¾“å…¥ç¡®è®¤çŸ­è¯­: `{confirm_phrase}`",
            key=f"{key_prefix}_phrase"
        )

        if st.button("âš ï¸ æˆ‘å·²çŸ¥æ™“é£é™©ï¼Œç¡®è®¤æ‰§è¡Œ", type="primary", use_container_width=True, key=f"{key_prefix}_btn"):

            # A. æ ¡éªŒå½“å‰ç”¨æˆ·å¯†ç 
            if require_current_password:
                if not current_pwd_input:
                    st.error("è¯·è¾“å…¥å½“å‰è´¦å·å¯†ç ã€‚")
                    return False, None

                # ä½¿ç”¨ AuthService è¿›è¡Œæ ¡éªŒ (ä¸æ”¹å˜ç™»å½•çŠ¶æ€)
                # æ³¨æ„: è¿™é‡Œéœ€è¦ä¸€ç§ä¸ç”Ÿæˆ Token çš„æ ¡éªŒæ–¹å¼ï¼Œæˆ–è€…å¤ç”¨ authenticate ä½†å¿½ç•¥ç»“æœ
                # ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç›´æ¥è°ƒç”¨ AuthService.authenticate
                ok, _, _ = AuthService.authenticate(username, current_pwd_input)
                if not ok:
                    st.error("å½“å‰è´¦å·å¯†ç é”™è¯¯ã€‚")
                    return False, None
                current_pwd_ok = True

            # B. æ ¡éªŒè¿ç»´å¯†ç 
            if require_operator_password:
                if operator_pwd_input != settings.DB_OPERATOR_PWD:
                    st.error("è¿ç»´æ“ä½œå¯†ç é”™è¯¯ã€‚")
                    return False, None
                operator_pwd_ok = True

            # C. æ ¡éªŒçŸ­è¯­
            if phrase_input != confirm_phrase:
                st.error(f"ç¡®è®¤çŸ­è¯­ä¸åŒ¹é…ï¼Œè¯·è¾“å…¥: {confirm_phrase}")
                return False, None

            # å…¨éƒ¨é€šè¿‡
            if current_pwd_ok and operator_pwd_ok:
                logger.info(f"é«˜å±æ“ä½œç¡®è®¤: {title} | User: {username}")
                return True, note

    return False, None
==================== END FILE: ui/components/safety.py ====================


==================== START FILE: ui/pages/db_data_change.py ====================
# ui/pages/db_data_change.py
"""
æ–‡ä»¶è¯´æ˜: æ•°æ®ä¿®æ”¹ä¸­å¿ƒ UI (Data Modification Center) - V5.1 Concurrency Safe
ä¸»è¦åŠŸèƒ½:
1. åº“å­˜ä¿®æ”¹:
   - å•å“ä¿®æ”¹: ä½¿ç”¨ 'btn_update_single_inv' ç­–ç•¥ã€‚
   - æ•´åˆ—åˆ é™¤: åŠ¨æ€é€‰æ‹© 'btn_drop_inv_col' (è€æ•°æ®) æˆ– 'btn_drop_inv_col_recent' (æ–°æ•°æ®)ã€‚
2. èµ„æ–™ç»´æŠ¤:
   - æ‰¹é‡æ›´æ–°: ä½¿ç”¨ 'btn_batch_update_cogs' ç­–ç•¥ã€‚
   - æ–°å¢ SKU: ä½¿ç”¨ 'btn_create_skus' ç­–ç•¥ã€‚
3. [New] å¹¶å‘é”: å¯¹ Data_Inventory å’Œ Data_COGS è¿›è¡Œäº’æ–¥æ§åˆ¶ã€‚
"""

import streamlit as st
import pandas as pd
import time
import datetime

from core.services.data_manager import DataManager
from ui.components.security import SecurityGate
from core.sys.context import set_current_function
from ui.kernel.session import SessionManager
from core.sys.lock_manager import LockManager  # [New]

# å®šä¹‰èµ„æºé”®
LOCK_KEY_INV_MOD = "Data_Inventory"
LOCK_KEY_COGS_MOD = "Data_COGS"


def _reset_wizard():
    st.session_state.inv_step = 1
    st.session_state.sel_date = None
    st.session_state.inv_action = None
    st.rerun()


def _init_cogs_data(mgr: DataManager):
    if "cogs_df" not in st.session_state:
        df = mgr.get_cogs_data()
        for col in ["Cost", "Freight", "Cog"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)
        st.session_state.cogs_df = df


def _init_new_sku_df():
    if "new_sku_df" not in st.session_state:
        st.session_state.new_sku_df = pd.DataFrame(
            columns=["SKU", "Category", "SubCategory", "Type", "Cost", "Freight", "Initial_Qty"]
        )


# --- Sub Renderers ---

def _render_inventory_tab(mgr: DataManager, username: str):
    # [New] é”æ£€æŸ¥
    can, msg = LockManager.check_access(LOCK_KEY_INV_MOD, username)
    if not can:
        st.error(msg)
        return

    if 'inv_step' not in st.session_state: st.session_state.inv_step = 1
    if 'sel_date' not in st.session_state: st.session_state.sel_date = None
    if 'inv_action' not in st.session_state: st.session_state.inv_action = None

    step = st.session_state.inv_step

    # Step 1
    if step == 1:
        st.markdown("#### Step 1: é€‰æ‹©ç›®æ ‡æœˆä»½")
        cols = mgr.get_inventory_columns()
        if not cols:
            st.warning("æš‚æ— åº“å­˜æ•°æ®åˆ—ã€‚")
        else:
            c1, c2 = st.columns([3, 1])
            with c1:
                sel = st.selectbox("è¯·é€‰æ‹©åº“å­˜æ—¥æœŸåˆ—:", cols)
            with c2:
                st.write("");
                st.write("")
                if st.button("ä¸‹ä¸€æ­¥ â¡ï¸", type="primary", use_container_width=True):
                    st.session_state.sel_date = sel
                    st.session_state.inv_step = 2
                    st.rerun()

    # Step 2
    elif step == 2:
        target = st.session_state.sel_date
        st.markdown(f"#### Step 2: é€‰æ‹©æ“ä½œ ({target})")

        c1, c2 = st.columns(2)
        with c1:
            with st.container(border=True):
                st.markdown("##### âœï¸ ä¿®æ”¹å•å“åº“å­˜")
                if st.button("è¿›å…¥ä¿®æ”¹æ¨¡å¼", use_container_width=True):
                    st.session_state.inv_action = "MODIFY"
                    st.session_state.inv_step = 3
                    st.rerun()
        with c2:
            with st.container(border=True):
                st.markdown("##### ğŸ—‘ï¸ åˆ é™¤æ•´åˆ—æ•°æ®")
                st.caption("âš ï¸ é«˜å±ï¼šç‰©ç†åˆ é™¤è¯¥æ—¥æœŸåˆ—æ‰€æœ‰æ•°æ®ã€‚")
                if st.button("è¿›å…¥åˆ é™¤æ¨¡å¼", type="primary", use_container_width=True):
                    st.session_state.inv_action = "DELETE"
                    st.session_state.inv_step = 3
                    st.rerun()

        st.markdown("---")
        if st.button("â¬…ï¸ è¿”å›"): _reset_wizard()

    # Step 3A: Modify
    elif step == 3 and st.session_state.inv_action == "MODIFY":
        target = st.session_state.sel_date
        st.markdown(f"#### Step 3: ä¿®æ”¹å•å“ ({target})")

        skus = mgr.get_all_skus()
        sel_sku = st.selectbox("é€‰æ‹© SKU:", skus)
        curr_val = mgr.get_inventory_value(target, sel_sku)

        c_m1, c_m2 = st.columns(2)
        c_m1.metric("å½“å‰åº“å­˜", curr_val)
        new_v = c_m2.number_input("æ–°åº“å­˜", value=curr_val, step=1)

        st.markdown("---")
        c1, c2 = st.columns([1, 1])
        with c1:
            if st.button("â¬…ï¸ è¿”å›"): st.session_state.inv_step = 2; st.rerun()
        with c2:
            # [Gate] å•å“ä¿®æ”¹
            is_ok, _ = SecurityGate.authorize_action(
                action_key="btn_update_single_inv",
                ui_key_suffix="inv_mod_single",
                require_reason=False
            )
            if is_ok:
                # [New] åŠ é”
                if LockManager.acquire_lock(LOCK_KEY_INV_MOD, username, f"ä¿®æ”¹åº“å­˜:{sel_sku}")[0]:
                    try:
                        success, msg = mgr.update_inventory_qty(target, sel_sku, int(new_v))
                        if success:
                            st.success(msg);
                            time.sleep(1);
                            _reset_wizard()
                        else:
                            st.error(msg)
                    finally:
                        LockManager.release_lock(LOCK_KEY_INV_MOD, username)
                else:
                    st.error("æ— æ³•è·å–é”ï¼Œèµ„æºæ­£å¿™")

    # Step 3B: Delete
    elif step == 3 and st.session_state.inv_action == "DELETE":
        target = st.session_state.sel_date
        st.markdown(f"#### Step 3: åˆ é™¤æ•´åˆ— ({target})")

        is_old = False
        try:
            t_date = datetime.datetime.strptime(target, "%Y-%m-%d").date()
            if (datetime.date.today() - t_date).days > 30: is_old = True
        except:
            is_old = True

        if is_old:
            gate_key = "btn_drop_inv_col"
            st.error(f"ğŸ’€ æ•°æ®å·²å½’æ¡£è¶…è¿‡30å¤©ï¼Œéœ€ **æœ€é«˜æƒé™éªŒè¯**ã€‚")
        else:
            gate_key = "btn_drop_inv_col_recent"
            st.warning(f"âš ï¸ è¿‘æœŸæ•°æ®ï¼Œéœ€ **å½“å‰ç”¨æˆ·å¯†ç **ã€‚")

        is_ok, reason = SecurityGate.authorize_action(
            action_key=gate_key,
            ui_key_suffix="inv_del_col",
            require_reason=True
        )

        st.markdown("---")
        if st.button("â¬…ï¸ è¿”å›"): st.session_state.inv_step = 2; st.rerun()

        if is_ok:
            # [New] åŠ é”
            if LockManager.acquire_lock(LOCK_KEY_INV_MOD, username, f"åˆ é™¤åˆ—:{target}")[0]:
                try:
                    with st.spinner("Deleting..."):
                        success, msg = mgr.drop_inventory_column(target, reason)
                    if success:
                        st.balloons();
                        st.success(msg);
                        time.sleep(2);
                        _reset_wizard()
                    else:
                        st.error(msg)
                finally:
                    LockManager.release_lock(LOCK_KEY_INV_MOD, username)
            else:
                st.error("æ— æ³•è·å–é”")


def _render_cogs_tab(mgr: DataManager, username: str):
    # [New] é”æ£€æŸ¥
    can, msg = LockManager.check_access(LOCK_KEY_COGS_MOD, username)
    if not can:
        st.error(msg)
        return

    mode = st.radio("æ¨¡å¼é€‰æ‹©", ["ğŸ“ ç¼–è¾‘ç°æœ‰æ¡£æ¡ˆ", "â• æ‰¹é‡æ–°å¢ SKU"], horizontal=True, label_visibility="collapsed")
    st.markdown("---")

    opt_cat = mgr.get_distinct_options("Category")
    opt_sub = mgr.get_distinct_options("SubCategory")
    opt_type = mgr.get_distinct_options("Type")

    # Mode A: Edit
    if "ç¼–è¾‘" in mode:
        _init_cogs_data(mgr)
        st.info("ğŸ’¡ è¯´æ˜: ä¿®æ”¹ Cost æˆ– Freight åï¼ŒCog ä¼šè‡ªåŠ¨æ›´æ–°ã€‚")

        df_edit = st.session_state.cogs_df
        if not df_edit.empty:
            df_edit["Cog"] = (df_edit["Cost"] + df_edit["Freight"]).round(2)

        edited_df = st.data_editor(
            df_edit,
            key="cogs_editor_smart",
            use_container_width=True,
            num_rows="fixed",
            hide_index=True,
            column_config={
                "SKU": st.column_config.TextColumn("SKU", disabled=True, width="medium"),
                "Cog": st.column_config.NumberColumn("Cog (Total)", disabled=True, format="%.2f"),
                "Cost": st.column_config.NumberColumn("Cost", min_value=0, format="%.2f", required=True),
                "Freight": st.column_config.NumberColumn("Freight", min_value=0, format="%.2f", required=True),
                "Category": st.column_config.SelectboxColumn("Category", options=opt_cat),
                "SubCategory": st.column_config.SelectboxColumn("SubCategory", options=opt_sub),
                "Type": st.column_config.SelectboxColumn("Type", options=opt_type),
                "id": None, "created_at": None, "updated_at": None
            }
        )

        if not edited_df.equals(st.session_state.cogs_df):
            st.session_state.cogs_df = edited_df
            st.rerun()

        st.markdown("---")
        # [Gate] æ‰¹é‡æ›´æ–°
        is_ok, _ = SecurityGate.authorize_action(
            action_key="btn_batch_update_cogs",
            ui_key_suffix="cogs_update",
            require_reason=False
        )

        if is_ok:
            # [New] åŠ é”
            if LockManager.acquire_lock(LOCK_KEY_COGS_MOD, username, "æ‰¹é‡æ›´æ–°COGS")[0]:
                try:
                    success, msg = mgr.update_cogs_smart(edited_df)
                    if success:
                        st.success(msg)
                        del st.session_state.cogs_df
                        time.sleep(2);
                        st.rerun()
                    else:
                        st.error(msg)
                finally:
                    LockManager.release_lock(LOCK_KEY_COGS_MOD, username)
            else:
                st.error("èµ„æºå¿™ï¼Œè¯·ç¨å")

    # Mode B: Create
    else:
        _init_new_sku_df()
        st.info("ğŸ’¡ è¯´æ˜: SKU, Cost, Freight ä¸ºå¿…å¡«é¡¹ã€‚Initial_Qty å°†å†™å…¥æœ€æ–°æœˆä»½åº“å­˜ã€‚")

        df_new = st.session_state.new_sku_df
        for c in ["Cost", "Freight", "Cog", "Initial_Qty"]:
            if c not in df_new.columns: df_new[c] = 0.0

        if not df_new.empty:
            df_new["Cost"] = pd.to_numeric(df_new["Cost"], errors='coerce').fillna(0.0)
            df_new["Freight"] = pd.to_numeric(df_new["Freight"], errors='coerce').fillna(0.0)
            df_new["Cog"] = (df_new["Cost"] + df_new["Freight"]).round(2)

        edited_new = st.data_editor(
            df_new,
            key="cogs_create_new",
            use_container_width=True,
            num_rows="dynamic",
            hide_index=True,
            column_config={
                "SKU": st.column_config.TextColumn("SKU (å¿…å¡«)", required=True),
                "Category": st.column_config.SelectboxColumn("Category", options=opt_cat),
                "SubCategory": st.column_config.SelectboxColumn("SubCategory", options=opt_sub),
                "Type": st.column_config.SelectboxColumn("Type", options=opt_type),
                "Cost": st.column_config.NumberColumn("Cost", min_value=0, format="%.2f", required=True),
                "Freight": st.column_config.NumberColumn("Freight", min_value=0, format="%.2f", required=True),
                "Cog": st.column_config.NumberColumn("Cog (Auto)", disabled=True, format="%.2f"),
                "Initial_Qty": st.column_config.NumberColumn("å½“å‰åº“å­˜", min_value=0, step=1, format="%d",
                                                             required=True)
            }
        )

        if not edited_new.equals(st.session_state.new_sku_df):
            st.session_state.new_sku_df = edited_new
            st.rerun()

        st.markdown("---")

        # [Gate] æ‰¹é‡æ–°å¢
        is_ok, _ = SecurityGate.authorize_action(
            action_key="btn_create_skus",
            ui_key_suffix="cogs_create",
            require_reason=False
        )

        if is_ok:
            valid_rows = []
            if not edited_new.empty:
                for _, row in edited_new.iterrows():
                    s = str(row.get("SKU", "")).strip()
                    if s and pd.notna(row.get("Cost")) and pd.notna(row.get("Freight")):
                        valid_rows.append(row.to_dict())

            if not valid_rows:
                st.error(" æ²¡æœ‰æœ‰æ•ˆçš„è¡Œ (SKU, Cost, Freight å‡ä¸ºå¿…å¡«)")
            else:
                # [New] åŠ é”
                if LockManager.acquire_lock(LOCK_KEY_COGS_MOD, username, "æ‰¹é‡æ–°å¢SKU")[0]:
                    try:
                        success, msg = mgr.batch_create_skus(valid_rows)
                        if success:
                            st.success(msg)
                            st.session_state.new_sku_df = pd.DataFrame(columns=edited_new.columns)
                            time.sleep(2);
                            st.rerun()
                        else:
                            st.error(msg)
                    finally:
                        LockManager.release_lock(LOCK_KEY_COGS_MOD, username)
                else:
                    st.error("èµ„æºå¿™")


# --- Main ---
def render():
    set_current_function("æ•°æ®ä¿®æ”¹ä¸­å¿ƒ")
    st.title("ğŸ› ï¸ æ•°æ®ä¿®æ”¹ä¸­å¿ƒ")

    # è·å–ç”¨æˆ·ä¿¡æ¯ç”¨äºé”ç®¡ç†
    user_info = SessionManager.get_current_user()
    username = user_info.get("username") if user_info else "Unknown"

    mgr = DataManager()
    tab_inv, tab_cogs = st.tabs(["ğŸ“¦ ä¿®æ”¹åº“å­˜", "ğŸ“ èµ„æ–™ç»´æŠ¤"])

    with tab_inv: _render_inventory_tab(mgr, username)
    with tab_cogs: _render_cogs_tab(mgr, username)
==================== END FILE: ui/pages/db_data_change.py ====================


==================== START FILE: ui/pages/db_admin.py ====================
# ui/pages/db_admin.py
"""
æ–‡ä»¶è¯´æ˜: æ•°æ®åº“è¿ç»´ä¸­å¿ƒ UI (Database Admin Console) - V5.0 Config Driven
ä¸»è¦åŠŸèƒ½:
1. [Refactor] å…¨é¢æ¥å…¥ SecurityGateï¼ŒåŸºäº Action Key é‰´æƒã€‚
2. äº¤äº’ä¼˜åŒ–: ä¿æŒè¿›åº¦æ¡ä¸å‹å¥½æç¤ºã€‚
3. æƒé™ç®¡ç†: åŠ¨ä½œæƒé™ç”± action_registry.json ç»Ÿä¸€æ§åˆ¶ã€‚
"""

import time
import os
import datetime
import streamlit as st
import pandas as pd
from typing import List, Dict, Tuple

from config.settings import settings
from core.services.database_service import DatabaseService
from ui.kernel.session import SessionManager
from ui.components.security import SecurityGate
from core.sys.context import set_current_function


def _build_options_map(files: List[str]) -> Tuple[List[str], Dict[str, str]]:
    options = {}
    for f in files:
        display = DatabaseService.parse_filename_to_display(f)
        if display in options: display += f" ({f})"
        options[display] = f
    return sorted(options.keys(), reverse=True), options


def _simulate_progress(label="Processing...", speed=0.02):
    bar = st.progress(0, text=label)
    for i in range(1, 91):
        time.sleep(speed)
        bar.progress(i, text=label)
    return bar


# --- Tab Renderers ---

def _render_backup_tab(service: DatabaseService):
    st.info("ğŸ’¡ è¯´æ˜ï¼šæ­¤æ“ä½œå°†ä¸ºå½“å‰æ•°æ®åº“çš„æ‰€æœ‰è¡¨åˆ›å»ºå…¨é‡å¿«ç…§ (System Snapshot)ã€‚")

    with st.container(border=True):
        c1, c2 = st.columns([3, 1])
        with c1:
            tag = st.text_input("å¤‡ä»½å¤‡æ³¨ (å¯é€‰)", placeholder="è¯·ä¸ºå¤‡ä»½æ·»åŠ å¤‡æ³¨, ä¾‹å¦‚: Before_Update")
        with c2:
            st.write("");
            st.write("")

            # [Gate] åˆ›å»ºå¤‡ä»½ (Key: btn_create_backup)
            is_approved, _ = SecurityGate.authorize_action(
                action_key="btn_create_backup",
                ui_key_suffix="backup",
                require_reason=False
            )

            if is_approved:
                bar = _simulate_progress("æ­£åœ¨æ‰§è¡Œå…¨é‡å¤‡ä»½ (Mysqldump)...")
                ok, msg = service.create_backup(tag)
                bar.progress(100, text="å¤‡ä»½å®Œæˆ")
                if ok:
                    st.success(msg);
                    time.sleep(1);
                    st.rerun()
                else:
                    st.error(msg)


def _render_restore_tab(service: DatabaseService):
    st.warning("âš ï¸ **é«˜å±è­¦å‘Š**ï¼šæ¢å¤æ“ä½œå°† **å®Œå…¨è¦†ç›–** å½“å‰æ•°æ®åº“ï¼Œä¸”ä¸å¯æ’¤é”€ï¼")
    files = service.list_backups()
    if not files:
        st.info("æš‚æ— å¯ç”¨è¿˜åŸç‚¹ã€‚")
        return

    labels, file_map = _build_options_map(files)
    sel_label = st.selectbox("è¯·é€‰æ‹©æ¢å¤æ—¶é—´ç‚¹:", labels)
    real_file = file_map[sel_label]

    st.divider()

    # [Gate] æ¢å¤æ•°æ®åº“ (Key: btn_restore_db)
    is_approved, _ = SecurityGate.authorize_action(
        action_key="btn_restore_db",
        ui_key_suffix="restore",
        require_reason=False,
        help_text="é«˜å±æ“ä½œï¼šè¯·è¾“å…¥æœ€é«˜æƒé™å¯†ç ç»„åˆä»¥è§£é”æ¢å¤æŒ‰é’®ã€‚"
    )

    if is_approved:
        bar = st.progress(0.0, text="åˆå§‹åŒ–æ¢å¤è¿›ç¨‹...")

        def update_ui(p):
            val = max(0.0, min(p, 1.0))
            bar.progress(val, text=f"æ­£åœ¨å†™å…¥æ•°æ®åº“... {int(val * 100)}%")

        with st.spinner("æ­£åœ¨è¿˜åŸæ•°æ®ï¼Œè¯·å‹¿å…³é—­é¡µé¢..."):
            ok, msg = service.restore_backup_with_progress(real_file, update_ui)

        if ok:
            st.balloons();
            st.success(f" æ•°æ®åº“å·²æˆåŠŸå›æ»šè‡³: {sel_label}")
        else:
            st.error(msg)


def _render_manage_tab(service: DatabaseService):
    files = service.list_backups()
    if not files: st.info("æš‚æ— å†å²å¤‡ä»½ã€‚"); return

    data = []
    for f in files:
        try:
            path = settings.BACKUP_DIR / f
            size = os.path.getsize(path) / 1024
            display = DatabaseService.parse_filename_to_display(f)
            data.append({"å¤‡ä»½å¿«ç…§": display, "æ–‡ä»¶å¤§å°": f"{size:.1f} KB"})
        except:
            pass
    st.dataframe(data, use_container_width=True)

    st.markdown("---")
    st.markdown("#### ğŸ—‘ï¸ åˆ é™¤å¤‡ä»½")

    labels, file_map = _build_options_map(files)
    sel_label = st.selectbox("é€‰æ‹©è¦åˆ é™¤çš„å¤‡ä»½:", labels, key="del_sel")
    real_file = file_map[sel_label]

    # [Gate] åˆ é™¤å¤‡ä»½ (Key: btn_delete_backup)
    is_approved, _ = SecurityGate.authorize_action(
        action_key="btn_delete_backup",
        ui_key_suffix="del_backup",
        require_reason=False
    )

    if is_approved:
        bar = _simulate_progress("æ­£åœ¨åˆ é™¤...", speed=0.01)
        ok, msg = service.delete_backup(real_file)
        bar.progress(100, text="å·²åˆ é™¤")
        if ok:
            st.toast(f"å·²åˆ é™¤: {sel_label}");
            time.sleep(1);
            st.rerun()
        else:
            st.error(msg)


def _render_delete_tab(service: DatabaseService):
    st.error("ğŸ’€ **æåº¦å±é™©åŒº**ï¼šæ­¤æ“ä½œå°†ç‰©ç†åˆ é™¤æ•°æ®åº“ä¸­çš„ä¸šåŠ¡æ•°æ®ï¼Œä¸å¯é€†ï¼")
    with st.container(border=True):
        st.markdown("#### ğŸ§¹ æŒ‰æ—¥æœŸåŒºé—´æ¸…æ´—æ•°æ®")
        c1, c2 = st.columns(2)
        today = datetime.date.today()
        with c1:
            start_d = st.date_input("èµ·å§‹æ—¥æœŸ (å«)", value=today, key="d_start")
        with c2:
            end_d = st.date_input("ç»“æŸæ—¥æœŸ (å«)", value=today, key="d_end")

        if start_d > end_d: st.error("æ—¥æœŸæ— æ•ˆ"); return

        st.info("å—å½±å“èŒƒå›´ï¼š\n1. äº¤æ˜“æ•°æ® (Transaction)\n2. æŠ¥è¡¨æºæ•°æ® (Clean Log)\n3. åº“å­˜åˆ— (Inventory Columns)")
        st.markdown("---")

        # [Gate] æ•°æ®æ¸…æ´— (Key: btn_clean_data)
        # æ­¤æ“ä½œå¼ºåˆ¶è¦æ±‚å¤‡æ³¨ (require_reason=True)
        is_approved, reason = SecurityGate.authorize_action(
            action_key="btn_clean_data",
            ui_key_suffix="clean_data",
            require_reason=True,
            help_text="è¯·è¾“å…¥æœ€é«˜æƒé™å¯†ç å¹¶å¡«å†™åŸå› ã€‚"
        )

        if is_approved:
            bar = st.progress(0, text="åˆå§‹åŒ–...")
            try:
                bar.progress(30, text="æ‰«ææ•°æ®è¡¨...")
                time.sleep(0.5)
                bar.progress(60, text="æ¸…æ´—æ•°æ®åˆ—...")

                ok, msg = service.delete_business_data_by_range(start_d, end_d, reason)
                bar.progress(100, text="å®Œæˆ")
                if ok:
                    st.balloons();
                    st.success(" æ•°æ®æ¸…æ´—å®Œæˆ")
                    with st.expander("è¯¦ç»†æŠ¥å‘Š"):
                        st.text(msg)
                else:
                    st.error(f"å¤±è´¥: {msg}")
            except Exception as e:
                st.error(f"Error: {e}")


# --- Main ---
def render():
    set_current_function("æ•°æ®åº“è¿ç»´")
    st.title("ğŸ—„ï¸ æ•°æ®åº“è¿ç»´ä¸­å¿ƒ")

    service = DatabaseService()
    user = SessionManager.get_current_user()
    is_admin = user.get("is_admin", False)
    is_super = (user.get("username") == settings.SUPER_ADMIN_USER)

    if not is_admin: st.error(" æƒé™ä¸è¶³"); return

    tabs = ["ğŸ“¤ åˆ›å»ºå¤‡ä»½", "ğŸ“¥ å¤‡ä»½æ¢å¤", "ğŸ—‚ï¸ å†å²å¤‡ä»½ç®¡ç†"]
    if is_super: tabs.append("â˜¢ï¸ æ•°æ®åˆ é™¤ (Data Deletion)")

    st_tabs = st.tabs(tabs)

    with st_tabs[0]:
        _render_backup_tab(service)
    with st_tabs[1]:
        _render_restore_tab(service)
    with st_tabs[2]:
        _render_manage_tab(service)
    if len(st_tabs) > 3:
        with st_tabs[3]: _render_delete_tab(service)
==================== END FILE: ui/pages/db_admin.py ====================


==================== START FILE: ui/pages/audit_logs.py ====================
# ui/pages/audit_logs.py
"""
æ–‡ä»¶è¯´æ˜: å®‰å…¨å®¡è®¡æ—¥å¿—ä¸­å¿ƒ (Audit Logs) - V5.0 Config Driven Security
ä¸»è¦åŠŸèƒ½:
1. å¤šç»´æ—¥å¿—è§†å›¾ (3 Tabs).
2. [Refactor] å®‰å…¨å‡çº§: æ¥å…¥ V5 SecurityGateï¼ŒåŸºäº Action Key é‰´æƒã€‚
3. è§†å›¾æ¨¡å¼: é»˜è®¤è„±æ•ï¼Œä¸Šå¸è§†è§’è§£é”ã€‚
4. ç‰¹æƒæ¸…æ´—: å¼ºåˆ¶å¤‡æ³¨ï¼Œè‡ªåŠ¨å®¡è®¡ã€‚
"""

import streamlit as st
import datetime
import pandas as pd
from typing import Tuple

from config.settings import settings
from core.services.log_service import LogService
from ui.kernel.session import SessionManager
from ui.styles import StyleManager
from ui.components.security import SecurityGate  # [New]
from core.sys.context import set_current_function
from core.sys.logger import get_logger

app_logger = get_logger("AuditCenter")


# ==============================================================================
# UI è¾…åŠ©ç»„ä»¶
# ==============================================================================
def _render_common_filters(df: pd.DataFrame, key_prefix: str):
    with st.container(border=True):
        c1, c2 = st.columns([1, 2])
        with c1:
            today = datetime.date.today()
            date_range = st.date_input("ğŸ“… æ—¶é—´èŒƒå›´", value=(today - datetime.timedelta(days=7), today),
                                       key=f"{key_prefix}_date")
        with c2:
            all_users = sorted(df["user"].unique().tolist()) if "user" in df.columns else []
            selected_users = st.multiselect("ğŸ‘¤ ç”¨æˆ·ç­›é€‰", all_users, key=f"{key_prefix}_user")
        return date_range, selected_users


def _apply_filters(df: pd.DataFrame, date_range, selected_users) -> pd.DataFrame:
    if df.empty: return df
    if len(date_range) == 2:
        s = date_range[0].strftime("%Y-%m-%d")
        e = (date_range[1] + datetime.timedelta(days=1)).strftime("%Y-%m-%d")
        df = df[(df["time"] >= s) & (df["time"] < e)]
    if selected_users:
        df = df[df["user"].isin(selected_users)]
    return df


def _process_display_data(df: pd.DataFrame, is_unmasked: bool) -> pd.DataFrame:
    if df.empty: return df
    df_show = df.copy()
    if not is_unmasked:
        if "message" in df_show.columns:
            df_show["message"] = df_show["message"].apply(
                lambda x: (str(x)[:80] + " ... (ğŸ”’ Masked)") if len(str(x)) > 80 else x)
        cols_to_drop = ["raw_msg", "trace", "loc"]
        df_show.drop(columns=[c for c in cols_to_drop if c in df_show.columns], inplace=True)
    return df_show


def _render_purge_zone(filename: str, tab_name: str):
    """[é‡æ„] ç‰¹æƒæ¸…æ´—åŒº - é…ç½®é©±åŠ¨"""
    user = SessionManager.get_current_user()
    if user.get("username") != settings.SUPER_ADMIN_USER: return

    st.markdown("---")
    with st.expander(f"â˜¢ï¸ ç‰¹æƒåŒºåŸŸ: æ¸…æ´— [{tab_name}] æ—¥å¿—", expanded=False):
        st.error("è­¦å‘Šï¼šæ­¤æ“ä½œä¸å¯é€†ï¼å°†ç‰©ç†åˆ é™¤æŒ‡å®šèŒƒå›´å†…çš„æ—¥å¿—æ–‡ä»¶ã€‚")

        c1, c2 = st.columns(2)
        today = datetime.date.today()
        p_start = c1.date_input("å¼€å§‹æ—¥æœŸ", value=today, key=f"{tab_name}_ps")
        p_end = c2.date_input("ç»“æŸæ—¥æœŸ", value=today, key=f"{tab_name}_pe")

        # [Security Gate V5]
        # Action Key: btn_purge_business / btn_purge_infra / btn_purge_system
        # å…·ä½“çš„å¯†ç ç­–ç•¥ç”± action_registry.json å®šä¹‰ (é»˜è®¤ admin+db+sec)
        action_key = f"btn_purge_{tab_name}"

        is_approved, reason = SecurityGate.authorize_action(
            action_key=action_key,
            ui_key_suffix=tab_name,
            require_reason=True
        )

        if is_approved:
            try:
                dt_s = datetime.datetime.combine(p_start, datetime.time(0, 0))
                dt_e = datetime.datetime.combine(p_end, datetime.time(23, 59))

                count = LogService.purge_logs_by_range(
                    filename, dt_s, dt_e,
                    operator=user.get("username"),
                    reason=reason
                )

                st.success(f" æ¸…æ´—å®Œæˆï¼Œå·²åˆ é™¤ {count} æ¡è®°å½•ã€‚")
                st.rerun()
            except Exception as e:
                st.error(f"æ‰§è¡Œå¤±è´¥: {e}")


# ==============================================================================
# Tab Renderers
# ==============================================================================
# (Tab Renderers é€»è¾‘ä¿æŒç®€æ´ï¼Œä¸ V4.0 æ— å¤§å˜åŒ–ï¼Œä¸»è¦å¤ç”¨ LogService)
def render_business_tab(is_unmasked: bool):
    df = LogService.get_logs_as_df("app.log")
    date_range, sel_users = _render_common_filters(df, "app")
    action_filter = st.multiselect(" åŠ¨ä½œç­›é€‰", df["action"].unique().tolist() if "action" in df.columns else [],
                                   key="app_act")

    df_filtered = _apply_filters(df, date_range, sel_users)
    if action_filter: df_filtered = df_filtered[df_filtered["action"].isin(action_filter)]
    df_display = _process_display_data(df_filtered, is_unmasked)

    st.markdown(f"**å…± {len(df_display)} æ¡è®°å½•**")
    cols = ["time", "user", "ip", "func", "action", "message"]
    if is_unmasked and "raw_msg" in df.columns: cols.append("raw_msg")
    cols = [c for c in cols if c in df_display.columns]

    st.dataframe(df_display[cols], use_container_width=True, hide_index=True)
    _render_purge_zone("app.log", "business")  # æ³¨æ„ key å°å†™åŒ¹é… json


def render_infra_tab(is_unmasked: bool):
    df = LogService.get_logs_as_df("audit.log")
    date_range, sel_users = _render_common_filters(df, "audit")
    sql_acts = st.multiselect(" SQL åŠ¨ä½œ", ["INSERT", "UPDATE", "DELETE", "TRUNCATE", "SELECT"], key="audit_act")

    df_filtered = _apply_filters(df, date_range, sel_users)
    if sql_acts:
        pat = "|".join(sql_acts)
        df_filtered = df_filtered[df_filtered["action"].astype(str).str.upper().str.contains(pat)]
    df_display = _process_display_data(df_filtered, is_unmasked)

    st.markdown(f"**å…± {len(df_display)} æ¡è®°å½•**")
    cols = ["time", "user", "ip", "func", "action", "table", "message"]
    if is_unmasked: cols.append("raw_msg")
    cols = [c for c in cols if c in df_display.columns]
    st.dataframe(df_display[cols], use_container_width=True, hide_index=True)
    _render_purge_zone("audit.log", "infra")


def render_system_tab(is_unmasked: bool):
    df = LogService.get_logs_as_df("error.log")
    if not df.empty and "func" in df.columns:
        st.markdown("#### ğŸ“Š æ•…éšœæ¦‚è§ˆ")
        c1, c2, c3 = st.columns(3)
        c1.metric("æ€»æŠ¥é”™æ•°", len(df))
        top_module = df["func"].mode()[0] if not df["func"].empty else "-"
        c2.metric("é«˜é¢‘æ¨¡å—", top_module)
        c3.metric("æœ€è¿‘æ—¶é—´", df.iloc[0]["time"] if not df.empty else "-")
        st.divider()

    date_range, sel_users = _render_common_filters(df, "error")
    all_funcs = df["func"].unique().tolist() if "func" in df.columns else []
    sel_funcs = st.multiselect("ğŸ§© æ•…éšœåŠŸèƒ½æ¨¡å—", all_funcs, key="err_func")

    df_filtered = _apply_filters(df, date_range, sel_users)
    if sel_funcs: df_filtered = df_filtered[df_filtered["func"].isin(sel_funcs)]
    df_display = _process_display_data(df_filtered, is_unmasked)

    st.markdown(f"**å…± {len(df_display)} æ¡å¼‚å¸¸**")
    cols = ["time", "user", "func", "message"]
    if is_unmasked:
        if "trace" in df.columns: cols.append("trace")
        if "loc" in df.columns: cols.append("loc")
    cols = [c for c in cols if c in df_display.columns]
    st.dataframe(df_display[cols], use_container_width=True, hide_index=True)
    _render_purge_zone("error.log", "system")


# ==============================================================================
# Main
# ==============================================================================
def render():
    set_current_function("å®‰å…¨å®¡è®¡æ—¥å¿—")
    user = SessionManager.get_current_user()
    if not user.get("is_admin", False): st.error(" æƒé™ä¸è¶³"); return

    # Top Bar
    c_title, c_tool = st.columns([3, 1])
    with c_title:
        st.title("ğŸ“œ å®‰å…¨å®¡è®¡æ—¥å¿—")

    if "audit_unmasked" not in st.session_state: st.session_state["audit_unmasked"] = False
    current_mode = st.session_state["audit_unmasked"]

    with c_tool:
        st.write("")
        if current_mode:
            StyleManager.apply_unmasked_style()
            if st.button("ğŸ”’ éšè—æ•æ„Ÿä¿¡æ¯", use_container_width=True, type="secondary"):
                st.session_state["audit_unmasked"] = False
                st.rerun()
        else:
            if st.button("ğŸ”“ æ˜¾ç¤ºå®Œæ•´ä¿¡æ¯", use_container_width=True, type="primary"):
                st.session_state["show_auth_modal"] = True

    # Unlock Modal
    if st.session_state.get("show_auth_modal", False) and not current_mode:
        with st.container(border=True):
            st.warning("âš ï¸ æ­£åœ¨è¯·æ±‚ [ä¸Šå¸è§†è§’]ï¼šæ­¤æ“ä½œå°†å±•ç¤ºæ‰€æœ‰æ•æ„Ÿæ•°æ®ï¼Œå¹¶è¢«è®°å½•ã€‚")

            # [Security Gate V5]
            # Key: btn_unlock_view (config: admin, db, sec)
            is_approved, _ = SecurityGate.authorize_action(
                action_key="btn_unlock_view",
                require_reason=False
            )

            if st.button(" å–æ¶ˆ", use_container_width=True):
                st.session_state["show_auth_modal"] = False
                st.rerun()

            if is_approved:
                st.session_state["audit_unmasked"] = True
                st.session_state["show_auth_modal"] = False
                st.rerun()

    # Tabs
    modules_config = settings.load_modules_config()
    audit_config = next((m for m in modules_config if m['key'] == 'audit'), None)
    if not audit_config: st.error("Audit Config Missing"); return

    visible_tabs = audit_config.get('tabs', [])
    tab_labels = [t['name'] for t in visible_tabs]
    st_tabs = st.tabs(tab_labels)

    for i, t_obj in enumerate(st_tabs):
        key = visible_tabs[i]['key']
        with t_obj:
            if key == "business":
                render_business_tab(current_mode)
            elif key == "infra":
                render_infra_tab(current_mode)
            elif key == "system":
                render_system_tab(current_mode)
==================== END FILE: ui/pages/audit_logs.py ====================


==================== START FILE: ui/pages/ai_gemini.py ====================
# ui/pages/ai_gemini.py
import streamlit as st
from core.services.chat_service import ChatService
from core.sys.context import get_current_user


def render():
    st.title("ğŸ¤– AI æ™ºèƒ½åŠ©æ‰‹")
    user = get_current_user()
    svc = ChatService(user)

    # ç®€å•èŠå¤©ç•Œé¢
    msgs = svc.get_active_session()["data"]["messages"]
    for m in msgs:
        with st.chat_message(m["role"]):
            st.write(m["content"])

    if prompt := st.chat_input("Ask something..."):
        svc.add_message(svc.get_active_session()["id"], "user", prompt)
        st.rerun()
        # æ³¨æ„: å®é™… AI è°ƒç”¨é€»è¾‘éœ€å¯¹æ¥ Google APIï¼Œè¿™é‡Œä»…å±•ç¤ºç•Œé¢æ¡†æ¶
==================== END FILE: ui/pages/ai_gemini.py ====================


==================== START FILE: ui/pages/user_admin/tab_users.py ====================
# ui/pages/user_admin/tab_users.py
"""
å­æ¨¡å—: ç”¨æˆ·ä¸æƒé™ç®¡ç† (User & Permission Management) - V5.0 UI Refactor
ä¸»è¦åŠŸèƒ½:
1. å·¦ä¾§: ç”¨æˆ·åˆ—è¡¨ + ç”¨æˆ·é…ç½® (å‡é™çº§/é”å®š/é‡ç½®) + æ³¨å†Œæ–°ç”¨æˆ·ã€‚
2. å³ä¾§: ç»†ç²’åº¦æƒé™æ ‘ (ç‹¬å è§†å›¾)ã€‚
"""
import streamlit as st
import time
from typing import List, Dict

from config.settings import settings
from core.services.auth.service import AuthService
from core.services.security.inventory import SecurityInventory
from core.services.security.policy_manager import SecurityPolicyManager
from ui.components.permission_tree import PermissionTreeRenderer
from ui.components.security import SecurityGate
from core.sys.logger import get_logger

logger = get_logger("UserAdmin")


def _is_super_admin(username):
    return username == settings.SUPER_ADMIN_USER


def _load_user_list():
    df = AuthService.list_users()
    if df.empty: return df

    # [UI] çŠ¶æ€ -> ç­‰çº§
    def get_level(row):
        if _is_super_admin(row["username"]): return "ğŸ‘‘ å…¨ç«™ç®¡ç†å‘˜"
        if row["is_locked"]: return "ğŸ”´ é”å®š"
        if row["is_admin"]: return "ğŸ›¡ï¸ ç®¡ç†å‘˜"
        return "ğŸŸ¢ ç”¨æˆ·"

    df["ç­‰çº§"] = df.apply(get_level, axis=1)
    return df


def _filter_tree_by_permissions(full_tree: List[Dict], user_perms: Dict[str, bool]) -> List[Dict]:
    filtered_tree = []
    for mod in full_tree:
        mod_key = mod["key"]
        if mod_key == "public" or user_perms.get(mod_key):
            new_mod = mod.copy()
            new_mod["children"] = []
            for tab in mod.get("children", []):
                tab_key = tab["key"]
                if tab_key in user_perms:
                    new_mod["children"].append(tab)
            if new_mod["children"]:
                filtered_tree.append(new_mod)
    return filtered_tree


def render(current_user: str, am_i_super: bool):
    # å¸ƒå±€è°ƒæ•´: å·¦ä¾§ç¨å¾®çª„ä¸€ç‚¹ï¼Œå³ä¾§å®½ä¸€ç‚¹ç»™æƒé™æ ‘
    c1, c2 = st.columns([1.5, 2.5])

    # æƒé™ä¸èŒèƒ½æ£€æŸ¥
    my_perms = {}
    if not am_i_super:
        my_perms = AuthService.get_permissions(current_user)

    can_create = am_i_super or SecurityPolicyManager.get_admin_capability("can_create_user")
    can_lock = am_i_super or SecurityPolicyManager.get_admin_capability("can_lock_user")
    can_reset = am_i_super or SecurityPolicyManager.get_admin_capability("can_reset_pwd")
    can_manage_perms = am_i_super or SecurityPolicyManager.get_admin_capability("can_manage_perms")

    # =========================================================================
    # å·¦ä¾§: åˆ—è¡¨ & åŸºç¡€é…ç½®
    # =========================================================================
    with c1:
        st.subheader("ğŸ‘¥ ç”¨æˆ·åå½•")
        df_users = _load_user_list()

        # 1. ç”¨æˆ·é€‰æ‹©å™¨
        selected_user = st.selectbox(
            "ğŸ‘‰ é€‰æ‹©æ“ä½œå¯¹è±¡:",
            df_users["username"].tolist() if not df_users.empty else [],
            key="ua_user_select"
        )

        # 2. ç”¨æˆ·è¡¨æ ¼
        if not df_users.empty:
            # åªæ˜¾ç¤ºæ ¸å¿ƒåˆ—
            st.dataframe(
                df_users[["username", "ç­‰çº§"]],
                use_container_width=True,
                hide_index=True
            )

        st.write("")  # Spacer

        # 3. é…ç½®åŒºåŸŸ (æŠ˜å æ¡†ï¼Œç§»åŠ¨è‡³æ­¤)
        if selected_user:
            target_info = df_users[df_users["username"] == selected_user].iloc[0]
            t_is_super = _is_super_admin(selected_user)
            t_is_admin = target_info["is_admin"]
            t_is_locked = target_info["is_locked"]

            # çŠ¶æ€é‡ç½®: åˆ‡æ¢ç”¨æˆ·æ—¶æ¸…ç† Tree çŠ¶æ€
            if st.session_state.get("last_ua_user") != selected_user:
                keys_to_del = [k for k in st.session_state.keys() if k.startswith("ptree_")]
                for k in keys_to_del: del st.session_state[k]
                st.session_state["last_ua_user"] = selected_user
                st.rerun()

            with st.expander(f"âš™ï¸ é…ç½®: {selected_user}", expanded=True):
                # A. ä¿¡æ¯æ¦‚è§ˆ
                mc1, mc2 = st.columns(2)
                mc1.metric("å¤±è´¥æ¬¡æ•°", target_info["failed_attempts"])
                mc2.metric("å½“å‰çŠ¶æ€", "ğŸ”´ é”å®š" if t_is_locked else "ğŸŸ¢ æ­£å¸¸")

                st.divider()

                # B. è§’è‰²å‡é™çº§ (ä»… Super)
                if am_i_super:
                    if t_is_super:
                        st.info("ğŸ”’ æ— æ³•å˜æ›´å…¨ç«™ç®¡ç†å‘˜è§’è‰²")
                    else:
                        rc1, rc2 = st.columns([1, 1])
                        with rc1:
                            if t_is_admin:
                                if st.button("â¬‡ï¸ é™çº§ä¸ºç”¨æˆ·", use_container_width=True):
                                    from core.components.db.client import DBClient
                                    DBClient.execute_stmt("UPDATE User_Account SET is_admin=0 WHERE username=:u",
                                                          {"u": selected_user})
                                    st.success("å·²é™çº§");
                                    time.sleep(0.5);
                                    st.rerun()
                            else:
                                st.write("")  # å ä½
                        with rc2:
                            if not t_is_admin:
                                if st.button("â¬†ï¸ å‡çº§ä¸ºç®¡ç†å‘˜", use_container_width=True):
                                    from core.components.db.client import DBClient
                                    DBClient.execute_stmt("UPDATE User_Account SET is_admin=1 WHERE username=:u",
                                                          {"u": selected_user})
                                    st.success("å·²å‡çº§");
                                    time.sleep(0.5);
                                    st.rerun()
                            else:
                                st.write("")  # å ä½
                    st.divider()

                # C. é”å®š/è§£é”
                if can_lock and not t_is_super:
                    lc1, lc2 = st.columns(2)
                    if t_is_locked:
                        with lc1:
                            # [Gate] è§£é”
                            if SecurityGate.authorize_action("btn_unlock_user", "unlock", help_text="è§£é”è´¦å·")[0]:
                                AuthService.set_lock_state(selected_user, False)
                                logger.info(f"è§£é”: {selected_user}", extra={"action": "UNLOCK", "user": current_user})
                                st.rerun()
                    else:
                        with lc2:
                            # [Gate] é”å®š
                            if SecurityGate.authorize_action("btn_lock_user", "lock", help_text="é”å®šè´¦å·")[0]:
                                AuthService.set_lock_state(selected_user, True)
                                logger.warning(f"é”å®š: {selected_user}", extra={"action": "LOCK", "user": current_user})
                                st.rerun()
                    st.divider()

                # D. é‡ç½®å¯†ç 
                if can_reset and not t_is_super:
                    new_pwd_reset = st.text_input("é‡ç½®å¯†ç ", type="password", placeholder="è¾“å…¥æ–°å¯†ç ",
                                                  key="reset_val")
                    if SecurityGate.authorize_action("btn_reset_pwd", "reset_pwd")[0]:
                        if new_pwd_reset:
                            AuthService.reset_password(selected_user, new_pwd_reset)
                            st.success("å¯†ç å·²é‡ç½®")
                        else:
                            st.error("å¯†ç ä¸ºç©º")

        # 4. æ³¨å†Œæ–°ç”¨æˆ· (æŠ˜å æ¡†ï¼Œæ”¾åœ¨æœ€ä¸‹)
        st.write("")
        if can_create:
            with st.expander("â• æ³¨å†Œæ–°ç”¨æˆ·", expanded=False):
                new_u = st.text_input("ç”¨æˆ·å", key="new_u")
                new_p = st.text_input("å¯†ç ", type="password", key="new_p")
                is_adm = st.checkbox("è®¾ä¸ºç®¡ç†å‘˜", key="new_is_adm") if am_i_super else False

                # [Gate] æ³¨å†Œ
                is_ok, _ = SecurityGate.authorize_action(
                    action_key="btn_create_user",
                    ui_key_suffix="create_user",
                    require_reason=False
                )

                if is_ok:
                    if new_u and new_p:
                        ok, msg = AuthService.create_user(new_u, new_p, is_adm)
                        if ok:
                            st.success("åˆ›å»ºæˆåŠŸ"); time.sleep(1); st.rerun()
                        else:
                            st.error(msg)
                    else:
                        st.error("è¯·å¡«å†™å®Œæ•´")

    # =========================================================================
    # å³ä¾§: ç»†ç²’åº¦æƒé™é…ç½® (ç‹¬å )
    # =========================================================================
    with c2:
        if not selected_user: return

        # é‡æ–°åŠ è½½çŠ¶æ€åˆ¤æ–­
        # æ³¨æ„: è¿™é‡Œä¸éœ€è¦å†è¯» df_usersï¼Œç›´æ¥ç”¨ selected_user æŸ¥æƒé™å³å¯
        # ä½†ä¸ºäº†è·å– is_super/is_admin å±æ€§ï¼Œä¸Šé¢å·²ç»æŸ¥è¿‡äº† target_info

        if t_is_super:
            st.info("ğŸ‘‘ å…¨ç«™ç®¡ç†å‘˜æ‹¥æœ‰ç³»ç»Ÿæ‰€æœ‰æƒé™ï¼Œæ— éœ€é…ç½®ç»†ç²’åº¦æƒé™ã€‚")
            st.caption("å¦‚éœ€ä¿®æ”¹å…¶å¯†ç ç­–ç•¥ï¼Œè¯·å‰å¾€ [å¯†ç åŠŸèƒ½ç®¡ç†] Tabã€‚")

        elif t_is_admin and not am_i_super:
            st.warning("ğŸ›¡ï¸ æ‚¨æ— æ³•ä¿®æ”¹å…¶ä»–ç®¡ç†å‘˜çš„æƒé™é…ç½®ã€‚")

        elif not can_manage_perms:
            st.warning("ğŸš« æ‚¨å½“å‰çš„èŒèƒ½è®¾å®šä¸å…è®¸é…ç½®ç”¨æˆ·æƒé™ã€‚")

        else:
            # --- æƒé™æ ‘æ¸²æŸ“åŒº ---
            # [Fix] ç§»é™¤äº†å¤šä½™çš„ "#### æƒé™æ˜ç»†" æ ‡é¢˜ï¼Œç›´æ¥ç”± Renderer æ¸²æŸ“

            full_tree = SecurityInventory.get_full_permission_tree()

            # è¾¹ç•Œè¿‡æ»¤: å¦‚æœæˆ‘ä¸æ˜¯ Superï¼Œåªèƒ½çœ‹åˆ°æˆ‘æœ‰æƒåˆ†é…çš„æ¨¡å—
            if not am_i_super:
                display_tree = _filter_tree_by_permissions(full_tree, my_perms)
                st.info("ï¸ ä»…æ˜¾ç¤ºæ‚¨æ‹¥æœ‰çš„æƒé™æ¨¡å— (æƒé™è¾¹ç•Œé™åˆ¶)ã€‚")
            else:
                display_tree = full_tree

            # è·å–ç›®æ ‡ç”¨æˆ·å½“å‰æƒé™
            target_perms = AuthService.get_permissions(selected_user)

            # æ¸²æŸ“ç»„ä»¶
            renderer = PermissionTreeRenderer(target_perms, session_prefix="ptree")
            renderer.render(display_tree)

            st.markdown("---")

            # [Gate] ä¿å­˜æƒé™
            is_ok_perm, _ = SecurityGate.authorize_action(
                action_key="btn_update_perms",
                ui_key_suffix="save_perms",
                require_reason=False
            )

            if is_ok_perm:
                selected_keys = renderer.get_selected_keys(display_tree)
                perms_map = {k: True for k in selected_keys}

                # å¢é‡æ›´æ–°é€»è¾‘
                if not am_i_super:
                    visible_keys = []

                    def extract_keys(nodes):
                        for n in nodes:
                            visible_keys.append(n["key"])
                            extract_keys(n.get("children", []))

                    extract_keys(display_tree)

                    final_perms = target_perms.copy()
                    for vk in visible_keys:
                        if vk in perms_map:
                            final_perms[vk] = True
                        else:
                            if vk in final_perms: del final_perms[vk]
                    AuthService.set_permissions(selected_user, final_perms)
                else:
                    AuthService.set_permissions(selected_user, perms_map)

                logger.info(f"æƒé™æ›´æ–°: {selected_user}", extra={"action": "UPDATE_PERMS", "user": current_user})
                st.success("æƒé™å·²ä¿å­˜ï¼")
                time.sleep(1);
                st.rerun()
==================== END FILE: ui/pages/user_admin/tab_users.py ====================


==================== START FILE: ui/pages/user_admin/tab_policies.py ====================
# ui/pages/user_admin/tab_policies.py
"""
å­æ¨¡å—: å¯†ç åŠŸèƒ½ç®¡ç† (Security Policy Management) - V2.2 Final Fix
ä¸»è¦åŠŸèƒ½:
1. å…¨ç«™ç®¡ç†å‘˜åœ¨æ­¤é…ç½®æ¯ä¸ªåŠ¨ä½œéœ€è¦éªŒè¯çš„å¯†ç ç»„åˆã€‚
2. æ”¯æŒ User/Query/Modify/DB/System äº”ç§å®‰å…¨çº§åˆ«ã€‚
"""
import streamlit as st
import time
from core.services.security.inventory import SecurityInventory
from core.services.security.policy_manager import SecurityPolicyManager


def render():
    """æ¸²æŸ“å…¥å£å‡½æ•°"""
    st.info("ğŸ’¡åœ¨æ­¤é…ç½®å…¨ç«™æ¯ä¸ªå…³é”®åŠ¨ä½œçš„å¯†ç éªŒè¯ç­‰çº§ã€‚æ›´æ”¹å³æ—¶ç”Ÿæ•ˆã€‚")

    # è·å–æ‰€æœ‰åŠ¨ä½œ
    actions = SecurityInventory.get_flat_action_list()

    # [æ ¸å¿ƒ] å®šä¹‰å¯é€‰çš„å®‰å…¨ä»¤ç‰Œ
    TOKEN_OPTIONS = ["user", "query", "modify", "db", "system"]

    TOKEN_LABELS = {
        "user": "å½“å‰ç”¨æˆ·å¯†ç ",
        "query": "L1-æŸ¥è¯¢å®‰ä¿ç ",
        "modify": "L2-ä¿®æ”¹å®‰ä¿ç ",
        "db": "L3-æ•°æ®åº“ç®¡ç†ç ",
        "system": "L4-ç³»ç»Ÿç®¡ç†ç "
    }

    with st.container(border=True):
        st.markdown(f"**å…± {len(actions)} ä¸ªå—æ§åŠ¨ä½œç‚¹**")
        # æŒ‰æ¨¡å—åˆ†ç»„
        modules = sorted(list(set(a["module_name"] for a in actions if a["module_name"])))

        for mod in modules:
            with st.expander(f"ğŸ“¦ {mod}", expanded=False):
                mod_actions = [a for a in actions if a["module_name"] == mod]
                for act in mod_actions:
                    c1, c2 = st.columns([2, 3])
                    key = act["key"]
                    name = act["name"]

                    # è·å–å½“å‰ç­–ç•¥ (ä¼˜å…ˆ Override)
                    current_tokens = SecurityPolicyManager.get_required_tokens(key)

                    with c1:
                        st.markdown(f"**{name}**")
                        st.caption(f"ID: `{key}`")
                        if act.get("description"):
                            st.caption(f"ğŸ“ {act['description']}")

                    with c2:
                        new_tokens = st.multiselect(
                            "éªŒè¯è¦æ±‚",
                            options=TOKEN_OPTIONS,
                            default=current_tokens,
                            key=f"pol_{key}",
                            format_func=lambda x: TOKEN_LABELS.get(x, x.upper())
                        )

                        # æ£€æŸ¥å˜æ›´
                        if set(new_tokens) != set(current_tokens):
                            if st.button("ğŸ’¾ æ›´æ–°ç­–ç•¥", key=f"save_{key}"):
                                SecurityPolicyManager.save_policy_override(key, new_tokens)
                                st.toast(f"ç­–ç•¥å·²æ›´æ–°: {name}")
                                time.sleep(0.5);
                                st.rerun()
                    st.divider()
==================== END FILE: ui/pages/user_admin/tab_policies.py ====================


==================== START FILE: ui/pages/user_admin/__init__.py ====================
# ui/pages/user_admin/__init__.py
"""
æ–‡ä»¶è¯´æ˜: ç”¨æˆ·ä¸æƒé™ç®¡ç†å…¥å£ (User Admin Entry)
ä¸»è¦åŠŸèƒ½:
1. è°ƒåº¦ Tab å­æ¨¡å—ã€‚
2. æƒé™åˆ¤æ–­ä¸ä¸Šä¸‹æ–‡è®¾ç½®ã€‚
"""

import streamlit as st
from config.settings import settings
from ui.kernel.session import SessionManager
from core.sys.context import set_current_function

# å¯¼å…¥å­æ¨¡å—
from . import tab_users
from . import tab_policies
from . import tab_functions


def _is_super_admin(username):
    return username == settings.SUPER_ADMIN_USER


def render():
    set_current_function("ç”¨æˆ·æƒé™ç®¡ç†")
    st.title("ğŸ‘¤ ç”¨æˆ·ä¸æƒé™ç®¡ç†ä¸­å¿ƒ")

    user = SessionManager.get_current_user()
    if not user:
        st.error("è¯·å…ˆç™»å½•")
        return

    username = user.get("username")
    is_super = _is_super_admin(username)

    # å®šä¹‰ Tabs
    tabs = ["ğŸ‘¥ ç”¨æˆ·ä¸æƒé™é…ç½®"]
    if is_super:
        tabs.append("ğŸ” å¯†ç åŠŸèƒ½ç®¡ç†")
        tabs.append("ğŸ›¡ï¸ èŒèƒ½ç®¡ç†")

    st_tabs = st.tabs(tabs)

    # Tab 1: ç”¨æˆ·ç®¡ç† (æ‰€æœ‰äººå¯è§ï¼Œå†…éƒ¨æœ‰æƒé™æ§åˆ¶)
    with st_tabs[0]:
        tab_users.render(username, is_super)

    # Tab 2: å¯†ç ç­–ç•¥ (ä»… Super)
    if is_super and len(st_tabs) > 1:
        with st_tabs[1]:
            tab_policies.render()

    # Tab 3: èŒèƒ½ç®¡ç† (ä»… Super)
    if is_super and len(st_tabs) > 2:
        with st_tabs[2]:
            tab_functions.render()
==================== END FILE: ui/pages/user_admin/__init__.py ====================


==================== START FILE: ui/pages/user_admin/tab_functions.py ====================
# ui/pages/user_admin/tab_functions.py
"""
å­æ¨¡å—: èŒèƒ½ç®¡ç† (Admin Capabilities)
"""
import streamlit as st
import time
from core.services.security.policy_manager import SecurityPolicyManager


def render():
    st.info("ğŸ’¡ ç®¡ç† 'æ™®é€šç®¡ç†å‘˜' çš„å…·ä½“ç®¡ç†èŒèƒ½ã€‚å…³é—­åï¼Œæ™®é€šç®¡ç†å‘˜å°†æ— æ³•çœ‹åˆ°å¯¹åº”çš„æ“ä½œæŒ‰é’®ã€‚")

    caps = {
        "can_create_user": "å…è®¸æ³¨å†Œæ–°ç”¨æˆ·",
        "can_lock_user": "å…è®¸é”å®š/è§£é”è´¦å·",
        "can_reset_pwd": "å…è®¸é‡ç½®ç”¨æˆ·å¯†ç ",
        "can_manage_perms": "å…è®¸é…ç½®ç”¨æˆ·æƒé™"
    }

    with st.container(border=True):
        st.markdown("#### ğŸ›¡ï¸ ç®¡ç†å‘˜èŒèƒ½å¼€å…³")
        for key, label in caps.items():
            c1, c2 = st.columns([3, 1])
            is_enabled = SecurityPolicyManager.get_admin_capability(key)

            with c1:
                st.write(f"**{label}**")
                st.caption(f"Key: `{key}`")

            with c2:
                new_val = st.toggle("å¯ç”¨", value=is_enabled, key=f"cap_{key}")
                if new_val != is_enabled:
                    if st.button(f"ä¿å­˜å˜æ›´ ({key})"):
                        SecurityPolicyManager.update_admin_capability(key, new_val)
                        st.success("å·²æ›´æ–°");
                        time.sleep(0.5);
                        st.rerun()
            st.divider()
==================== END FILE: ui/pages/user_admin/tab_functions.py ====================


==================== START FILE: ui/pages/home/__init__.py ====================
# ui/pages/home/__init__.py
"""
æ–‡ä»¶è¯´æ˜: ç³»ç»Ÿé¦–é¡µ (Home Page)
ä¸»è¦åŠŸèƒ½:
1. æ³¨å…¥å½“å‰åŠŸèƒ½ä¸Šä¸‹æ–‡ (For Logging)ã€‚
2. æ˜¾ç¤ºæ¬¢è¿æ¨ªå¹… (Big Logo)ã€‚
3. æ˜¾ç¤ºè‡ªåŠ¨æ»šåŠ¨çš„ Patch Notes (æ›´æ–°æ—¥å¿—)ã€‚
"""

import streamlit as st
import streamlit.components.v1 as components
from config.settings import settings
from ui.kernel.layout import get_base64_encoded_file
from core.sys.context import set_current_function


def _parse_text_to_html(raw_text):
    """æ–‡æœ¬è½¬ HTML ç®€å•æ ¼å¼åŒ–"""
    raw_text = raw_text.strip()
    lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
    if not lines: return ""

    html_parts = []
    # æ ‡é¢˜ (ç¬¬ä¸€è¡Œ)
    title = lines[0]
    html_parts.append(f"<b>{title}</b>")

    # åˆ—è¡¨ (åç»­è¡Œ)
    if len(lines) > 1:
        html_parts.append("<ul>")
        for line in lines[1:]:
            content = line.lstrip("-").lstrip("*").strip()
            html_parts.append(f"<li>{content}</li>")
        html_parts.append("</ul>")

    return "".join(html_parts)


def render():
    # [Context] æ³¨å…¥å½“å‰åŠŸèƒ½å (ç”¨äºå®¡è®¡æ—¥å¿—)
    set_current_function("ç³»ç»Ÿé¦–é¡µ")

    # 1. é¡¶éƒ¨å¸ƒå±€
    logo_path = settings.ASSETS_DIR / "Logo.png"
    logo_b64 = get_base64_encoded_file(logo_path)

    col_left, col_right = st.columns([1.4, 1])

    with col_left:
        # å·¦ä¾§ï¼šæ¬¢è¿åŒºåŸŸ
        img_tag = f'<img src="data:image/png;base64,{logo_b64}" class="big-logo" style="width: 80%; max-width: 380px;">' if logo_b64 else "<h1>ğŸ¦…</h1>"

        st.markdown(f"""
            <div style="display: flex; flex-direction: column; align-items: center; justify-content: center; height: 70vh; text-align: center;">
                {img_tag}
                <div style="font-size: 36px; font-weight: 200; color: #FFFFFF; letter-spacing: 4px; margin: 20px 0 10px 0;">Eaglestar Pro</div>
                <div style="font-size: 16px; color: rgba(255, 255, 255, 0.5); font-family: monospace; background: rgba(0,0,0,0.3); padding: 8px 20px; border-radius: 20px; border: 1px solid rgba(255,255,255,0.05);">
                    â¬…ï¸ è¯·åœ¨å·¦ä¾§å¯¼èˆªæ é€‰æ‹©åŠŸèƒ½æ¨¡å—
                </div>
            </div>
        """, unsafe_allow_html=True)

    with col_right:
        # å³ä¾§ï¼šPatch Note é¢æ¿ (å¤åˆ» V1.5.3 æ ·å¼)
        st.markdown("""
            <style>
            .patch-panel {
                background-color: rgba(25, 25, 25, 0.6);
                border: 1px solid rgba(255, 255, 255, 0.1);
                border-top: 3px solid #4EC9B0; border-radius: 6px; 
                height: 65vh; overflow-y: scroll; padding: 0;
                margin-top: 2vh;
                box-shadow: 0 10px 30px rgba(0,0,0,0.3); backdrop-filter: blur(10px);
                scrollbar-width: none; -ms-overflow-style: none;
            }
            .patch-panel::-webkit-scrollbar { display: none; }
            .patch-header {
                position: sticky; top: 0; background-color: rgba(30, 30, 30, 0.95);
                padding: 15px 20px; border-bottom: 1px solid rgba(255,255,255,0.1);
                font-size: 16px; font-weight: 700; color: #4EC9B0;
                display: flex; justify-content: space-between; align-items: center; z-index: 10;
            }
            .patch-content { padding: 20px; }
            .patch-item {
                margin-bottom: 25px; position: relative; padding-left: 20px;
                border-left: 2px solid rgba(255,255,255,0.1);
            }
            .patch-ver { display: block; font-size: 15px; font-weight: bold; color: #E0E0E0; margin-bottom: 4px; }
            .patch-date { font-size: 12px; color: #666; margin-left: 10px; font-weight: normal; }
            .patch-desc { font-size: 13px; color: #B0B0B0; line-height: 1.6; }
            .patch-desc ul { padding-left: 18px; margin: 5px 0; }
            .patch-desc li { margin-bottom: 4px; }
            </style>
        """, unsafe_allow_html=True)

        # å‡†å¤‡ HTML å†…å®¹
        patch_list = settings.PATCH_NOTES_LIST
        # æŒ‰æ—¥æœŸå€’åº (æœ€æ–°çš„åœ¨æœ€å‰) -> ä½†ä¸ºäº†æ»šåŠ¨é€šå¸¸æ˜¯æ­£åº
        # è¿™é‡Œæˆ‘ä»¬æŒ‰åŸé€»è¾‘
        try:
            # ç®€å•æŒ‰æ—¥æœŸå­—ç¬¦ä¸²æ’åº
            patch_list = sorted(patch_list, key=lambda x: x['date'])
        except:
            pass

        items_html = ""
        if not patch_list:
            items_html = "<div style='padding:20px; color:#666;'>æš‚æ— æ›´æ–°è®°å½•</div>"
        else:
            for note in patch_list:
                formatted_desc = _parse_text_to_html(note['desc'])
                items_html += f'<div class="patch-item"><span class="patch-ver">{note["ver"]} <span class="patch-date">{note["date"]}</span></span><div class="patch-desc">{formatted_desc}</div></div>'

        # åŒå€å†…å®¹ä»¥å®ç°æ— ç¼æ»šåŠ¨
        double_content = items_html + items_html if patch_list else items_html

        final_html = f'''
        <div class="patch-panel" id="auto-scroll-panel">
            <div class="patch-header">
                <span>ğŸ“… æ›´æ–°æ—¥å¿— (Changelog)</span>
                <span style="font-size:12px; opacity:0.6;">Current: {settings.APP_VERSION}</span>
            </div>
            <div class="patch-content">{double_content}</div>
        </div>
        '''
        st.markdown(final_html, unsafe_allow_html=True)

        # JS è‡ªåŠ¨æ»šåŠ¨
        js_code = """
        <script>
            function setupAutoScroll() {
                const panel = window.parent.document.getElementById('auto-scroll-panel');
                if (!panel) { setTimeout(setupAutoScroll, 100); return; }
                let isPaused = false;
                let resumeTimeout = null;
                const speed = 0.3; 
                let scrollAccumulator = panel.scrollTop;
                function step() {
                    if (!isPaused) {
                        scrollAccumulator += speed;
                        panel.scrollTop = scrollAccumulator;
                        if (panel.scrollTop >= (panel.scrollHeight / 2)) {
                            scrollAccumulator = 0;
                            panel.scrollTop = 0;
                        }
                    } else {
                        scrollAccumulator = panel.scrollTop;
                    }
                    requestAnimationFrame(step);
                }
                const pause = () => { isPaused = true; if (resumeTimeout) clearTimeout(resumeTimeout); };
                const resume = () => {
                    if (resumeTimeout) clearTimeout(resumeTimeout);
                    resumeTimeout = setTimeout(() => { isPaused = false; }, 1000);
                };
                panel.addEventListener('mouseenter', pause);
                panel.addEventListener('mouseleave', resume);
                panel.addEventListener('wheel', () => { pause(); resume(); });
                requestAnimationFrame(step);
            }
            setupAutoScroll();
        </script>
        """
        components.html(js_code, height=0)
==================== END FILE: ui/pages/home/__init__.py ====================


==================== START FILE: ui/pages/etl_ingest/trans_wizard.py ====================
# ui/pages/etl_ingest/trans_wizard.py
"""
æ–‡ä»¶è¯´æ˜: äº¤æ˜“æ•°æ®å¤„ç†å‘å¯¼ UI - V2.5 Fix Phantom Error
ä¸»è¦åŠŸèƒ½:
1. æ•´åˆ Ingest, Parser, Transformer, Correction å››å¤§æœåŠ¡ã€‚
2. [Fix] è‡ªæ„ˆæœºåˆ¶: å¦‚æœ parser æ ‡è®°äº†é”™è¯¯ä½† UI å‘ç° SKU å…¶å®æ˜¯æœ‰æ•ˆçš„ï¼Œè‡ªåŠ¨ä¿®å¤ã€‚
3. [Fix] äº¤äº’ä¼˜åŒ–: é¿å…æ‰‹åŠ¨å½•å…¥æ—¶é»˜è®¤å€¼ä¸º 'è§£æå¤±è´¥'ã€‚
"""

import streamlit as st
import time
import io
from typing import List, Tuple

from core.services.etl.ingest import IngestService
from core.services.etl.parser import TransactionParser
from core.services.etl.transformer import TransactionTransformer
from core.services.correction import CorrectionService
from ui.components.security import SecurityGate
from ui.kernel.session import SessionManager
from core.sys.lock_manager import LockManager

LOCK_RESOURCE_KEY = "Data_Transaction"


class TransactionWizard:

    def __init__(self):
        self.ingest_svc = IngestService()
        self.parser_svc = TransactionParser()
        self.transform_svc = TransactionTransformer()
        self.correction_svc = CorrectionService()
        self._init_state()

    def _init_state(self):
        if 'etl_stage' not in st.session_state:
            st.session_state.etl_stage = 'UPLOAD'
        if 'etl_pending_count' not in st.session_state:
            st.session_state.etl_pending_count = 0

    def render(self):
        user_info = SessionManager.get_current_user()
        username = user_info.get("username") if user_info else "Unknown"

        can_access, msg = LockManager.check_access(LOCK_RESOURCE_KEY, username)
        if not can_access:
            st.error(msg)
            st.info("ğŸ’¡ äº¤æ˜“æ•°æ®è¡¨ (Data_Transaction) æ˜¯å…¨å±€å…±äº«èµ„æºï¼ŒåŒä¸€æ—¶é—´åªèƒ½ç”±ä¸€åç®¡ç†å‘˜æ“ä½œã€‚")
            return

        stage = st.session_state.etl_stage

        if stage == 'UPLOAD':
            self._render_upload_step(username)
        elif stage == 'CLEANING':
            self._render_cleaning_step()
        elif stage == 'DONE':
            self._render_done_step(username)

    def _safe_str(self, val) -> str:
        if val is None: return ""
        s = str(val).strip()
        if s.lower() in ['none', 'nan', 'null', '']: return ""
        return s

    def _classify_files(self, files: List) -> Tuple[List, List]:
        trans_files = []
        earn_files = []
        for f in files:
            try:
                head = f.read(2048).decode('utf-8', errors='ignore').lower()
                f.seek(0)
                if "transaction report" in head:
                    trans_files.append(f)
                elif "order earnings report" in head:
                    earn_files.append(f)
                else:
                    st.warning(f"âš ï¸ æ— æ³•è¯†åˆ«æ–‡ä»¶ç±»å‹ï¼Œå·²è·³è¿‡: {f.name}")
            except Exception as e:
                st.error(f"æ–‡ä»¶è¯»å–é”™è¯¯ {f.name}: {e}")
        return trans_files, earn_files

    # --- Step 1: Upload ---
    def _render_upload_step(self, username: str):
        st.markdown("#### 1ï¸âƒ£ æ•°æ®ä¸Šä¼  (Upload)")
        st.info("è¯·ä¸Šä¼  eBay åŸå§‹æŠ¥è¡¨ã€‚")

        uploaded_files = st.file_uploader(
            "æ‹–æ‹½æˆ–ç‚¹å‡»ä¸Šä¼  CSV æ–‡ä»¶ (æ”¯æŒå¤šé€‰)",
            type=['csv'],
            accept_multiple_files=True,
            key="mixed_uploader"
        )

        st.markdown("---")

        is_approved, _ = SecurityGate.authorize_action(
            action_key="btn_start_etl_pipeline",
            ui_key_suffix="start_etl",
            require_reason=False,
            help_text="éªŒè¯é€šè¿‡åå°†æ¸…ç©ºæ—§æ•°æ®å¹¶å¼€å§‹å¤„ç†ã€‚"
        )

        if is_approved:
            if not uploaded_files:
                st.error(" è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶")
                return

            locked, lock_msg = LockManager.acquire_lock(LOCK_RESOURCE_KEY, username, "ETLäº¤æ˜“æ•°æ®å¤„ç†")
            if not locked:
                st.error(f" åŠ é”å¤±è´¥: {lock_msg}")
                return

            trans_files, earn_files = self._classify_files(uploaded_files)

            if not trans_files and not earn_files:
                st.error(" æœªè¯†åˆ«åˆ°æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶ã€‚")
                LockManager.release_lock(LOCK_RESOURCE_KEY, username)
                return

            st.success(f" è¯†åˆ«ç»“æœ: Transaction x {len(trans_files)} | Earning x {len(earn_files)}")
            time.sleep(1)

            self._run_pipeline_phase_1(trans_files, earn_files)

    def _run_pipeline_phase_1(self, trans_files, earn_files):
        with st.status("æ­£åœ¨å¤„ç†æ•°æ®...", expanded=True) as status:
            status.write("ğŸ“¥ [Ingest] æ­£åœ¨è¯»å–å¹¶å¯¼å…¥åŸå§‹æ•°æ®...")
            self.ingest_svc.run_ingest_pipeline(trans_files, earn_files)

            status.write("ğŸ§© [Parser] æ­£åœ¨è§£æ SKU ç»“æ„...")
            res = self.parser_svc.run()

            pending = res.get("pending_count", 0)
            fixed = len(res.get("auto_fixed", []))
            status.write(f"   â””â”€  è‡ªåŠ¨ä¿®å¤: {fixed} æ¡ | âš ï¸ å¾…äººå·¥å¤„ç†: {pending} æ¡")

            if pending > 0:
                st.session_state.etl_pending_count = pending
                st.session_state.etl_stage = 'CLEANING'
                status.update(label="âš ï¸ å‘ç°å¼‚å¸¸æ•°æ®ï¼Œéœ€äººå·¥æ¸…æ´—", state="error")
                time.sleep(1)
                st.rerun()
            else:
                self._run_pipeline_phase_2(status)

    # --- Step 2: Cleaning ---
    def _render_cleaning_step(self):
        count = st.session_state.etl_pending_count
        st.markdown(f"#### 2ï¸âƒ£ æ•°æ®æ¸…æ´— (Cleaning) - å‰©ä½™: {count}")

        row = self.correction_svc.get_next_pending_issue()

        if row is None:
            st.success("ğŸ‰ æ‰€æœ‰å¼‚å¸¸å·²ä¿®å¤ï¼")
            if st.button("â¡ï¸ ç»§ç»­æ‰§è¡Œè½¬æ¢", type="primary"):
                with st.status("æ­£åœ¨å®Œæˆå‰©ä½™æ­¥éª¤...", expanded=True) as status:
                    self._run_pipeline_phase_2(status)
            return

        self._render_fix_card(row)

    def _render_fix_card(self, row):
        order_id = row['Order number']
        label = row['Custom label']
        title = row['Item title']

        bad_sku = "PARSE_FAILED"
        bad_qty = "0"
        bad_idx = 1
        found_issue = False

        # 1. æŸ¥æ‰¾é”™è¯¯ä½ç½®
        for i in range(1, 11):
            s = self._safe_str(row.get(f'P_SKU{i}'))
            q = self._safe_str(row.get(f'P_Quantity{i}'))
            if s:
                if not self.correction_svc.is_valid_sku(s):
                    bad_sku = s
                    bad_qty = q
                    bad_idx = i
                    found_issue = True
                    break

        # [Fix] è‡ªæ„ˆé€»è¾‘: å¦‚æœ Flag=99 ä½†æ‰¾ä¸åˆ°ä»»ä½•æ— æ•ˆ SKUï¼Œè¯´æ˜å®ƒå·²ç»ä¿®å¥½äº† (æ•°æ®åº“æ»å)
        if not found_issue:
            # åˆ¤æ–­æ˜¯å¦çœŸçš„æ˜¯ç©ºæ•°æ® (è§£æå®Œå…¨å¤±è´¥) è¿˜æ˜¯ phantom error
            has_any_sku = False
            for i in range(1, 11):
                if self._safe_str(row.get(f'P_SKU{i}')):
                    has_any_sku = True
                    break

            if has_any_sku:
                # æœ‰ SKU ä¸”éƒ½æœ‰æ•ˆ -> Phantom Error -> Auto Skip
                st.toast(f"æ£€æµ‹åˆ°è®¢å• {order_id} æ•°æ®å·²ä¿®å¤ï¼Œè‡ªåŠ¨è·³è¿‡...")
                self.correction_svc.mark_as_skipped(order_id)
                time.sleep(0.5)
                st.rerun()
                return
            else:
                # çœŸçš„æ²¡è§£æå‡ºæ¥
                bad_sku = "è§£æå¤±è´¥(ç©ºæ•°æ®)"
                bad_qty = "0"
                bad_idx = 1

        with st.container(border=True):
            st.error(f" å¼‚å¸¸å†…å®¹: **{bad_sku}** (Qty: {bad_qty})")

            c_info1, c_info2 = st.columns([2, 1])
            with c_info1:
                st.caption("åŸå§‹æ ‡ç­¾ (Custom Label)")
                st.code(label, language="text")
            with c_info2:
                st.caption("å…³è”è®¢å•")
                st.write(order_id)
                st.caption("å•†å“æ ‡é¢˜")
                st.write(title[:50] + "..." if len(str(title)) > 50 else title)

            st.divider()

            if bad_sku == "è§£æå¤±è´¥(ç©ºæ•°æ®)":
                radio_opts = ["âœï¸ æ‰‹åŠ¨å½•å…¥"]
                sel_idx = 0
            else:
                suggestions = self.correction_svc.get_fuzzy_suggestions(bad_sku)
                radio_opts = suggestions + ["âœï¸ æ‰‹åŠ¨å½•å…¥"]
                sel_idx = 0

            selection = st.radio("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:", radio_opts, index=sel_idx, horizontal=True, key=f"rad_{order_id}")

            c_in1, c_in2 = st.columns(2)
            with c_in1:
                # [Fix] å¦‚æœæ˜¯æ‰‹åŠ¨å½•å…¥ï¼Œä¸”åŸå†…å®¹æ˜¯"è§£æå¤±è´¥"ï¼Œåˆ™æ¸…ç©ºè¾“å…¥æ¡†ï¼Œæ–¹ä¾¿ç”¨æˆ·è¾“å…¥
                default_sku = "" if selection == "âœï¸ æ‰‹åŠ¨å½•å…¥" else selection
                new_sku = st.text_input("æ­£ç¡® SKU", value=default_sku, key=f"fix_sku_{order_id}").strip().upper()

            with c_in2:
                # [Fix] å¦‚æœåŸæ•°é‡æ˜¯æ— æ•ˆçš„ï¼Œé»˜è®¤ç»™ 1
                try:
                    float(bad_qty)
                    default_qty = bad_qty
                except:
                    default_qty = "1"
                new_qty = st.text_input("æ­£ç¡®æ•°é‡", value=default_qty, key=f"fix_qty_{order_id}").strip()

            st.write("")

            is_ok, _ = SecurityGate.authorize_action("btn_commit_sku_fix", ui_key_suffix=f"fix_{order_id}")

            if is_ok:
                if not new_sku:
                    st.error("SKU ä¸èƒ½ä¸ºç©º")
                elif not self.correction_svc.is_valid_sku(new_sku):
                    st.error(f"SKU [{new_sku}] æ— æ•ˆ (èµ„æ–™åº“ä¸­ä¸å­˜åœ¨)ï¼Œè¯·æ£€æŸ¥æ‹¼å†™ã€‚")
                else:
                    try:
                        q_val = float(new_qty)
                        if q_val <= 0: raise ValueError

                        if self.correction_svc.apply_fix_transactional(
                                order_id, bad_idx, label, bad_sku, bad_qty, new_sku, str(int(q_val))
                        ):
                            st.toast(f"å·²ä¿®å¤: {new_sku}")
                            time.sleep(0.5)
                            st.rerun()
                        else:
                            st.error("ä¿®å¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
                    except:
                        st.error(f"æ•°é‡æ— æ•ˆ: '{new_qty}' (å¿…é¡»æ˜¯å¤§äº0çš„æ•°å­—)")

    def _run_pipeline_phase_2(self, status_container):
        status_container.write("âš™ï¸ [Transformer] æ­£åœ¨è®¡ç®—åˆ©æ¶¦ä¸åˆ†æ‘Š...")
        try:
            self.transform_svc.run()
            status_container.update(label=" ETL æµç¨‹å…¨éƒ¨å®Œæˆï¼", state="complete")
            st.session_state.etl_stage = 'DONE'
            time.sleep(1)
            st.rerun()
        except Exception as e:
            status_container.update(label=" è½¬æ¢å¤±è´¥", state="error")
            st.error(f"Error: {e}")

    def _render_done_step(self, username: str):
        st.balloons()
        st.success("ğŸ‰ æ•°æ®å¤„ç†å®Œæˆï¼Clean Log å·²ç”Ÿæˆã€‚")
        LockManager.release_lock(LOCK_RESOURCE_KEY, username)

        c1, c2 = st.columns(2)
        with c1: st.info("ğŸ‘‰ å‰å¾€ [å•†ä¸šæ™ºèƒ½æŠ¥è¡¨] æŸ¥çœ‹åˆ†æç»“æœ")
        with c2:
            if st.button("ğŸ”„ å¼€å§‹æ–°ä¸€è½®ä¸Šä¼ "):
                st.session_state.etl_stage = 'UPLOAD'
                st.rerun()
==================== END FILE: ui/pages/etl_ingest/trans_wizard.py ====================


==================== START FILE: ui/pages/etl_ingest/__init__.py ====================
# ui/pages/etl_ingest/__init__.py
"""
æ–‡ä»¶è¯´æ˜: ETL æ•°æ®é›†æˆé¡µé¢å…¥å£
ä¸»è¦åŠŸèƒ½:
1. åŒ…å« "äº¤æ˜“æ•°æ®å¤„ç†" å’Œ "åº“å­˜åŒæ­¥" ä¸¤ä¸ª Tabã€‚
2. åˆ†åˆ«è°ƒç”¨ TransactionWizard å’Œ InventoryWizard ç»„ä»¶ã€‚
"""

import streamlit as st
from config.settings import settings
from ui.pages.etl_ingest.trans_wizard import TransactionWizard
from ui.pages.etl_ingest.inv_wizard import InventoryWizard

def render():
    st.title("ğŸ”Œ æ•°æ®é›†æˆ (Data Integration)")
    st.caption(f"Enterprise ETL Pipeline {settings.APP_VERSION}")

    # åˆ†é¡µå¯¼èˆª
    tab_trans, tab_inv = st.tabs(["ğŸ“Š äº¤æ˜“æ•°æ® (Transaction)", "ğŸ“¦ åº“å­˜åŒæ­¥ (Inventory)"])

    # Tab 1: äº¤æ˜“æ•°æ®
    with tab_trans:
        wizard = TransactionWizard()
        wizard.render()

    # Tab 2: åº“å­˜åŒæ­¥
    with tab_inv:
        inv_wizard = InventoryWizard()
        inv_wizard.render()
==================== END FILE: ui/pages/etl_ingest/__init__.py ====================


==================== START FILE: ui/pages/etl_ingest/inv_wizard.py ====================
# ui/pages/etl_ingest/inv_wizard.py
"""
æ–‡ä»¶è¯´æ˜: åº“å­˜åŒæ­¥å‘å¯¼ UI - V3.0 Locked
ä¸»è¦åŠŸèƒ½:
1. æä¾›åº“å­˜ CSV ä¸Šä¼ å…¥å£ã€‚
2. è°ƒç”¨åç«¯æœåŠ¡æ ¡éªŒ SKU åˆæ³•æ€§ã€‚
3. [Fix] æ—¥æœŸç²¾åº¦: æ”¯æŒç²¾ç¡®åˆ°æ—¥çš„åº“å­˜å¿«ç…§ã€‚
4. [New] å¹¶å‘æ§åˆ¶: å†™å…¥æ•°æ®åº“å‰è·å– 'Data_Inventory' é”ï¼Œé˜²æ­¢å¹¶å‘å†™å…¥å¯¼è‡´åº“å­˜æ•°æ®é”™ä¹±ã€‚
"""

import streamlit as st
from datetime import date
import time

from config.settings import settings
from core.services.inventory.service import InventoryService
from ui.components.security import SecurityGate
from ui.kernel.session import SessionManager
from core.sys.lock_manager import LockManager  # [New]

# å®šä¹‰èµ„æº Key (å¯¹åº” LockManager ä¸­çš„æ ‡å‡†)
LOCK_KEY_INV = "Data_Inventory"


class InventoryWizard:

    def __init__(self):
        self.svc = InventoryService()
        self._init_state()

    def _init_state(self):
        """åˆå§‹åŒ– UI ä¼šè¯çŠ¶æ€"""
        if 'inv_df_clean' not in st.session_state:
            st.session_state.inv_df_clean = None
        if 'inv_target_date' not in st.session_state:
            st.session_state.inv_target_date = None

    def render(self):
        # [New] 1. é”æ£€æŸ¥ (Pre-check)
        user_info = SessionManager.get_current_user()
        username = user_info.get("username") if user_info else "Unknown"

        can, msg = LockManager.check_access(LOCK_KEY_INV, username)
        if not can:
            st.error(msg)
            st.info("ğŸ’¡ åº“å­˜è¡¨æ­£åœ¨è¢«å…¶ä»–ç”¨æˆ·æ“ä½œï¼Œè¯·ç¨åé‡è¯•ã€‚")
            return

        st.subheader("ğŸ“¦ åº“å­˜åŒæ­¥ (Inventory Sync)")
        st.info("ğŸ’¡ è¯´æ˜: ä¸Šä¼ ç›˜ç‚¹ CSV æ–‡ä»¶ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æ›´æ–° `Data_Inventory` è¡¨ã€‚")

        # --- Step 1: æ–‡ä»¶ä¸Šä¼  ---
        col1, col2 = st.columns([2, 1])
        with col1:
            uploaded_file = st.file_uploader("é€‰æ‹© CSV æ–‡ä»¶ (éœ€åŒ…å« SKU, Quantity)", type=['csv'], key="inv_uploader")

        with col2:
            # é»˜è®¤å½’å±æ—¥æœŸä¸ºä»Šå¤©
            default_date = date.today()
            target_date = st.date_input("å½’å±æ—¥æœŸ (Target Date)", value=default_date)
            # [Fix] æ ¼å¼åŒ–ä¸º YYYY-MM-DDï¼Œä¿ç•™ç²¾ç¡®æ—¥æœŸ
            target_date_str = target_date.strftime("%Y-%m-%d")

        if uploaded_file and st.button("ğŸ” å¼€å§‹æ ¡éªŒ", type="primary"):
            with st.spinner("æ­£åœ¨è¯»å–å¹¶æ ¡éªŒ..."):
                # ç›´æ¥ä¼ å†…å­˜æ–‡ä»¶å¯¹è±¡ç»™ Service
                passed, errors, df = self.svc.validate_csv(uploaded_file)

                if passed:
                    st.session_state.inv_df_clean = df
                    st.session_state.inv_target_date = target_date_str
                    st.success(f" æ ¡éªŒé€šè¿‡! å…± {len(df)} è¡Œæœ‰æ•ˆæ•°æ®ã€‚")
                else:
                    st.error(" æ ¡éªŒå¤±è´¥")
                    with st.expander("æŸ¥çœ‹è¯¦ç»†é”™è¯¯", expanded=True):
                        if errors and "æœªæ‰¾åˆ°" in errors[0]:
                            st.write(errors)
                        else:
                            st.write("ä»¥ä¸‹ SKU åœ¨ç³»ç»Ÿä¸­ä¸å­˜åœ¨ (è¯·å…ˆå» [æ•°æ®ä¿®æ”¹ä¸­å¿ƒ] æ³¨å†Œ):")
                            st.write(errors)
                    # æ ¡éªŒå¤±è´¥æ¸…ç©ºçŠ¶æ€
                    st.session_state.inv_df_clean = None

        st.markdown("---")

        # --- Step 2: æ‰§è¡Œå…¥åº“ ---
        if st.session_state.inv_df_clean is not None:
            df = st.session_state.inv_df_clean
            target_str = st.session_state.inv_target_date

            st.markdown(f"#### ğŸš€ å‡†å¤‡å…¥åº“: `{target_str}`")
            st.dataframe(df.head(), use_container_width=True)
            st.caption(f"é¢„è§ˆå‰ 5 è¡Œ (å…± {len(df)} è¡Œ)")

            # [Gate] å®‰å…¨éªŒè¯
            is_approved, _ = SecurityGate.authorize_action(
                action_key="btn_sync_inventory",
                ui_key_suffix="inv_sync",
                require_reason=False,
                help_text="ç¡®è®¤å°†æ­¤åº“å­˜æ•°æ®å†™å…¥æ•°æ®åº“ã€‚"
            )

            if is_approved:
                # [New] 2. åŠ é” -> æ‰§è¡Œ -> é‡Šæ”¾
                locked, lock_msg = LockManager.acquire_lock(LOCK_KEY_INV, username, "åº“å­˜å…¨é‡åŒæ­¥")
                if not locked:
                    st.error(f" åŠ é”å¤±è´¥: {lock_msg}")
                    return

                try:
                    with st.spinner("æ­£åœ¨å†™å…¥æ•°æ®åº“..."):
                        msg = self.svc.sync_inventory_to_db(df, target_str)
                        st.balloons()
                        st.success(msg)
                        # å®Œæˆåæ¸…ç†çŠ¶æ€ï¼Œé˜²æ­¢é‡å¤æäº¤
                        st.session_state.inv_df_clean = None
                except Exception as e:
                    st.error(f"åŒæ­¥å¤±è´¥: {e}")
                finally:
                    # [New] 3. ç¡®ä¿é‡Šæ”¾é”
                    LockManager.release_lock(LOCK_KEY_INV, username)
==================== END FILE: ui/pages/etl_ingest/inv_wizard.py ====================


==================== START FILE: ui/pages/data_visualization/__init__.py ====================
# ui/pages/data_visualization/__init__.py
"""
æ–‡ä»¶è¯´æ˜: æ•°æ®äº¤äº’å¯è§†åŒ–é¡µé¢ (Data Visualization) - V2.3 Final Polish
ä¸»è¦åŠŸèƒ½:
1. æä¾›å¤šç»´åº¦çš„é”€å”®ä¸æˆæœ¬åˆ†æå›¾è¡¨ã€‚
2. [Fix] ç»Ÿè®¡æ–¹å¼å…¨ä¸­æ–‡æ˜¾ç¤ºã€‚
3. [Fix] å›¾è¡¨ä¼˜åŒ–:
   - è¶‹åŠ¿çº¿å›¾ä¾‹: ä¸ºçº¢è‰²è™šçº¿æ·»åŠ æ˜ç¡®çš„å›¾ä¾‹è¯´æ˜ ("ğŸ”´ æ•´ä½“è¶‹åŠ¿")ã€‚
   - å¸ƒå±€: å›¾ä¾‹ç½®åº•ï¼Œé¡¶éƒ¨å¢åŠ  Padding é˜²æ­¢æˆªæ–­ã€‚
   - äº¤äº’: ç¦ç”¨æ»šè½®ç¼©æ”¾ï¼ŒTooltip æ—¥æœŸæ ¼å¼åŒ–ã€‚
"""

import streamlit as st
import datetime
import pandas as pd
import altair as alt
from config.settings import settings
from core.services.visual_service import VisualService

# --- æ˜ å°„å­—å…¸ (UI ä¸­æ–‡ -> DB å­—æ®µåç¼€) ---
ACTION_MAP = {
    "äº§å“é”€å”®": "Sales", "è®¢å•å–æ¶ˆ": "Cancel", "æ— å¹³å°ä»‹å…¥ä¸»åŠ¨é€€è´§": "Return",
    "å¹³å°ä»‹å…¥ç”¨æˆ·é€€è´§": "Request", "å¹³å°ä»‹å…¥å¼ºåˆ¶é€€è´§": "Case", "ç¬¬ä¸‰æ–¹ä»…é€€æ¬¾": "Dispute"
}
ACTION_MAP_REV = {v: k for k, v in ACTION_MAP.items()}

SHIP_MAP = {
    "æ™®é€šé‚®é€’": "ShipRegular", "é‚®è´¹ç½šæ¬¾": "ShipUnder",
    "é‚®è´¹è¶…æ”¯": "ShipOver", "åŒ…é€€è´§é‚®è´¹": "ShipReturn"
}

FEE_MAP = {
    "äº§å“æˆæœ¬": "COGS", "å¹³å°è´¹ç”¨": "PlatformFee"
}

# è§†å›¾æ¨¡å¼æ˜ å°„
VIEW_MODE_MAP = {
    "é”€å”®é¢": "Amount",
    "äº§å“æ•°é‡": "Quantity",
    "è®¢å•æ•°": "Order",
    "ç™¾åˆ†æ¯”(ç›¸å¯¹é”€å”®é¢å æ¯”)": "Percentage"
}


@st.cache_resource
def get_service():
    return VisualService()


@st.cache_data(ttl=600)
def load_data(start, end, stores):
    """ç¼“å­˜æ•°æ®åŠ è½½ç»“æœ"""
    svc = get_service()
    return svc.load_and_aggregate(start, end, stores)


def render():
    st.title("ğŸ“ˆ æ•°æ®äº¤äº’å¯è§†åŒ– (Visuals)")
    st.caption(f"Enterprise Analytics Cockpit {settings.APP_VERSION}")

    # =========================================================================
    # 1. æ§åˆ¶é¢æ¿ (Control Panel)
    # =========================================================================
    with st.container(border=True):
        c1, c2, c3 = st.columns([1.5, 2, 1])

        with c1:
            st.markdown("### ğŸ‘ï¸ ç»Ÿè®¡æ–¹å¼")
            view_mode_label = st.radio(
                "Mode",
                list(VIEW_MODE_MAP.keys()),
                horizontal=True,
                label_visibility="collapsed"
            )
            view_mode = VIEW_MODE_MAP[view_mode_label]

        with c2:
            st.markdown("### ğŸ“… æ—¶é—´åŒºé—´")
            today = datetime.date.today()
            last_month = today - datetime.timedelta(days=30)
            date_range = st.date_input("Range", value=(last_month, today), label_visibility="collapsed")

        with c3:
            st.markdown("### ğŸª åº—é“º")
            stores = st.multiselect("Store", ["esplus", "88"], default=["esplus", "88"], label_visibility="collapsed")

    if st.button("ğŸ§¹ å¼ºåˆ¶åˆ·æ–°ç¼“å­˜", help="å¦‚æœæ•°æ®æœ‰æ›´æ–°ä½†å›¾è¡¨æ²¡å˜ï¼Œè¯·ç‚¹å‡»æ­¤æŒ‰é’®"):
        st.cache_data.clear()
        st.rerun()

    st.markdown("---")

    col_filters, col_main = st.columns([1, 3.5])

    # =========================================================================
    # 2. å·¦ä¾§ç­›é€‰å™¨ (Filters)
    # =========================================================================
    with col_filters:
        st.markdown("#### ğŸ› ï¸ æŒ‡æ ‡ç­›é€‰")

        actions = st.multiselect("é”€å”®ç±»å‹", list(ACTION_MAP.keys()), default=[ACTION_MAP_REV["Sales"]])

        disable_ship = (view_mode == "Quantity")
        ship_types = st.multiselect("é‚®è´¹ç±»å‹", list(SHIP_MAP.keys()), disabled=disable_ship)

        disable_fee = (view_mode == "Quantity") or (view_mode == "Order")
        fees = st.multiselect("é”€å”®è´¹ç”¨", list(FEE_MAP.keys()), disabled=disable_fee)

    # =========================================================================
    # 3. ä¸»å›¾è¡¨åŒº (Charts)
    # =========================================================================
    with col_main:
        if len(date_range) != 2 or not stores:
            st.info("è¯·é€‰æ‹©å®Œæ•´çš„æ—¶é—´èŒƒå›´å’Œåº—é“ºã€‚")
            return

        with st.spinner("æ­£åœ¨è®¡ç®—èšåˆæ•°æ®..."):
            try:
                df_agg, _ = load_data(date_range[0], date_range[1], stores)
            except Exception as e:
                st.error(f"Error: {e}")
                return

        if df_agg.empty:
            st.warning("æ‰€é€‰èŒƒå›´å†…æš‚æ— æ•°æ®ã€‚")
            return

        # å‡†å¤‡ç»˜å›¾åˆ—
        show_cols = []
        denom_col = "Sales_Amount"

        def process_cols(ui_list, map_dict, suffix, is_total=False):
            for ui_item in ui_list:
                db_key = map_dict[ui_item]
                if is_total:
                    col_name = f"Total_{db_key}"
                else:
                    col_name = f"{db_key}_{suffix}"

                if col_name not in df_agg.columns: continue

                if view_mode == "Percentage":
                    if is_total:
                        num_col = f"Total_{db_key}"
                    else:
                        num_col = f"{db_key}_Amount"

                    if num_col in df_agg.columns and denom_col in df_agg.columns:
                        new_col = f"{ui_item} å æ¯”"
                        df_agg[new_col] = (df_agg[num_col] / df_agg[denom_col].replace(0, 1)).fillna(0)
                        show_cols.append(new_col)
                else:
                    df_agg[ui_item] = df_agg[col_name]
                    show_cols.append(ui_item)

        current_suffix = view_mode if view_mode != "Percentage" else "Amount"
        process_cols(actions, ACTION_MAP, current_suffix, is_total=False)
        if not disable_ship: process_cols(ship_types, SHIP_MAP, current_suffix, is_total=True)
        if not disable_fee: process_cols(fees, FEE_MAP, current_suffix, is_total=True)

        # Render Altair
        if show_cols:
            df_chart = df_agg[['DateStr'] + show_cols].rename(columns={'DateStr': 'æ—¥æœŸ'})
            df_melt = df_chart.melt('æ—¥æœŸ', var_name='æŒ‡æ ‡', value_name='æ•°å€¼')

            # åŠ¨æ€æ ¼å¼
            if view_mode == "Amount":
                y_format = '$.2s'
            elif view_mode == "Percentage":
                y_format = '.1%'
            else:
                y_format = ',.0f'

            # --- Base Chart ---
            base = alt.Chart(df_melt).encode(
                x=alt.X('æ—¥æœŸ:T', axis=alt.Axis(format='%Y-%m-%d', title='æ—¥æœŸ')),
                y=alt.Y('æ•°å€¼:Q', axis=alt.Axis(format=y_format)),
                color=alt.Color('æŒ‡æ ‡:N', legend=alt.Legend(orient='bottom', title=None)),
                tooltip=[
                    alt.Tooltip('æ—¥æœŸ:T', format='%Y-%m-%d', title='æ—¥æœŸ'),
                    'æŒ‡æ ‡',
                    alt.Tooltip('æ•°å€¼', format=y_format)
                ]
            )

            # --- Layers Logic ---
            if len(show_cols) == 1:
                # 1. é¢ç§¯å›¾
                area = base.mark_area(opacity=0.3)
                # 2. æ•°æ®æŠ˜çº¿
                line = base.mark_line(point=True)

                # 3. è¶‹åŠ¿çº¿ (Trend Line)
                # [Fix] ä½¿ç”¨ transform_calculate æ·»åŠ ä¸€ä¸ª Label å­—æ®µ
                # [Fix] è¦†ç›– color ç¼–ç ï¼Œä½¿å…¶æ˜¾ç¤ºç‹¬ç«‹çš„å›¾ä¾‹
                trend = base.transform_loess('æ—¥æœŸ', 'æ•°å€¼', bandwidth=0.3).transform_calculate(
                    trend_label="'ğŸ”´ æ•´ä½“è¶‹åŠ¿ (Trend)'"
                ).mark_line(
                    strokeDash=[5, 5],
                    strokeWidth=2,
                    opacity=0.8
                ).encode(
                    color=alt.Color('trend_label:N',
                                    scale=alt.Scale(range=['#FF4B4B']),
                                    legend=alt.Legend(orient='bottom', title=None)
                                    )
                )

                final_chart = (area + line + trend).properties(
                    padding={'top': 30, 'left': 10, 'right': 10, 'bottom': 0}
                )
            else:
                # å¤šæŒ‡æ ‡: ä»…æŠ˜çº¿
                line = base.mark_line(point=True)
                final_chart = line.properties(
                    padding={'top': 30, 'left': 10, 'right': 10, 'bottom': 0}
                )

            st.altair_chart(final_chart, use_container_width=True)
        else:
            st.info("è¯·åœ¨å·¦ä¾§å‹¾é€‰è‡³å°‘ä¸€ä¸ªæŒ‡æ ‡ä»¥å±•ç¤ºå›¾è¡¨ã€‚")
==================== END FILE: ui/pages/data_visualization/__init__.py ====================


==================== START FILE: ui/pages/reports/center.py ====================
# ui/pages/reports/center.py
"""
æ–‡ä»¶è¯´æ˜: æŠ¥è¡¨ä¸­å¿ƒ UI (Report Center) - V2.1 Audit Logged
ä¸»è¦åŠŸèƒ½:
1. å±•ç¤º output/{user}/ ç›®å½•ä¸‹çš„æ‰€æœ‰æŠ¥è¡¨æ–‡ä»¶ã€‚
2. æä¾›æ–‡ä»¶çš„å•ä¸€ä¸‹è½½å’Œæ‰¹é‡ ZIP ä¸‹è½½ã€‚
3. [Fix] å®¡è®¡è¿½è¸ª: æ‰€æœ‰ä¸‹è½½è¡Œä¸º (ZIP/CSV) å‡ä¼šè¢«è®°å½•åˆ°ä¸šåŠ¡æ“ä½œæ—¥å¿—ä¸­ã€‚
4. æä¾›æ–‡ä»¶æ¸…ç†åŠŸèƒ½ã€‚
"""

import streamlit as st
import datetime
import time
from pathlib import Path

from core.services.report_manager import ReportFileManager
from core.components.utils.csv_parser import parse_compound_csv
from core.sys.logger import get_logger
from ui.kernel.session import SessionManager

# åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
app_logger = get_logger("ReportCenter")


def _log_download_event(filename: str):
    """
    [å›è°ƒå‡½æ•°] è®°å½•ä¸‹è½½å®¡è®¡æ—¥å¿—
    æ³¨æ„: æ­¤å‡½æ•°ä¼šåœ¨ç‚¹å‡»æŒ‰é’®ç¬é—´æ‰§è¡Œ
    """
    user_info = SessionManager.get_current_user()
    username = user_info.get("username", "Unknown") if user_info else "System"

    app_logger.info(
        f"æ•°æ®å¯¼å‡º: ç”¨æˆ·ä¸‹è½½äº†æ–‡ä»¶ [{filename}]",
        extra={
            "user": username,
            "action": "DOWNLOAD_FILE",
            "func": "æŠ¥è¡¨ä¸­å¿ƒ",
            "table": filename  # å€Ÿç”¨ table å­—æ®µè®°å½•æ–‡ä»¶åï¼Œæ–¹ä¾¿æ£€ç´¢
        }
    )


def render():
    st.subheader("ğŸ“‚ æŠ¥è¡¨ä¸­å¿ƒ (Report Center)")
    st.info("åœ¨æ­¤é¢„è§ˆã€ä¸‹è½½æˆ–ç®¡ç†å·²ç”Ÿæˆçš„åˆ†ææŠ¥è¡¨ã€‚æ–‡ä»¶ä»…åœ¨å½“å‰ä¼šè¯ä¿ç•™ï¼Œé€€å‡ºåè‡ªåŠ¨æ¸…é™¤ã€‚")

    manager = ReportFileManager()
    files = manager.get_generated_files()

    if not files:
        st.warning("ğŸ“­ æš‚æ— æŠ¥è¡¨æ–‡ä»¶ã€‚è¯·å…ˆå‰å¾€ [ç”ŸæˆæŠ¥è¡¨] é¡µé¢è¿è¡Œåˆ†æã€‚")
        if st.button("ğŸ”„ åˆ·æ–°åˆ—è¡¨"):
            st.rerun()
        return

    # =========================================================================
    # 1. å·¥å…·æ  (Toolbar)
    # =========================================================================
    col_tools, col_list = st.columns([1, 2.5])

    with col_tools:
        st.markdown("#### ğŸ› ï¸ æ“ä½œ")

        # æ‰“åŒ…ä¸‹è½½
        zip_data = manager.create_zip_archive()
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')
        zip_name = f"Reports_All_{timestamp}.zip"

        st.download_button(
            label="ğŸ“¦ æ‰“åŒ…ä¸‹è½½æ‰€æœ‰ (.zip)",
            data=zip_data,
            file_name=zip_name,
            mime="application/zip",
            use_container_width=True,
            type="primary",
            # [Fix] ç»‘å®šå®¡è®¡å›è°ƒ
            on_click=_log_download_event,
            args=(zip_name,)
        )

        st.markdown("---")

        # æ¸…ç©ºæŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶", type="secondary", use_container_width=True):
            manager.clear_all_reports()
            st.toast("å·²æ¸…ç©ºæ‰€æœ‰æŠ¥è¡¨")
            time.sleep(0.5)
            st.rerun()

    # =========================================================================
    # 2. æ–‡ä»¶é¢„è§ˆåŒº (Preview)
    # =========================================================================
    with col_list:
        st.markdown(f"#### ğŸ“„ æ–‡ä»¶åˆ—è¡¨ ({len(files)})")

        selected_file = st.selectbox("é€‰æ‹©è¦é¢„è§ˆçš„æ–‡ä»¶:", files)

        if selected_file:
            # è·å–ç»å¯¹è·¯å¾„
            file_path = manager.get_file_path(selected_file)

            # å•æ–‡ä»¶ä¸‹è½½
            try:
                with open(file_path, "rb") as f:
                    st.download_button(
                        label=f"ğŸ“¥ ä¸‹è½½: {selected_file}",
                        data=f,
                        file_name=selected_file,
                        mime="text/csv",
                        # [Fix] ç»‘å®šå®¡è®¡å›è°ƒ
                        on_click=_log_download_event,
                        args=(selected_file,)
                    )
            except Exception as e:
                st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")

            st.markdown("---")

            # æ™ºèƒ½è§£æä¸é¢„è§ˆ
            st.caption("ğŸ“Š æŠ¥è¡¨é¢„è§ˆ (Preview):")
            try:
                tables = parse_compound_csv(Path(file_path))
                if not tables:
                    st.warning("æ— æ³•è§£ææ–‡ä»¶å†…å®¹æˆ–æ–‡ä»¶ä¸ºç©ºã€‚")
                else:
                    for title, df in tables:
                        with st.expander(f"ğŸ“Œ {title} ({len(df)} è¡Œ)", expanded=True):
                            st.dataframe(df, use_container_width=True)
            except Exception as e:
                st.error(f"é¢„è§ˆè§£æå¤±è´¥: {e}")
==================== END FILE: ui/pages/reports/center.py ====================


==================== START FILE: ui/pages/reports/__init__.py ====================
# ui/pages/reports/__init__.py
"""
æ–‡ä»¶è¯´æ˜: å•†ä¸šæ™ºèƒ½æŠ¥è¡¨é¡µé¢å…¥å£
ä¸»è¦åŠŸèƒ½:
1. åˆå§‹åŒ–æŠ¥è¡¨é¡µé¢æ ‡é¢˜ã€‚
2. è°ƒç”¨ TabManager åŠ¨æ€åŠ è½½ 'reports' æ¨¡å—ä¸‹çš„ Tabs (ç”Ÿæˆå™¨ & ä¸­å¿ƒ)ã€‚
"""

import streamlit as st
from config.settings import settings
from ui.kernel.tab_manager import TabManager

def render():
    st.title("ğŸ“Š å•†ä¸šæ™ºèƒ½æŠ¥è¡¨ (BI Reports)")
    st.caption(f"Enterprise Analytics Engine {settings.APP_VERSION}")

    # è°ƒç”¨åŠ¨æ€ Tab å¼•æ“
    # è¿™é‡Œçš„ 'reports' å¯¹åº” modules.json é‡Œçš„ key
    TabManager.render_tabs("reports")
==================== END FILE: ui/pages/reports/__init__.py ====================


==================== START FILE: ui/pages/reports/generator.py ====================
# ui/pages/reports/generator.py
"""
æ–‡ä»¶è¯´æ˜: æŠ¥è¡¨ç”Ÿæˆå™¨ UI (Report Generator) - V2.2 Log Window Fixed
ä¸»è¦åŠŸèƒ½:
1. æä¾›åˆ†æå‘¨æœŸä¸ä¸šåŠ¡å‚æ•°é…ç½®ã€‚
2. [Fix] åŒè¿›åº¦æ¡: ä¼˜åŒ–æ˜¾ç¤ºé€»è¾‘ã€‚
3. [Fix] å®æ—¶æ—¥å¿—å°: ä½¿ç”¨å›ºå®šé«˜åº¦å®¹å™¨ + å€’åºæ’åˆ— (Newest at Top)ï¼Œè§£å†³æ»šåŠ¨é—®é¢˜ã€‚
4. å®‰å…¨åˆè§„: æ¥å…¥ SecurityGateã€‚
"""

import streamlit as st
import datetime
import time
import traceback
from typing import List

from config.settings import settings
from core.services.report_manager import ReportFileManager
from ui.components.security import SecurityGate

# å¼•å…¥æ‰€æœ‰ä¸šåŠ¡åˆ†ææœåŠ¡
from core.services.finance.sales import SalesQtyAnalyzer
from core.services.finance.profit_sku import SkuProfitAnalyzer
from core.services.finance.profit_listing import ListingProfitAnalyzer
from core.services.finance.profit_combo import ComboProfitAnalyzer
from core.services.crm import CustomerAnalyzer
from core.services.logistics import ShippingAnalyzer
from core.services.prediction import PredictionService
from core.services.ordering import OrderingService
from core.services.inventory_snapshot import InventorySnapshot


def _render_log_window(log_messages: List[str]):
    """
    [UIç»„ä»¶] æ¸²æŸ“å®æ—¶æ»šåŠ¨æ—¥å¿—çª—å£
    ç­–ç•¥: å€’åºæ˜¾ç¤º (æœ€æ–°çš„åœ¨æœ€ä¸Šé¢)ï¼Œåˆ©ç”¨ HTML/CSS å®ç°å›ºå®šé«˜åº¦æ»šåŠ¨ã€‚
    """
    # å°†æ—¥å¿—åˆ—è¡¨è¿æ¥ä¸º HTMLï¼Œç¬¬ä¸€æ¡é«˜äº®
    html_content = ""
    for i, msg in enumerate(log_messages):
        color = "#4EC9B0" if i == 0 else "#AAAAAA"  # æœ€æ–°ä¸€æ¡äº®è‰²ï¼Œæ—§çš„ç°è‰²
        weight = "bold" if i == 0 else "normal"
        icon = "ğŸŸ¢" if "å®Œæˆ" in msg else ("â³" if "æ­£åœ¨" in msg else "ï¸")
        if "å¤±è´¥" in msg or "Error" in msg:
            icon = "ğŸ”´";
            color = "#FF4B4B"

        html_content += f"""
        <div style='margin-bottom: 4px; font-family: monospace; font-size: 13px; color: {color}; font-weight: {weight}; border-bottom: 1px dashed rgba(255,255,255,0.1); padding-bottom: 2px;'>
            <span style='margin-right: 6px;'>{icon}</span>{msg}
        </div>
        """

    # å®¹å™¨æ ·å¼
    st.markdown(f"""
        <div style="
            height: 300px; 
            overflow-y: auto; 
            background-color: #1E1E1E; 
            border: 1px solid #333; 
            border-radius: 8px; 
            padding: 15px;
            box-shadow: inset 0 0 10px rgba(0,0,0,0.5);
        ">
            {html_content}
        </div>
    """, unsafe_allow_html=True)


def render():
    st.subheader("ğŸš€ æŠ¥è¡¨ç”Ÿæˆå™¨ (Generator)")
    st.info("ğŸ’¡ è¯´æ˜: ç³»ç»Ÿå°†åŸºäºæ¸…æ´—åçš„æ•°æ®æ‰§è¡Œå…¨ç»´åº¦ä¸šåŠ¡åˆ†æã€‚ç”Ÿæˆçš„æŠ¥è¡¨å°†ä¿å­˜åœ¨æ‚¨çš„ä¸ªäººç›®å½•ä¸­ã€‚")

    # =========================================================================
    # 1. å‚æ•°é…ç½®åŒº
    # =========================================================================
    with st.container(border=True):
        st.markdown("#### ğŸ“… 1. åˆ†æå‘¨æœŸ")
        c1, c2 = st.columns(2)
        with c1:
            today = datetime.date.today()
            first_curr = today.replace(day=1)
            last_prev = first_curr - datetime.timedelta(days=1)
            first_prev = last_prev.replace(day=1)
            start_date = st.date_input("å¼€å§‹æ—¥æœŸ (Start Date)", value=first_prev)
        with c2:
            end_date = st.date_input("ç»“æŸæ—¥æœŸ (End Date)", value=last_prev)

    with st.container(border=True):
        st.markdown("#### âš™ï¸ 2. ä¸šåŠ¡å‚æ•° (Business Logic)")
        col_loss, col_supply = st.columns(2)
        with col_loss:
            st.markdown("**ğŸ“‰ è€—æŸç‡**")
            lr_case = st.slider("Case", 0.0, 1.0, settings.LOSS_RATES['CASE'], 0.01)
            lr_req = st.slider("Request", 0.0, 1.0, settings.LOSS_RATES['REQUEST'], 0.01)
            lr_ret = st.slider("Return", 0.0, 1.0, settings.LOSS_RATES['RETURN'], 0.01)
            lr_disp = st.slider("Dispute", 0.0, 1.0, settings.LOSS_RATES['DISPUTE'], 0.01)
        with col_supply:
            st.markdown("**ğŸ“¦ ä¾›åº”é“¾**")
            lead_time = st.number_input("è®¢è´§æå‰æœŸ", value=float(settings.LEAD_MONTH), step=0.5)
            safety_stock = st.number_input("å®‰å…¨åº“å­˜", value=float(settings.MIN_SAFETY_MONTH), step=0.5)

    st.markdown("---")

    if start_date > end_date:
        st.error(" å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸ")
        return

    # [Security Gate V5]
    is_approved, _ = SecurityGate.authorize_action(
        action_key="btn_generate_report",
        ui_key_suffix="gen_report",
        require_reason=False,
        help_text="éªŒè¯é€šè¿‡åå°†å¯åŠ¨å…¨é‡åˆ†æå¼•æ“ã€‚"
    )

    if is_approved:
        # 1. æ›´æ–°é…ç½®
        settings.LOSS_RATES.update({'CASE': lr_case, 'REQUEST': lr_req, 'RETURN': lr_ret, 'DISPUTE': lr_disp})
        settings.LEAD_MONTH = lead_time
        settings.MIN_SAFETY_MONTH = safety_stock

        # 2. æ¸…ç†
        mgr = ReportFileManager()
        mgr.clear_all_reports()

        suffix = f"{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}"

        # 3. ä»»åŠ¡åˆ—è¡¨
        tasks = [
            ("ğŸ“¦ SKU é”€é‡ç»Ÿè®¡", SalesQtyAnalyzer),
            ("ğŸ’° SKU åˆ©æ¶¦ä¸è¯Šæ–­", SkuProfitAnalyzer),
            ("ğŸ”— Listing è¡¨ç°åˆ†æ", ListingProfitAnalyzer),
            ("ğŸ Combo ç­–ç•¥åˆ†æ", ComboProfitAnalyzer),
            ("ğŸ‘¥ å®¢æˆ·ç”»åƒä¸é£é™©", CustomerAnalyzer),
            ("ğŸšš ç‰©æµæ•ˆç›Šè¯Šæ–­", ShippingAnalyzer),
            ("ğŸ¦ åº“å­˜èµ„äº§å¿«ç…§", InventorySnapshot),
            ("ğŸ¤– AI é”€é‡é¢„æµ‹", PredictionService),
            ("ğŸ›’ æ™ºèƒ½è¡¥è´§è®¡ç®—", OrderingService)
        ]

        total_tasks = len(tasks)
        success_count = 0

        # åˆå§‹åŒ– UI å ä½ç¬¦
        progress_zone = st.empty()
        log_zone = st.empty()

        # å®æ—¶æ—¥å¿—åˆ—è¡¨ (Newest at Top)
        logs = []

        def update_ui(main_p, sub_p, msg=None):
            if msg:
                timestamp = datetime.datetime.now().strftime("%H:%M:%S")
                logs.insert(0, f"[{timestamp}] {msg}")  # æ’å…¥åˆ°æœ€å‰

            # æ¸²æŸ“è¿›åº¦æ¡
            with progress_zone.container():
                st.write(f"**æ€»ä½“è¿›åº¦ ({int(main_p * 100)}%)**")
                st.progress(main_p)
                st.write(f"å½“å‰ä»»åŠ¡è¿›åº¦ ({int(sub_p * 100)}%)")
                st.progress(sub_p)

            # æ¸²æŸ“æ—¥å¿—çª—
            with log_zone.container():
                _render_log_window(logs)

        # å¼€å§‹æ‰§è¡Œ
        update_ui(0.0, 0.0, "ğŸš€ åˆ†æå¼•æ“å¯åŠ¨...")

        for idx, (title, ServiceClass) in enumerate(tasks):
            update_ui(idx / total_tasks, 0.0, f"æ­£åœ¨è¿è¡Œ: {title}...")

            try:
                # æ¨¡æ‹Ÿä¸€ç‚¹å­è¿›åº¦ï¼Œè®©ç”¨æˆ·çŸ¥é“æ²¡å¡æ­»
                update_ui(idx / total_tasks, 0.1)

                # æ‰§è¡Œä¸šåŠ¡é€»è¾‘ (åŒæ­¥é˜»å¡)
                svc = ServiceClass(start_date, end_date, suffix)
                svc.run()

                # å®Œæˆ
                success_count += 1
                update_ui((idx + 1) / total_tasks, 1.0, f" å®Œæˆ: {title}")

            except Exception as e:
                update_ui((idx + 1) / total_tasks, 0.0, f" å¤±è´¥: {title} - {str(e)}")
                st.error(f"Error in {title}: {e}")
                st.code(traceback.format_exc())

        # æœ€ç»ˆçŠ¶æ€
        if success_count == len(tasks):
            st.balloons()
            st.success("ğŸ‰ æ‰€æœ‰æŠ¥è¡¨å·²ç”Ÿæˆï¼è¯·å‰å¾€ [æŠ¥è¡¨ä¸­å¿ƒ] ä¸‹è½½ã€‚")
        else:
            st.warning("âš ï¸ éƒ¨åˆ†ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šæ–¹æ—¥å¿—ã€‚")
==================== END FILE: ui/pages/reports/generator.py ====================


==================== START FILE: ui/kernel/layout.py ====================
# ui/kernel/layout.py
"""
æ–‡ä»¶è¯´æ˜: UI å¸ƒå±€å¼•æ“ (Layout Engine)
ä¸»è¦åŠŸèƒ½:
1. åŠ è½½å¹¶æ¸²æŸ“èƒŒæ™¯è§†é¢‘ (Background Video)ã€‚
2. æ³¨å…¥å…¨å±€ CSS æ ·å¼ (Transparency Hack)ã€‚
3. æ¸²æŸ“é¡¶éƒ¨ Header (Logo + Title + Clock + Version Info)ã€‚
"""

import streamlit as st
import base64
import streamlit.components.v1 as components
from config.settings import settings


def get_base64_encoded_file(file_path):
    """è¯»å–æ–‡ä»¶å¹¶è½¬æ¢ä¸º base64 ç¼–ç """
    if not file_path.exists():
        return None
    try:
        with open(file_path, "rb") as f:
            return base64.b64encode(f.read()).decode("utf-8")
    except Exception:
        return None


def set_main_background_video():
    """
    æ¸²æŸ“å…¨å±èƒŒæ™¯è§†é¢‘ï¼Œå¼ºåˆ¶è¦†ç›– Streamlit é»˜è®¤èƒŒæ™¯ã€‚
    ä¾èµ–: ui/assets/background.mp4
    """
    video_path = settings.ASSETS_DIR / "background.mp4"
    video_b64 = get_base64_encoded_file(video_path)

    if not video_b64:
        st.markdown(
            """<style>.stApp { background-color: #0e1117; }</style>""",
            unsafe_allow_html=True
        )
        return

    # æ ¸å¿ƒ CSSï¼šæš´åŠ›å°†æ‰€æœ‰å¯èƒ½é®æŒ¡è§†é¢‘çš„å®¹å™¨è®¾ä¸ºé€æ˜
    # [Fix] ç§»é™¤äº†å¯¹ stHeader çš„ visibility: hiddenï¼Œé˜²æ­¢ä¾§è¾¹æ å±•å¼€æŒ‰é’®æ¶ˆå¤±
    css_code = f"""
    <style>
        /* 1. å…¨å±€å®¹å™¨é€æ˜åŒ– */
        .stApp {{ background: transparent !important; }}
        /* 2. æ»šåŠ¨å®¹å™¨é€æ˜åŒ– */
        [data-testid="stAppViewContainer"] {{ background: transparent !important; }}
        /* 3. ä¸»å†…å®¹åŒºé€æ˜åŒ– */
        [data-testid="stMainBlockContainer"] {{ background: transparent !important; }}

        /* 4. é¡¶éƒ¨ Header é€æ˜åŒ– (ä½†ä¿ç•™å¯è§æ€§ï¼Œä»¥ä¾¿ç‚¹å‡»ä¾§è¾¹æ æŒ‰é’®) */
        [data-testid="stHeader"] {{ 
            background: transparent !important; 
            /* visibility: hidden; <--- å·²ç§»é™¤æ­¤è¡Œï¼Œä¿®å¤ä¾§è¾¹æ æ— æ³•å±•å¼€çš„é—®é¢˜ */
        }}

        /* 5. èƒŒæ™¯è§†é¢‘å®šä½ */
        #bg-video {{
            position: fixed;
            top: 0; left: 0;
            width: 100vw; height: 100vh;
            object-fit: cover;
            z-index: -100;
        }}

        /* 6. é»‘è‰²é®ç½© (å¢åŠ å¯è¯»æ€§) */
        #bg-overlay {{
            position: fixed;
            top: 0; left: 0;
            width: 100vw; height: 100vh;
            background: rgba(0, 0, 0, 0.70);
            z-index: -99;
            backdrop-filter: blur(3px);
        }}

        /* 7. ä¾§è¾¹æ æ ·å¼ */
        [data-testid="stSidebar"] {{
            background-color: rgba(20, 20, 20, 0.9) !important;
            border-right: 1px solid rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
        }}

        /* 8. å…¨å±€å­—ä½“å¢å¼º */
        h1, h2, h3, h4, h5, h6, p, span, div, label, li {{
            color: #E0E0E0 !important;
            text-shadow: 0px 1px 2px rgba(0,0,0,0.8);
        }}

        /* 9. è¾“å…¥æ¡†ç¾åŒ– */
        .stTextInput input, .stNumberInput input, .stSelectbox div[data-baseweb="select"] {{
            background-color: rgba(40, 40, 40, 0.6) !important;
            color: white !important;
            border: 1px solid rgba(255, 255, 255, 0.2) !important;
        }}
    </style>
    """

    video_html = f"""
    {css_code}
    <video id="bg-video" autoplay loop muted playsinline>
        <source src="data:video/mp4;base64,{video_b64}" type="video/mp4">
    </video>
    <div id="bg-overlay"></div>
    """
    st.markdown(video_html, unsafe_allow_html=True)


def render_header():
    """
    æ¸²æŸ“é¡¶éƒ¨è‡ªå®šä¹‰ Header (Logo + Title + Clock)
    """
    logo_path = settings.ASSETS_DIR / "Logo.png"
    logo_b64 = get_base64_encoded_file(logo_path)

    logo_img = (
        f'<img src="data:image/png;base64,{logo_b64}" class="header-logo">'
        if logo_b64 else "ğŸ¦…"
    )

    st.markdown(f"""
        <style>
        .header-wrapper {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 10px 0px 20px 0px;
            border-bottom: 1px solid rgba(255,255,255,0.15);
            margin-bottom: 20px;
        }}
        .header-left {{ display: flex; align-items: center; gap: 15px; }}
        .header-logo {{ height: 50px; width: auto; }}
        .main-title {{ font-size: 24px; font-weight: 700; color: #fff; letter-spacing: 1px; }}
        .sub-title {{ font-size: 12px; color: rgba(255,255,255,0.7) !important; }}
        .meta-info {{ font-size: 10px; color: rgba(255,255,255,0.5) !important; margin-top: 2px; }}

        .header-right {{ text-align: right; }}
        .company-badge {{
            font-size: 12px; font-weight: 600; color: #E74C3C !important;
            background: rgba(0,0,0,0.5); padding: 2px 8px; border-radius: 8px;
            border: 1px solid rgba(231, 76, 60, 0.4);
        }}
        #clock-display {{
            font-family: 'Monaco', monospace;
            font-size: 15px;
            color: #4EC9B0 !important; 
            margin-top: 5px;
            font-weight: bold;
        }}
        </style>

        <div class="header-wrapper">
            <div class="header-left">
                {logo_img}
                <div>
                    <div class="main-title">{settings.APP_NAME}</div>
                    <div class="sub-title">Enterprise Intelligence Platform</div>
                    <div class="meta-info">
                        Ver: {settings.APP_VERSION} &nbsp;|&nbsp; 
                        Released: {settings.VERSION_DATE} &nbsp;|&nbsp; 
                        By: {settings.AUTHOR}
                    </div>
                </div>
            </div>
            <div class="header-right">
                <span class="company-badge">ğŸ¦… EAGLESTAR INC.</span>
                <div id="clock-display">Initializing...</div>
            </div>
        </div>
    """, unsafe_allow_html=True)

    # JS åŠ¨æ€æ—¶é’Ÿ
    js_code = """
    <script>
        function updateClock() {
            const now = new Date();
            const dateStr = now.getFullYear() + "-" + 
                            String(now.getMonth()+1).padStart(2, '0') + "-" + 
                            String(now.getDate()).padStart(2, '0');
            const timeStr = String(now.getHours()).padStart(2, '0') + ":" + 
                            String(now.getMinutes()).padStart(2, '0') + ":" + 
                            String(now.getSeconds()).padStart(2, '0');
            try {
                const clock = window.parent.document.getElementById('clock-display');
                if (clock) {
                    clock.innerText = "ğŸ•’ " + dateStr + " " + timeStr;
                }
            } catch(e) {}
        }
        setInterval(updateClock, 1000);
        updateClock();
    </script>
    """
    components.html(js_code, height=0)
==================== END FILE: ui/kernel/layout.py ====================


==================== START FILE: ui/kernel/session.py ====================
# ui/kernel/session.py
"""
æ–‡ä»¶è¯´æ˜: UI ä¼šè¯ç®¡ç†å™¨ (Session Manager)
ä¸»è¦åŠŸèƒ½:
1. ç®¡ç† Streamlit Session State (User, Token, ActiveTime)ã€‚
2. å¤„ç†ç™»å½• (Login) ä¸æ³¨é”€ (Logout) é€»è¾‘ã€‚
3. [New] ç™»å‡ºæ—¶è‡ªåŠ¨é‡Šæ”¾é”ï¼Œå¹¶è§¦å‘ç›¸å…³è¡¨çš„è„æ•°æ®æ¸…æ´— (Rollback)ã€‚
4. è‡ªåŠ¨æ³¨å…¥ Contextã€‚
"""

import streamlit as st
import time
import shutil
from typing import Optional

# å¼•å…¥åç«¯æœåŠ¡
from config.settings import settings
from core.services.auth.service import AuthService
from core.sys.context import set_context, clear_context
from core.sys.lock_manager import LockManager  # [New]
from core.services.etl.repository import ETLRepository  # [New] ç”¨äºæ¸…æ´—æ•°æ®

# Session State Keys
SESSION_KEY_USER = "auth_user"
SESSION_KEY_TOKEN = "auth_token"
SESSION_KEY_LAST_ACTIVE = "last_active"


class SessionManager:

    @staticmethod
    def init_session():
        """åˆå§‹åŒ– Session State é»˜è®¤å€¼"""
        if SESSION_KEY_USER not in st.session_state:
            st.session_state[SESSION_KEY_USER] = None
        if SESSION_KEY_TOKEN not in st.session_state:
            st.session_state[SESSION_KEY_TOKEN] = None
        if SESSION_KEY_LAST_ACTIVE not in st.session_state:
            st.session_state[SESSION_KEY_LAST_ACTIVE] = time.time()

    @staticmethod
    def login(username, password) -> bool:
        """æ‰§è¡Œç™»å½•"""
        try:
            # å°è¯•è·å–çœŸå® IP (é€‚é… Proxy/Cloud)
            headers = st.context.headers
            if "X-Forwarded-For" in headers:
                ip = headers["X-Forwarded-For"].split(",")[0]
            else:
                ip = "127.0.0.1"
        except:
            ip = "127.0.0.1"

        ok, user, msg = AuthService.authenticate(username, password, ip)

        if ok and user:
            # 1. ç”Ÿæˆæ–° Token
            token = AuthService.refresh_session_token(user.username)

            # 2. å†™å…¥ Session State
            st.session_state[SESSION_KEY_USER] = {
                "username": user.username,
                "is_admin": user.is_admin,
                "ip": ip
            }
            st.session_state[SESSION_KEY_TOKEN] = token
            st.session_state[SESSION_KEY_LAST_ACTIVE] = time.time()

            # 3. ç«‹å³è®¾ç½®ä¸Šä¸‹æ–‡
            set_context(username=user.username, ip=ip)
            return True
        else:
            st.error(msg)
            return False

    @staticmethod
    def logout():
        """
        [å®‰å…¨ç™»å‡º] æ¸…ç†ç¯å¢ƒã€é‡Šæ”¾é”å¹¶æ‰§è¡Œå¿…è¦çš„å›æ»š
        """
        user_data = st.session_state.get(SESSION_KEY_USER)
        if user_data:
            username = user_data.get("username")
            if username:
                # [New] 1. é‡Šæ”¾æ‰€æœ‰é”ï¼Œå¹¶è·å–è¢«é‡Šæ”¾çš„èµ„æºåˆ—è¡¨
                released_resources = LockManager.release_all_user_locks(username)

                # [New] 2. ä¸šåŠ¡å›æ»šé€»è¾‘ (Rollback Strategy)
                # å¦‚æœç”¨æˆ·æŒæœ‰ 'Data_Transaction' é”ï¼Œè¯´æ˜ä»–åœ¨ä¸Šä¼ äº¤æ˜“æ•°æ®ä¸­é€”é€€å‡ºäº†
                # æ­¤æ—¶å¿…é¡»æ¸…ç©ºåŸå§‹è¡¨ï¼Œé˜²æ­¢ä¸‹æ¬¡ä¸Šä¼ æ—¶æ•°æ®æ··åˆ
                if 'Data_Transaction' in released_resources:
                    print(f"âš ï¸ [Session] ç”¨æˆ· {username} ä¸­æ–­äº†äº¤æ˜“æ•°æ®ä¸Šä¼ ï¼Œæ­£åœ¨æ‰§è¡Œæ•°æ®å›æ»š (Truncate Raw Tables)...")
                    # è°ƒç”¨ ETL ä»“åº“æ¸…ç©º Data_Transaction å’Œ Data_Order_Earning
                    ETLRepository.truncate_raw_tables()

                    # 3. æ¸…ç†è¯¥ç”¨æˆ·çš„ä¸´æ—¶æ–‡ä»¶ç›®å½• output/{user}
                user_output_dir = settings.OUTPUT_DIR / username
                if user_output_dir.exists():
                    try:
                        shutil.rmtree(user_output_dir)
                        print(f"ğŸ§¹ [Session] å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {user_output_dir}")
                    except Exception as e:
                        print(f"âš ï¸ [Session] æ¸…ç†ç›®å½•å¤±è´¥: {e}")

        # æ¸…é™¤ Context
        clear_context()
        # æ¸…é™¤ Session State
        st.session_state.clear()
        # åˆ·æ–°é¡µé¢
        st.rerun()

    @staticmethod
    def get_current_user() -> Optional[dict]:
        """è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯ (Dict)"""
        return st.session_state.get(SESSION_KEY_USER)

    @staticmethod
    def ensure_context():
        """
        [ä¸­é—´ä»¶] æ¯æ¬¡é¡µé¢é‡ç»˜æ—¶å¿…é¡»è°ƒç”¨
        ä½œç”¨ï¼šä» Session State æ¢å¤ Contextï¼Œé˜²æ­¢é¡µé¢åˆ·æ–°å Context ä¸¢å¤±ã€‚
        """
        user_data = st.session_state.get(SESSION_KEY_USER)
        if user_data:
            # åˆ·æ–°æ´»è·ƒæ—¶é—´
            st.session_state[SESSION_KEY_LAST_ACTIVE] = time.time()

            # [å…³é”®] é‡æ–°æ³¨å…¥ä¸Šä¸‹æ–‡ç»™åç«¯ä½¿ç”¨
            set_context(username=user_data["username"], ip=user_data.get("ip"))
        else:
            # æœªç™»å½•çŠ¶æ€
            set_context(username=None)
==================== END FILE: ui/kernel/session.py ====================


==================== START FILE: ui/kernel/__init__.py ====================
# ui/pages/home/__init__.py
"""
æ–‡ä»¶è¯´æ˜: ç³»ç»Ÿé¦–é¡µ (Home Page)
ä¸»è¦åŠŸèƒ½:
1. æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯å’Œç³»ç»ŸçŠ¶æ€ dashboardã€‚
2. è¯»å–å¹¶å±•ç¤º Patch Notesã€‚
"""

import streamlit as st
import os
from config.settings import settings


def render():
    st.title(f"ğŸ  {settings.APP_NAME}")
    st.caption(f"Version: {settings.APP_VERSION}")
    st.markdown("---")

    # 1. ç³»ç»ŸçŠ¶æ€å¡ç‰‡
    col1, col2, col3 = st.columns(3)

    with col1:
        with st.container(border=True):
            st.metric("Database", "Connected", "Active")
            st.caption(f"Host: {settings.DB_HOST}")

    with col2:
        with st.container(border=True):
            st.metric("Environment", "Production", "Stable")
            st.caption(f"Logs: {settings.LOG_DIR}")

    with col3:
        with st.container(border=True):
            st.metric("User Mode", "Multi-User", "Isolated")
            st.caption("Output: Isolated per Session")

    # 2. æ¬¢è¿è¯­
    st.info("ğŸ‘‹ æ¬¢è¿å›æ¥ï¼æ–°ä¸€ä»£æ¶æ„å·²éƒ¨ç½²å®Œæˆã€‚è¯·ä»å·¦ä¾§å¯¼èˆªæ é€‰æ‹©åŠŸèƒ½æ¨¡å—ã€‚")

    # 3. æ›´æ–°æ—¥å¿—
    st.subheader("ğŸ“… æ›´æ–°æ—¥å¿— (Changelog)")

    patch_file = settings.PATCH_NOTES_FILE
    if patch_file.exists():
        with st.expander("æŸ¥çœ‹è¯¦ç»†æ›´æ–°è®°å½•", expanded=True):
            try:
                with open(patch_file, "r", encoding="utf-8") as f:
                    st.text(f.read())
            except Exception as e:
                st.error(f"æ— æ³•è¯»å–æ—¥å¿—: {e}")
    else:
        st.warning("æš‚æ— æ›´æ–°æ—¥å¿—æ–‡ä»¶ã€‚")
==================== END FILE: ui/kernel/__init__.py ====================


==================== START FILE: ui/kernel/tab_manager.py ====================
# ui/kernel/tab_manager.py
"""
æ–‡ä»¶è¯´æ˜: åŠ¨æ€ Tab ç®¡ç†å™¨ (Dynamic Tab Manager)
ä¸»è¦åŠŸèƒ½:
1. æ ¹æ® config/modules.json è‡ªåŠ¨æ¸²æŸ“ Tabsã€‚
2. [Fix] Tab çº§æƒé™é‰´æƒï¼šè¿›å…¥ Tab å‰æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ‹¥æœ‰ specific permission keyã€‚
3. [Fix] æ‹¦æˆªæç¤ºï¼šæ— æƒé™æ—¶æ˜¾ç¤ºå‹å¥½æç¤ºï¼Œé˜»æ­¢ä»£ç åŠ è½½ã€‚
"""

import streamlit as st
import importlib
import traceback
from config.settings import settings
from ui.kernel.session import SessionManager
from core.services.auth.service import AuthService


class TabManager:

    @staticmethod
    def render_tabs(page_key: str):
        """
        [ä¸»å…¥å£] æ¸²æŸ“æŒ‡å®šé¡µé¢çš„ Tabs
        """
        # 1. è¯»å–é…ç½®
        modules = settings.load_modules_config()
        module_config = next((m for m in modules if m['key'] == page_key), None)

        if not module_config:
            st.error(f" Configuration not found for module: {page_key}")
            return

        tabs_config = module_config.get("tabs", [])
        if not tabs_config:
            st.info("ï¸ No tabs configured.")
            return

        # 2. ç»˜åˆ¶ Streamlit Tabs
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å…ˆæ¸²æŸ“å‡ºæ‰€æœ‰çš„ Tab æ ‡ç­¾ï¼Œè®©ç”¨æˆ·çœ‹åˆ°â€œæœ‰è¿™ä¸ªåŠŸèƒ½â€
        # ä½†æ˜¯åœ¨ç‚¹è¿›å»ï¼ˆwith tabï¼‰çš„æ—¶å€™ï¼Œå†å¡æƒé™ã€‚
        tab_labels = [t['name'] for t in tabs_config]
        st_tabs = st.tabs(tab_labels)

        # 3. è·å–å½“å‰ç”¨æˆ·æƒé™
        user = SessionManager.get_current_user()
        username = user.get("username")
        is_admin = user.get("is_admin", False)
        # å…¨ç«™ç®¡ç†å‘˜
        is_super = (username == settings.SUPER_ADMIN_USER)

        # å¦‚æœä¸æ˜¯ç®¡ç†å‘˜ï¼Œéœ€è¦æŸ¥è¡¨è·å–ç»†ç²’åº¦æƒé™
        user_perms = {}
        if not is_admin and not is_super:
            user_perms = AuthService.get_permissions(username)

        # 4. å¾ªç¯æ¸²æŸ“å†…å®¹
        for i, tab_obj in enumerate(st_tabs):
            t_conf = tabs_config[i]
            perm_key = t_conf.get("permission")

            with tab_obj:
                # --- æƒé™æ£€æŸ¥é€»è¾‘ ---
                has_access = False

                # è§„åˆ™1: å…¨ç«™ç®¡ç†å‘˜æˆ–æ™®é€šç®¡ç†å‘˜ -> æ”¾è¡Œ
                if is_super or is_admin:
                    has_access = True

                # è§„åˆ™2: å¦‚æœæ²¡æœ‰å®šä¹‰ permission key -> é»˜è®¤æ”¾è¡Œ (Public Tab)
                elif not perm_key:
                    has_access = True

                # è§„åˆ™3: æ™®é€šç”¨æˆ· -> æŸ¥è¡¨
                elif perm_key in user_perms:
                    has_access = True

                # --- æ¸²æŸ“æˆ–æ‹¦æˆª ---
                if has_access:
                    TabManager._load_and_render(t_conf)
                else:
                    st.error(" æ‚¨æ²¡æœ‰æƒé™è®¿é—®æ­¤åŠŸèƒ½æ¨¡å— (Access Denied)")
                    st.info(f"ç¼ºå°‘æƒé™å‡­è¯: `{perm_key}`")
                    st.caption("è¯·è”ç³»ç®¡ç†å‘˜ä¸ºæ‚¨å¼€é€šæ­¤ Tab çš„è®¿é—®æƒé™ã€‚")

    @staticmethod
    def _load_and_render(config):
        """åŠ¨æ€åŠ è½½ä»£ç """
        module_path = config['path']
        try:
            mod = importlib.import_module(module_path)
            if hasattr(mod, "render"):
                mod.render()
            else:
                st.error(f" Module {module_path} missing render()")
        except ImportError:
            st.info(f"ğŸš§ åŠŸèƒ½ [{config['name']}] æ­£åœ¨å»ºè®¾ä¸­...")
        except Exception as e:
            st.error(f"ğŸ”¥ Error loading tab: {config['name']}")
            st.code(traceback.format_exc())
==================== END FILE: ui/kernel/tab_manager.py ====================


==================== START FILE: ui/kernel/error_handler.py ====================
# ui/kernel/error_handler.py
"""
æ–‡ä»¶è¯´æ˜: å…¨å±€é”™è¯¯ç†”æ–­å™¨ (System Error Handler) - V2.0 Smart Debounce
ä¸»è¦åŠŸèƒ½:
1. æ•è·è¿è¡Œæ—¶å¼‚å¸¸ã€‚
2. [Fix] æ™ºèƒ½é˜²æŠ–: ç›¸åŒé”™è¯¯åœ¨ 2 ç§’å†…åªè®°å½•ä¸€æ¬¡ï¼Œé˜²æ­¢æ—¥å¿—åˆ·å±ã€‚
3. [Fix] æ ¼å¼ä¼˜åŒ–: æ¸…æ™°è®°å½• é”™è¯¯ä½ç½®ã€æ‘˜è¦ã€ä»¥åŠå®Œæ•´çš„ Tracebackã€‚
4. æ¸²æŸ“å‹å¥½çš„é”™è¯¯æç¤ºé¡µé¢ (Safe UI)ã€‚
"""

import sys
import traceback
import hashlib
import time
import streamlit as st

from core.sys.logger import get_error_logger
from ui.kernel.session import SessionManager

# è·å–ä¸“ç”¨é”™è¯¯æ—¥å¿—è®°å½•å™¨
error_logger = get_error_logger()


class ErrorHandler:
    # ç®€å•çš„å†…å­˜é˜²æŠ–ç¼“å­˜ (Global level in module)
    # ç»“æ„: {error_hash: timestamp}
    _error_cache = {}
    DEBOUNCE_SECONDS = 2.0

    @staticmethod
    def handle_exception(module_name: str, exception: Exception):
        """
        [æ ¸å¿ƒ] ç»Ÿä¸€å¼‚å¸¸å¤„ç†é€»è¾‘ (å¸¦é˜²æŠ–)
        """
        # 1. è·å–å¼‚å¸¸ä¸Šä¸‹æ–‡
        exc_type, exc_value, exc_traceback = sys.exc_info()

        # 2. ç”Ÿæˆé”™è¯¯æŒ‡çº¹ (Fingerprint)
        # ç»„åˆ: æ¨¡å—å + å¼‚å¸¸ç±»å‹ + å¼‚å¸¸æ¶ˆæ¯
        error_signature = f"{module_name}:{str(exc_type)}:{str(exc_value)}"
        error_hash = hashlib.md5(error_signature.encode('utf-8')).hexdigest()

        current_time = time.time()
        last_time = ErrorHandler._error_cache.get(error_hash, 0)

        # [é˜²æŠ–æ£€æŸ¥] å¦‚æœ 2ç§’å†…å·²è®°å½•è¿‡åŒç±»é”™è¯¯ï¼Œè·³è¿‡æ—¥å¿—è®°å½•
        should_log = (current_time - last_time) > ErrorHandler.DEBOUNCE_SECONDS

        if should_log:
            # æ›´æ–°æ—¶é—´æˆ³
            ErrorHandler._error_cache[error_hash] = current_time

            # --- æ‰§è¡Œæ—¥å¿—è®°å½• ---

            # A. æå–æºå¤´
            tb_list = traceback.extract_tb(exc_traceback)
            error_file = "Unknown"
            error_func = "Unknown"
            if tb_list:
                last_frame = tb_list[-1]
                error_file = last_frame.filename
                error_func = last_frame.name

            # B. è·å–ç”¨æˆ·
            user_ctx = SessionManager.get_current_user() or {}
            username = user_ctx.get("username", "Guest")

            # C. æ„é€ å®Œæ•´å †æ ˆ
            full_trace = "".join(traceback.format_exception(exc_type, exc_value, exc_traceback))

            # D. å†™å…¥æ—¥å¿—
            # Message ä»…åŒ…å«æ‘˜è¦ï¼ŒDetail æ”¾å…¥ extra
            log_summary = f"[{module_name}] {exc_type.__name__}: {str(exc_value)}"

            error_logger.error(
                log_summary,
                extra={
                    "user": username,
                    "error_path": error_file,
                    "error_func": error_func,
                    # å°† Traceback é™„åœ¨ message åæˆ–ç‹¬ç«‹è®°å½•ï¼Œå–å†³äº formatterã€‚
                    # è¿™é‡Œ logger formatter ä¼šè‡ªåŠ¨å¤„ç†æ ¼å¼ã€‚
                }
            )
            # å¯ä»¥åœ¨è¿™é‡Œé¢å¤– print å †æ ˆåˆ° stderr ä»¥ä¾¿è°ƒè¯•
            # sys.stderr.write(full_trace)

        # 3. æ¸²æŸ“å‰å°å‹å¥½ç•Œé¢ (Safe UI)
        # æ¸…é™¤å½“å‰å¯èƒ½çš„è„æ¸²æŸ“
        try:
            st.empty()
        except:
            pass

        st.markdown("---")
        st.error(f"ğŸš« ç³»ç»Ÿé‡åˆ°é—®é¢˜: {module_name}")
        st.warning("æŠ€æœ¯æ”¯æŒä»£ç  (Error Ref): " + error_hash[:8])

        if st.button("ğŸ  è¿”å›ç³»ç»Ÿé¦–é¡µ (Home)", type="primary", use_container_width=True):
            st.rerun()
==================== END FILE: ui/kernel/error_handler.py ====================


==================== START FILE: ui/kernel/router.py ====================
# ui/kernel/router.py
"""
æ–‡ä»¶è¯´æ˜: åŠ¨æ€è·¯ç”±å¼•æ“ (Dynamic Router) - Refactored Controller
ä¸»è¦åŠŸèƒ½:
1. ä½œä¸º MVC æ¨¡å¼ä¸­çš„ Controllerï¼Œåè°ƒå„ç»„ä»¶å·¥ä½œã€‚
2. æƒé™é‰´æƒ (RBAC)ã€‚
3. è°ƒç”¨ ErrorHandler è¿›è¡Œå…¨å±€é”™è¯¯ç†”æ–­ã€‚
"""

import streamlit as st
import importlib
from typing import Dict

# å¼•å…¥é…ç½®ä¸æ‹†åˆ†åçš„ç»„ä»¶
from config.settings import settings
from ui.kernel.session import SessionManager
from ui.kernel.layout import render_header, set_main_background_video
from ui.kernel.smart_info import SmartInfoService
from ui.kernel.components import UIComponents
from ui.kernel.error_handler import ErrorHandler
from core.services.auth.service import AuthService


class Router:

    def __init__(self):
        # 1. åŠ è½½é…ç½®
        self.modules = settings.load_modules_config()
        # 2. åˆå§‹åŒ–ä¼šè¯ç¯å¢ƒ
        SessionManager.init_session()
        SessionManager.ensure_context()

    def render(self):
        """[ä¸»å…¥å£] æ¸²æŸ“å…¨ç«™æµç¨‹"""
        user = SessionManager.get_current_user()

        # 1. å…¨å±€èƒŒæ™¯
        set_main_background_video()

        # 2. æœªç™»å½• -> æ¸²æŸ“ç™»å½•é¡µ
        if not user:
            UIComponents.render_login_page()
            return

        # 3. å·²ç™»å½• -> æ¸²æŸ“ä¸»ç•Œé¢
        # Header ç½®é¡¶
        render_header()
        # Sidebar + Content
        self._render_sidebar(user)

    def _render_sidebar(self, user):
        """æ¸²æŸ“ä¾§è¾¹æ æ§åˆ¶åŒº"""
        username = user.get("username", "Guest")
        ip = user.get("ip", "UNKNOWN")
        is_admin = user.get("is_admin", False)

        # 1. èº«ä»½åˆ¤å®šé€»è¾‘
        if username == settings.SUPER_ADMIN_USER:
            role_tag = "ğŸ‘‘ å…¨ç«™ç®¡ç†å‘˜"
            role_style = "color: #FFD700; border: 1px solid #FFD700;"
        elif is_admin:
            role_tag = "ğŸ›¡ï¸ ç®¡ç†å‘˜"
            role_style = "color: #4EC9B0; border: 1px solid #4EC9B0;"
        else:
            role_tag = "ğŸŸ¢ æ™®é€šç”¨æˆ·"
            role_style = "color: #aaa; border: 1px solid #aaa;"

        # 2. è·å–æ™ºèƒ½ä¿¡æ¯ (è°ƒç”¨ Service)
        smart_info = SmartInfoService.get_smart_info(ip)

        # 3. æ¸²æŸ“ç”¨æˆ·ä¿¡æ¯å¡ç‰‡ (è°ƒç”¨ Components)
        UIComponents.render_user_card(username, role_tag, role_style, ip, smart_info)

        # 4. é€€å‡ºæŒ‰é’®
        if st.sidebar.button("ğŸšª é€€å‡ºç™»å½• (Logout)", use_container_width=True):
            SessionManager.logout()

        st.sidebar.markdown("---")
        st.sidebar.markdown("### â‰¡ å¯¼èˆªèœå•")

        # 5. æƒé™è¿‡æ»¤
        visible_modules = [m for m in self.modules if self._check_permission(user, m)]

        if not visible_modules:
            st.sidebar.error("No modules available for your role.")
            return

        # 6. å¯¼èˆªé€‰æ‹©å™¨
        nav_key = "router_nav_selection"
        options_map = {m['name']: m for m in visible_modules}

        selected_name = st.sidebar.radio(
            "Go to",
            list(options_map.keys()),
            label_visibility="collapsed",
            key=nav_key
        )

        # 7. åŠ è½½ç›®æ ‡é¡µé¢
        if selected_name:
            target_config = options_map[selected_name]
            self._load_page(target_config)

    def _check_permission(self, user: Dict, config: Dict) -> bool:
        """[RBAC] æƒé™æ£€æŸ¥"""
        if not config.get("enabled", True): return False

        username = user.get("username")
        is_admin = user.get("is_admin", False)

        # Super Admin é€šè¡Œ
        if username == settings.SUPER_ADMIN_USER: return True

        perm_key = config.get("permission", "public")

        if perm_key == "public": return True
        if perm_key == "admin_only": return is_admin

        # Admin é»˜è®¤æ‹¥æœ‰ä¸šåŠ¡æƒé™ï¼Œä½†ä¹Ÿå…è®¸è¢«æ˜¾å¼é™åˆ¶(è§†ä¸šåŠ¡éœ€æ±‚)
        if is_admin: return True

        # æ™®é€šç”¨æˆ·æŸ¥è¡¨
        user_perms = AuthService.get_permissions(username)
        return user_perms.get(perm_key, False)

    def _load_page(self, config):
        """[Loader] åŠ¨æ€åŠ è½½æ¨¡å—ï¼Œå§”æ‰˜ ErrorHandler å¤„ç†å¼‚å¸¸"""
        module_path = config['path']
        try:
            # åŠ¨æ€å¯¼å…¥
            mod = importlib.import_module(module_path)

            # æ¸²æŸ“é¡µé¢
            if hasattr(mod, "render"):
                with st.container():
                    mod.render()
            else:
                st.error(f" æ¨¡å— {module_path} ç¼ºå°‘ render() å‡½æ•°")

        except ImportError:
            st.info(f"ğŸš§ åŠŸèƒ½ [{config['name']}] æ­£åœ¨å»ºè®¾ä¸­...")

        except Exception as e:
            # [Fix] ä½¿ç”¨ä¸“ç”¨çš„ ErrorHandler è¿›è¡Œç†”æ–­å¤„ç†
            ErrorHandler.handle_exception(config.get('name', module_path), e)
==================== END FILE: ui/kernel/router.py ====================


==================== START FILE: ui/kernel/components.py ====================
# ui/kernel/components.py
"""
æ–‡ä»¶è¯´æ˜: UI ç»„ä»¶åº“ (UI Components Library)
ä¸»è¦åŠŸèƒ½:
1. å°è£…å¤æ‚çš„ UI æ¸²æŸ“é€»è¾‘ (HTML/CSS)ã€‚
2. æä¾›ç™»å½•é¡µé¢æ¸²æŸ“å™¨ (Login Page)ã€‚
3. æä¾›ä¾§è¾¹æ ç”¨æˆ·ä¿¡æ¯å¡ç‰‡æ¸²æŸ“å™¨ (User Info Card)ã€‚
4. ä¿æŒ UI é£æ ¼ä¸€è‡´æ€§ (Glassmorphism)ã€‚
"""

import streamlit as st
import time
from typing import Dict

from config.settings import settings
from ui.kernel.layout import get_base64_encoded_file
from ui.kernel.session import SessionManager


class UIComponents:

    @staticmethod
    def render_login_page():
        """
        æ¸²æŸ“å®Œæ•´çš„ç™»å½•é¡µé¢
        åŒ…å«: 10å€å¤§Logo, ç»ç’ƒæ‹Ÿæ€å¡ç‰‡, ç‰ˆæœ¬é¡µè„š
        """
        # å±…ä¸­å¸ƒå±€
        _, col, _ = st.columns([1, 1.2, 1])

        with col:
            # 1. Logo åŠ è½½
            logo_path = settings.ASSETS_DIR / "Logo.png"
            logo_b64 = get_base64_encoded_file(logo_path)

            if logo_b64:
                # è§†è§‰å†²å‡»åŠ›: 100% å®½åº¦
                logo_html = f'<img src="data:image/png;base64,{logo_b64}" style="width: 100%; max-width: 800px; margin-bottom: 30px;">'
            else:
                logo_html = '<div style="font-size: 200px; margin-bottom: 20px; line-height: 1;">ğŸ¦…</div>'

            # 2. ç»ç’ƒæ‹Ÿæ€å¡ç‰‡
            card_html = f"""
            <div style="
                margin-top: 60px;
                padding: 50px 40px 30px 40px;
                background: rgba(20, 20, 30, 0.85);
                border-radius: 20px;
                border: 1px solid rgba(255, 255, 255, 0.1);
                box-shadow: 0 25px 60px rgba(0,0,0,0.6);
                text-align: center;
                backdrop-filter: blur(12px);
            ">
                {logo_html}
                <div style="
                    font-size: 28px; 
                    font-weight: 800; 
                    color: #fff; 
                    letter-spacing: 2px;
                    margin-bottom: 8px;
                    text-transform: uppercase;
                ">{settings.APP_NAME}</div>
                <div style="
                    font-size: 14px; 
                    color: #4EC9B0; 
                    font-family: 'Courier New', monospace; 
                    margin-bottom: 30px;
                    font-weight: bold;
                ">Enterprise Intelligence Platform</div>
            </div>
            """
            st.markdown(card_html, unsafe_allow_html=True)

            # 3. ç™»å½•è¡¨å•
            with st.form("login_form"):
                st.markdown('<div style="height: 15px;"></div>', unsafe_allow_html=True)

                u = st.text_input("ç”¨æˆ·å", placeholder="è¯·è¾“å…¥è´¦å·")
                p = st.text_input("å¯†ç ", type="password", placeholder="è¯·è¾“å…¥å¯†ç ")

                st.markdown('<div style="height: 20px;"></div>', unsafe_allow_html=True)

                if st.form_submit_button("ç™»å½•ç³»ç»Ÿ (Sign In)", type="primary", use_container_width=True):
                    with st.spinner("æ­£åœ¨éªŒè¯èº«ä»½..."):
                        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿæ„Ÿ
                        time.sleep(0.5)
                        if SessionManager.login(u, p):
                            st.success(" éªŒè¯é€šè¿‡ï¼Œæ¬¢è¿å›æ¥ï¼")
                            time.sleep(0.5)
                            st.rerun()

            # 4. é¡µè„šä¿¡æ¯
            footer_html = f"""
            <div style="
                text-align: center; 
                color: rgba(255,255,255,0.4); 
                font-size: 11px; 
                margin-top: 30px; 
                line-height: 1.8;
                font-family: 'Consolas', 'Monaco', monospace;
            ">
                <div>Current System Version: {settings.APP_VERSION}</div>
                <div>Updated by: {settings.AUTHOR}</div>
                <div>Released on: {settings.VERSION_DATE}</div>
            </div>
            """
            st.markdown(footer_html, unsafe_allow_html=True)

    @staticmethod
    def render_user_card(username: str, role_tag: str, role_style: str, ip: str, smart_info: Dict[str, str]):
        """
        æ¸²æŸ“ä¾§è¾¹æ ç”¨æˆ·ä¿¡æ¯å¡ç‰‡
        """
        card_html = f"""
        <div style="
            padding: 20px 15px; 
            background: rgba(25, 25, 35, 0.95); 
            border-radius: 12px; 
            border: 1px solid rgba(255,255,255,0.1); 
            margin-bottom: 20px; 
            box-shadow: 0 4px 15px rgba(0,0,0,0.3);
        ">
            <div style="font-size: 16px; font-weight: bold; color: #4EC9B0; margin-bottom: 5px;">
                {smart_info['weather_icon']} {smart_info['greeting']}
            </div>
            <div style="font-size: 12px; color: #aaa; margin-bottom: 15px; font-style: italic;">
                {smart_info['tips']}
            </div>
            <div style="display: flex; align-items: center; margin-bottom: 10px;">
                <div style="font-size: 28px; margin-right: 10px;">ğŸ‘¤</div>
                <div>
                    <div style="font-size: 18px; font-weight: 800; color: #FFF; letter-spacing: 0.5px;">
                        {username}
                    </div>
                    <div style="
                        font-size: 11px; 
                        {role_style} 
                        border-radius: 4px; 
                        padding: 1px 6px; 
                        display: inline-block; 
                        margin-top: 4px;
                    ">
                        {role_tag}
                    </div>
                </div>
            </div>
            <hr style="border-color: rgba(255,255,255,0.1); margin: 10px 0;">
            <div style="font-size: 12px; color: #ccc; line-height: 1.6;">
                <div>ğŸ“ {smart_info['city']}</div>
                <div>ğŸŒ¡ï¸ {smart_info['temp']} {smart_info['weather_desc']}</div>
                <div>ğŸŒ {ip}</div>
            </div>
        </div>
        """
        st.sidebar.markdown(card_html, unsafe_allow_html=True)
==================== END FILE: ui/kernel/components.py ====================


==================== START FILE: ui/kernel/smart_info.py ====================
# ui/kernel/smart_info.py
"""
æ–‡ä»¶è¯´æ˜: æ™ºèƒ½ä¿¡æ¯æœåŠ¡ (Smart Info Service)
ä¸»è¦åŠŸèƒ½:
1. è·å–å®¢æˆ·ç«¯ IP åŠå…¶åœ°ç†ä½ç½® (GeoIP)ã€‚
2. è·å–å®æ—¶å¤©æ°”æ•°æ® (Open-Meteo API)ã€‚
3. ç”ŸæˆåŸºäºåœºæ™¯çš„æš–å¿ƒé—®å€™è¯­ (Contextual Greetings)ã€‚
4. ä¸¥æ ¼å¤„ç†æ—¶åŒºè½¬æ¢ï¼Œç¡®ä¿æ—¶é—´æ˜¾ç¤ºå‡†ç¡®ã€‚
"""

import requests
import datetime
import random
import streamlit as st
from typing import Dict, Any


class SmartInfoService:
    # è¯­æ–™åº“ï¼šæ ¹æ®åœºæ™¯ç”Ÿæˆæš–å¿ƒæç¤º
    TIPS_DB = {
        "morning": [
            "ä¸€æ—¥ä¹‹è®¡åœ¨äºæ™¨ï¼Œæ„¿ä»Šå¤©ä»£ç æ— Bugï¼â˜•",
            "åˆæ˜¯å……æ»¡å¸Œæœ›çš„ä¸€å¤©ï¼ŒåŠ æ²¹ï¼ğŸ’ª",
            "æ™¨å…‰ç†¹å¾®ï¼Œè®°å¾—åƒæ—©é¤å“¦ ğŸ¥ª",
            "ä¿æŒä¸“æ³¨ï¼Œä»Šå¤©çš„æ•ˆç‡ä¸€å®šå¾ˆé«˜ "
        ],
        "noon": [
            "åˆä¼‘æ—¶é—´åˆ°ï¼Œç»™å¤§è„‘å……ä¸ªç”µå§ ğŸ”‹",
            "è®°å¾—æŒ‰æ—¶åƒé¥­ï¼Œèº«ä½“æ˜¯é©å‘½çš„æœ¬é’± ğŸ±",
            "å°æ†©ç‰‡åˆ»ï¼Œä¸‹åˆç²¾ç¥ç™¾å€ ğŸ’¤"
        ],
        "afternoon": [
            "ä¸‹åˆå¥½ï¼Œæ¥æ¯å’–å•¡ææç¥ï¼Ÿâ˜•",
            "ä¿æŒèŠ‚å¥ï¼Œèƒœåˆ©å°±åœ¨çœ¼å‰ ğŸš€",
            "ç°åœ¨çš„åŠªåŠ›ï¼Œéƒ½æ˜¯ä¸ºäº†æ›´å¥½çš„æœªæ¥ ğŸŒŸ",
            "ä¼¸ä¸ªæ‡’è…°ï¼Œæ´»åŠ¨ä¸€ä¸‹ç­‹éª¨å§ ğŸ§˜"
        ],
        "evening": [
            "å¤œå¹•é™ä¸´ï¼Œä»Šå¤©çš„æˆå°±å·²è¶³å¤Ÿé—ªè€€ ",
            "å·¥ä½œè¾›è‹¦äº†ï¼Œæ„¿è¿™å¤œæ™šæ¸©æŸ”å¾…ä½  ğŸŒ™",
            "è¯¥æ”¾æ¾ä¸€ä¸‹äº†ï¼Œäº«å—å±äºä½ çš„æ—¶å…‰ ğŸ·"
        ],
        "midnight": [
            "æ˜Ÿå…‰ä¸é—®èµ¶è·¯äººï¼Œä½†ä¹Ÿè¯·æ—©ç‚¹ä¼‘æ¯ ğŸ›Œ",
            "å¤œæ·±äº†ï¼Œå¥åº·ç¬¬ä¸€ï¼Œå¿«å»ç¡å§ ğŸŒ™",
            "å…¨ä¸–ç•Œéƒ½ç¡äº†ï¼Œæ™šå®‰ï¼Œè¿½æ¢¦äºº ğŸŒ "
        ],
        "rain": [
            "å¬ç€é›¨å£°å†™ä»£ç ï¼Œä¹Ÿæ˜¯ä¸€ç§æµªæ¼« ğŸŒ§ï¸",
            "å‡ºé—¨è®°å¾—å¸¦ä¼ï¼Œåˆ«æ·‹æ¹¿äº† â˜‚ï¸",
            "é›¨å¤©è·¯æ»‘ï¼Œé€šå‹¤è·¯ä¸Šæ³¨æ„å®‰å…¨ ğŸš—"
        ],
        "hot": [
            "å¤©æ°”ç‚çƒ­ï¼Œè®°å¾—å¤šå–æ°´é™æ¸© ğŸ¥¤",
            "æ³¨æ„é˜²æš‘ï¼Œå¿ƒé™è‡ªç„¶å‡‰ ğŸƒ"
        ],
        "cold": [
            "æ°”æ¸©è¾ƒä½ï¼Œæ³¨æ„ä¿æš–ï¼Œåˆ«ç€å‡‰äº† ğŸ§£",
            "æ‰‹å†·å—ï¼Ÿæ§æ¯çƒ­èŒ¶æš–æš–æ‰‹ ğŸµ"
        ],
        "friday": [
            "å‘¨äº”äº†ï¼å‘¨æœ«å°±åœ¨å‘ä½ æ‹›æ‰‹ ğŸ‰",
            "åšæŒä¸€ä¸‹ï¼Œé©¬ä¸Šå°±æ˜¯å¿«ä¹å‘¨æœ«ï¼ğŸ»"
        ],
        "weekend": [
            "å‘¨æœ«æ„‰å¿«ï¼äº«å—ç”Ÿæ´»ï¼Œå½»åº•æ”¾æ¾ ğŸ–ï¸",
            "å……å……ç”µï¼Œä¸ºäº†ä¸‹å‘¨æ›´å¥½çš„å‡ºå‘ ğŸ”‹"
        ]
    }

    @staticmethod
    def _generate_warm_tips(hour: int, weather_desc: str, temp_c: float) -> str:
        """æ ¹æ® æ—¶é—´ + å¤©æ°” + æ˜ŸæœŸ ç”Ÿæˆæ–‡æ¡ˆ"""
        weekday = datetime.datetime.now().weekday()  # 0=Mon, 6=Sun
        candidates = []

        # 1. æç«¯å¤©æ°”ä¼˜å…ˆ
        if "é›¨" in weather_desc: candidates.extend(SmartInfoService.TIPS_DB["rain"])
        if temp_c > 32: candidates.extend(SmartInfoService.TIPS_DB["hot"])
        if temp_c < 10: candidates.extend(SmartInfoService.TIPS_DB["cold"])

        # 2. ç‰¹æ®Šæ—¶é—´ç‚¹ (å‘¨äº”/å‘¨æœ«)
        if weekday == 4:
            candidates.extend(SmartInfoService.TIPS_DB["friday"])
        elif weekday >= 5:
            candidates.extend(SmartInfoService.TIPS_DB["weekend"])

        # 3. åŸºç¡€æ—¶é—´æ®µå…œåº•
        if 5 <= hour < 11:
            candidates.extend(SmartInfoService.TIPS_DB["morning"])
        elif 11 <= hour < 13:
            candidates.extend(SmartInfoService.TIPS_DB["noon"])
        elif 13 <= hour < 18:
            candidates.extend(SmartInfoService.TIPS_DB["afternoon"])
        elif 18 <= hour < 23:
            candidates.extend(SmartInfoService.TIPS_DB["evening"])
        else:
            candidates.extend(SmartInfoService.TIPS_DB["midnight"])

        return random.choice(candidates) if candidates else "Welcome Back!"

    @staticmethod
    @st.cache_data(ttl=3600)
    def get_smart_info(ip: str) -> Dict[str, str]:
        """
        [æ ¸å¿ƒé€»è¾‘] è·å–åœ°ç†ä½ç½®å’Œå¤©æ°”
        """
        info = {
            "city": "Unknown City", "weather_icon": "ğŸŒ¤ï¸", "weather_desc": "N/A",
            "temp": "--", "greeting": "Hello", "tips": "Welcome back!"
        }

        query_ip = ip
        # æœ¬åœ°å¼€å‘ç¯å¢ƒå¤„ç†
        if ip in ["UNKNOWN", "127.0.0.1", "localhost", "::1"]:
            try:
                public_ip = requests.get('https://api.ipify.org', timeout=1.5).text
                if public_ip:
                    query_ip = public_ip
                    info["city"] = "Localhost (Dev)"
            except:
                info["city"] = "Localhost"
                return info

        try:
            # 1. IP å®šä½ (IP-API)
            loc_url = f"http://ip-api.com/json/{query_ip}"
            loc_res = requests.get(loc_url, timeout=3).json()

            if loc_res.get("status") == "success":
                lat, lon = loc_res["lat"], loc_res["lon"]
                real_city = loc_res.get("city", "Unknown")
                timezone_str = loc_res.get("timezone", "UTC")

                if info["city"] == "Localhost (Dev)":
                    info["city"] = f"{real_city} (Local Dev)"
                else:
                    info["city"] = real_city

                # 2. å¤©æ°”æŸ¥è¯¢ (Open-Meteo)
                w_url = f"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true&timezone={timezone_str}"
                w_res = requests.get(w_url, timeout=3).json()

                temp_val = 25.0

                if "current_weather" in w_res:
                    curr = w_res["current_weather"]
                    temp_val = curr["temperature"]
                    code = curr["weathercode"]
                    is_day = curr["is_day"]

                    info["temp"] = f"{temp_val}Â°C"

                    # WMO Code Mapping
                    if code == 0:
                        info["weather_desc"] = "æ™´æœ—"
                        info["weather_icon"] = "â˜€ï¸" if is_day else "ğŸŒ™"
                    elif code in [1, 2, 3]:
                        info["weather_desc"] = "å¤šäº‘"
                        info["weather_icon"] = "â›…"
                    elif code in [45, 48]:
                        info["weather_desc"] = "é›¾"
                        info["weather_icon"] = "ğŸŒ«ï¸"
                    elif 51 <= code <= 67:
                        info["weather_desc"] = "é›¨"
                        info["weather_icon"] = "ğŸŒ§ï¸"
                    elif 71 <= code <= 77:
                        info["weather_desc"] = "é›ª"
                        info["weather_icon"] = "â„ï¸"
                    elif code >= 95:
                        info["weather_desc"] = "é›·æš´"
                        info["weather_icon"] = "â›ˆï¸"
                    else:
                        info["weather_desc"] = "é˜´"
                        info["weather_icon"] = "â˜ï¸"

                # 3. æ—¶åŒºä¸é—®å€™è¯­ç”Ÿæˆ
                try:
                    import pytz
                    target_tz = pytz.timezone(timezone_str)
                    local_now = datetime.datetime.now(target_tz)
                    local_hour = local_now.hour
                except:
                    local_now = datetime.datetime.now()
                    local_hour = local_now.hour

                if 5 <= local_hour < 11:
                    info["greeting"] = "æ—©ä¸Šå¥½"
                elif 11 <= local_hour < 13:
                    info["greeting"] = "ä¸­åˆå¥½"
                elif 13 <= local_hour < 18:
                    info["greeting"] = "ä¸‹åˆå¥½"
                elif 18 <= local_hour < 23:
                    info["greeting"] = "æ™šä¸Šå¥½"
                else:
                    info["greeting"] = "å¤œæ·±äº†"

                info["tips"] = SmartInfoService._generate_warm_tips(local_hour, info["weather_desc"], temp_val)

        except Exception:
            pass

        return info
==================== END FILE: ui/kernel/smart_info.py ====================


==================== START FILE: tools/normalize_schema.py ====================
# tools/normalize_schema.py
"""
æ–‡ä»¶è¯´æ˜: æ•°æ®åº“æ—¥æœŸæ ¼å¼æ ‡å‡†åŒ–å·¥å…· (One-time Tool)
è¿è¡Œæ–¹å¼: python tools/normalize_schema.py
ä¸»è¦åŠŸèƒ½:
1. è§„èŒƒåŒ–äº¤æ˜“è¡¨å’ŒæŠ¥è¡¨çš„ç¬¬ä¸€åˆ—æ—¥æœŸä¸º YYYY-MM-DDã€‚
2. è§„èŒƒåŒ–åº“å­˜è¡¨çš„åˆ—åä¸º YYYY-MM-DD (è‡ªåŠ¨è®¡ç®—æœˆæœ«å·¥ä½œæ—¥)ã€‚
"""

import sys
import os
from pathlib import Path
import pandas as pd
from dateutil import parser
from pandas.tseries.offsets import BMonthEnd
from sqlalchemy import text

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ Pathï¼Œä»¥ä¾¿å¯¼å…¥ config
project_root = Path(__file__).resolve().parent.parent
sys.path.append(str(project_root))

from core.components.db.client import DBClient

# å®šä¹‰ç›®æ ‡è¡¨å
TABLE_TRANS = "Data_Transaction_Done"  # ç”¨æˆ·æŒ‡å®šçš„æ–°è¡¨å
TABLE_CLEAN = "Data_Clean_Log"
TABLE_INV = "Data_Inventory"


def normalize_date_str(date_val):
    """å°è¯•å°†å„ç§æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸º YYYY-MM-DD"""
    if pd.isna(date_val): return None
    s = str(date_val).strip()
    try:
        # ä¼˜å…ˆå°è¯•æ ‡å‡†æ ¼å¼
        dt = pd.to_datetime(s, errors='raise')
        return dt.strftime("%Y-%m-%d")
    except:
        try:
            # å°è¯• dateutil è§£æ (æ”¯æŒ Jun 30, 2025 ç­‰)
            dt = parser.parse(s)
            return dt.strftime("%Y-%m-%d")
        except:
            return s  # è§£æå¤±è´¥ä¿æŒåŸæ ·


def get_last_business_day(year_month_str):
    """
    è¾“å…¥ '2025-06', è¿”å›è¯¥æœˆæœ€åä¸€ä¸ªå·¥ä½œæ—¥ '2025-06-30'
    """
    try:
        dt = pd.to_datetime(year_month_str)
        # BMonthEnd: Business Month End
        offset = BMonthEnd()
        last_bd = offset.rollforward(dt)
        return last_bd.strftime("%Y-%m-%d")
    except:
        return None


def run_normalization():
    print("ğŸš€ å¼€å§‹æ‰§è¡Œæ•°æ®åº“æ—¥æœŸæ ‡å‡†åŒ–...")

    # 1. å¤„ç†è¡Œæ•°æ®æ—¥æœŸ (Data_Transaction_Done & Data_Clean_Log)
    # æ³¨æ„: è¿™é‡Œå‡è®¾ç¬¬ä¸€åˆ—å°±æ˜¯æ—¥æœŸåˆ—ã€‚é€šå¸¸æ˜¯ 'transaction creation date' æˆ– 'order date'
    row_tables = [TABLE_TRANS, TABLE_CLEAN]

    for tbl in row_tables:
        print(f"ğŸ“‹ æ­£åœ¨æ£€æŸ¥è¡¨: {tbl} ...")
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            check_sql = f"SHOW TABLES LIKE '{tbl}'"
            if DBClient.read_df(check_sql).empty:
                print(f"   âš ï¸ è¡¨ {tbl} ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
                continue

            # è¯»å–å…¨éƒ¨æ•°æ®
            df = DBClient.read_df(f"SELECT * FROM `{tbl}`")
            if df.empty:
                print(f"   ï¸ è¡¨ {tbl} ä¸ºç©ºã€‚")
                continue

            # è·å–ç¬¬ä¸€åˆ—åˆ—å
            date_col = df.columns[0]
            print(f"   - é”å®šæ—¥æœŸåˆ—: [{date_col}]")

            # æ‰§è¡Œè½¬æ¢
            original_dates = df[date_col].tolist()
            new_dates = [normalize_date_str(d) for d in original_dates]

            # æ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–
            if original_dates == new_dates:
                print("    æ—¥æœŸæ ¼å¼å·²ç¬¦åˆæ ‡å‡†ï¼Œæ— éœ€ä¿®æ”¹ã€‚")
                continue

            # æ›´æ–°æ•°æ®åº“ (æ‰¹é‡æ›´æ–°æ•ˆç‡ä½ï¼Œè¿™é‡Œä½¿ç”¨å…¨é‡è¦†ç›–ç­–ç•¥)
            # ä¸ºäº†å®‰å…¨ï¼Œä½¿ç”¨ä¸´æ—¶è¡¨ç­–ç•¥
            print(f"   ğŸ”„ å‘ç°æ ¼å¼å·®å¼‚ï¼Œæ­£åœ¨æ ‡å‡†åŒ– {len(df)} è¡Œæ•°æ®...")
            df[date_col] = new_dates

            with DBClient.atomic_transaction() as conn:
                # 1. å†™å…¥ä¸´æ—¶è¡¨
                temp_tbl = f"{tbl}_temp_norm"
                df.to_sql(temp_tbl, conn, if_exists='replace', index=False)

                # 2. äº¤æ¢è¡¨å (åŸå­æ“ä½œ)
                # MySQLä¸æ”¯æŒç›´æ¥ rename è¦†ç›–ï¼Œéœ€è¦ drop old
                conn.execute(text(f"DROP TABLE `{tbl}`"))
                conn.execute(text(f"RENAME TABLE `{temp_tbl}` TO `{tbl}`"))

            print(f"    è¡¨ {tbl} æ›´æ–°å®Œæˆã€‚")

        except Exception as e:
            print(f"    å¤„ç†è¡¨ {tbl} å¤±è´¥: {e}")

    # 2. å¤„ç†åˆ—åæ—¥æœŸ (Data_Inventory)
    print(f"ğŸ“‹ æ­£åœ¨æ£€æŸ¥è¡¨: {TABLE_INV} (åˆ—åæ ‡å‡†åŒ–)...")
    try:
        # è·å–å½“å‰åˆ—å
        df_inv = DBClient.read_df(f"SELECT * FROM `{TABLE_INV}` LIMIT 1")
        if df_inv.empty:
            # å¦‚æœè¡¨ä¸ºç©ºï¼Œä¹Ÿéœ€è¦è·å–åˆ—ç»“æ„
            # é‡æ–°æŸ¥è¯¢ schema
            pass

        curr_cols = df_inv.columns.tolist()
        rename_map = {}

        for col in curr_cols:
            # åŒ¹é… YYYY-MM æ ¼å¼ (ç®€å•æ­£åˆ™: 4æ•°å­—-2æ•°å­—)
            # æ’é™¤å·²æ˜¯ YYYY-MM-DD (10ä½) çš„
            if len(col) == 7 and col[4] == '-' and col[:4].isdigit() and col[5:].isdigit():
                new_col = get_last_business_day(col)
                if new_col and new_col != col:
                    rename_map[col] = new_col

        if not rename_map:
            print(f"    {TABLE_INV} åˆ—åå·²ç¬¦åˆæ ‡å‡†ã€‚")
        else:
            print(f"   ğŸ”„ éœ€è¦é‡å‘½åçš„åˆ—: {rename_map}")
            with DBClient.atomic_transaction() as conn:
                for old_c, new_c in rename_map.items():
                    # ç”Ÿæˆ ALTER TABLE è¯­å¥
                    # æ³¨æ„: CHANGE COLUMN éœ€è¦æŒ‡å®šåˆ—ç±»å‹ï¼Œè¿™é‡Œå‡è®¾æ˜¯ INT (åº“å­˜æ•°é‡)
                    # ä¸ºäº†ç¨³å¦¥ï¼Œå…ˆæŸ¥ä¸€ä¸‹ç±»å‹? å‡è®¾é»˜è®¤æ˜¯ INT DEFAULT 0
                    sql = f"ALTER TABLE `{TABLE_INV}` CHANGE COLUMN `{old_c}` `{new_c}` INT DEFAULT 0"
                    conn.execute(text(sql))
                    print(f"      - Renamed: {old_c} -> {new_c}")
            print(f"    {TABLE_INV} åˆ—åæ›´æ–°å®Œæˆã€‚")

    except Exception as e:
        print(f"    å¤„ç†è¡¨ {TABLE_INV} å¤±è´¥: {e}")

    print("ğŸ‰ æ‰€æœ‰æ ‡å‡†åŒ–ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚")


if __name__ == "__main__":
    run_normalization()
==================== END FILE: tools/normalize_schema.py ====================


==================== START FILE: tools/pack_project.py ====================
# tools/pack_project.py
import os
import datetime

# === é…ç½®åŒºåŸŸ ===
EXTENSIONS = ['.py', '.md', '.json', '.sh', '.env']
# è¡¥ä¸è¯´æ˜æ–‡ä»¶çš„å¯èƒ½è·¯å¾„ (ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•)
# ä¼˜å…ˆæŸ¥æ‰¾ ui/assets/ ä¸‹çš„æ–‡ä»¶
POSSIBLE_PATHS = [
    'ui/assets/patch_notes.txt',
    'assets/patch_notes.txt',
    'patch_notes.txt'
]

IGNORE_DIRS = {
    '.venv', '.git', '__pycache__', '.idea', '.vscode',
    'backup', 'VersionHistory', 'output', 'Temp', 'uploader'
}

OUTPUT_DIR_NAME = 'VersionHistory'


def get_project_root():
    """è·å–é¡¹ç›®æ ¹ç›®å½• (å³ tools çš„ä¸Šä¸€çº§)"""
    current_script = os.path.abspath(__file__)
    return os.path.dirname(os.path.dirname(current_script))


def get_version_info(project_root):
    """
    å°è¯•ä» patch_notes.txt è¯»å–ç‰ˆæœ¬å· (VERSION=V1.x.x)
    å¦‚æœè¯»å–å¤±è´¥ï¼Œåˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ (Date_YYYYMMDD) ä½œä¸ºå…œåº•ã€‚
    """
    target_path = None
    # åœ¨å¯èƒ½çš„è·¯å¾„ä¸­å¯»æ‰¾ patch notes
    for rel_path in POSSIBLE_PATHS:
        full_path = os.path.join(project_root, rel_path)
        if os.path.exists(full_path):
            target_path = full_path
            break

    ver = None
    if target_path:
        try:
            with open(target_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    # åŒ¹é… VERSION=V1.6.0
                    if line.startswith("VERSION="):
                        ver = line.split("=")[1].strip()
                        break
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç‰ˆæœ¬ä¿¡æ¯å‡ºé”™: {e}")

    # å¦‚æœæ²¡æ‰¾åˆ° VERSION= æ ‡è®°æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ—¥æœŸå…œåº•
    if not ver:
        ver = datetime.datetime.now().strftime("Date_%Y%m%d")
        print(f"âš ï¸ æœªæ£€æµ‹åˆ°ç‰ˆæœ¬å·ï¼Œä½¿ç”¨é»˜è®¤æ—¥æœŸåç¼€: {ver}")
    else:
        print(f" æ£€æµ‹åˆ°ç‰ˆæœ¬å·: {ver}")

    return ver, target_path


def pack_project():
    project_root = get_project_root()
    version, patch_note_path = get_version_info(project_root)

    # è¾“å‡ºç›®å½•åœ¨æ ¹ç›®å½•ä¸‹
    output_dir = os.path.join(project_root, OUTPUT_DIR_NAME)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    output_filename = f"project_all_code_{version}.txt"
    output_file_path = os.path.join(output_dir, output_filename)

    print(f"ğŸ“¦ æ‰“åŒ…èŒƒå›´: {project_root}")
    print(f"ğŸ“„ ç›®æ ‡æ–‡ä»¶: {output_file_path}")

    files_processed = 0

    with open(output_file_path, 'w', encoding='utf-8') as outfile:
        outfile.write(f"Project Root: {project_root}\n")
        outfile.write(f"Version: {version}\n")
        outfile.write(f"Generated at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        outfile.write("=" * 50 + "\n\n")

        # 1. ä¼˜å…ˆå†™å…¥ Patch Note (å¦‚æœå­˜åœ¨)
        if patch_note_path:
            try:
                rel_path = os.path.relpath(patch_note_path, project_root)
                with open(patch_note_path, 'r', encoding='utf-8') as pf:
                    content = pf.read()
                    outfile.write(f"\n{'=' * 20} START SPECIAL FILE: {rel_path} {'=' * 20}\n")
                    outfile.write(content)
                    outfile.write(f"\n{'=' * 20} END SPECIAL FILE: {rel_path} {'=' * 20}\n\n")
                print(f"å·²åŒ…å«: {rel_path}")
            except Exception as e:
                print(f"âš ï¸ è¯»å– Patch Note å¤±è´¥: {e}")

        # 2. éå†é¡¹ç›®æ–‡ä»¶
        for root, dirs, files in os.walk(project_root):
            # è¿‡æ»¤å¿½ç•¥çš„ç›®å½•
            dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]

            for file in files:
                if file == output_filename: continue  # è·³è¿‡è¾“å‡ºæ–‡ä»¶æœ¬èº«

                # è·³è¿‡ patch note (å› ä¸ºå‰é¢å·²ç»å†™è¿‡äº†)
                full_path = os.path.join(root, file)
                if patch_note_path and os.path.abspath(full_path) == os.path.abspath(patch_note_path):
                    continue

                if any(file.endswith(ext) for ext in EXTENSIONS):
                    rel_path = os.path.relpath(full_path, project_root)

                    try:
                        with open(full_path, 'r', encoding='utf-8') as infile:
                            content = infile.read()
                        outfile.write(f"\n{'=' * 20} START FILE: {rel_path} {'=' * 20}\n")
                        outfile.write(content)
                        outfile.write(f"\n{'=' * 20} END FILE: {rel_path} {'=' * 20}\n\n")
                        files_processed += 1
                        print(f"å·²æ‰“åŒ…: {rel_path}")
                    except Exception as e:
                        print(f"âš ï¸ è¯»å–å¤±è´¥: {rel_path} - {e}")

    print("-" * 50)
    print(f" æ‰“åŒ…å®Œæˆï¼å…± {files_processed} ä¸ªæ–‡ä»¶ã€‚")
    print(f"ğŸ“‚ æ–‡ä»¶ä½ç½®: {output_file_path}")


if __name__ == '__main__':
    pack_project()
==================== END FILE: tools/pack_project.py ====================


==================== START FILE: core/repository/sku_repo.py ====================
# core/repository/sku_repo.py
"""
æ–‡ä»¶è¯´æ˜: SKU ä¸åº“å­˜æ•°æ®ä»“åº“ (SKU Repository)
ä¸»è¦åŠŸèƒ½:
1. è·å–æœ€æ–°çš„åº“å­˜å¿«ç…§ã€‚
2. è·å–æ‰€æœ‰ SKU çš„æˆæœ¬ä¿¡æ¯ã€‚
3. æä¾›æœ‰æ•ˆçš„ SKU åˆ—è¡¨ã€‚
"""

import pandas as pd
from typing import List
from core.components.db.client import DBClient


class SkuRepository:

    def get_inventory_latest(self) -> pd.DataFrame:
        """
        è·å–æœ€æ–°çš„åº“å­˜æ•°æ® (è‡ªåŠ¨è¯†åˆ«æœ€åä¸€ä¸ªæ—¥æœŸåˆ—)
        Returns: DataFrame [SKU, Quantity]
        """
        # 1. è¯»è¡¨ç»“æ„
        try:
            schema_df = DBClient.read_df("SELECT * FROM Data_Inventory LIMIT 0")
        except:
            return pd.DataFrame(columns=["SKU", "Quantity"])

        # 2. æ‰¾æ—¥æœŸåˆ— (å«æœ‰ '-')
        date_cols = [c for c in schema_df.columns if '-' in str(c)]
        if not date_cols:
            return pd.DataFrame(columns=["SKU", "Quantity"])

        # 3. å–æœ€æ–°
        latest_col = sorted(date_cols)[-1]

        # 4. æŸ¥è¯¢
        sql = f"SELECT SKU, `{latest_col}` as Quantity FROM Data_Inventory"
        df = DBClient.read_df(sql)

        # å½’ä¸€åŒ– SKU
        if not df.empty:
            df['SKU'] = df['SKU'].astype(str).str.strip().str.upper()
            df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce').fillna(0)

        return df

    def get_all_cogs(self) -> pd.DataFrame:
        """è·å– SKU æ¡£æ¡ˆ (å«æˆæœ¬)"""
        df = DBClient.read_df("SELECT * FROM Data_COGS")
        if not df.empty and 'SKU' in df.columns:
            df['SKU'] = df['SKU'].astype(str).str.strip().str.upper()
        return df

    def get_valid_skus(self) -> List[str]:
        """è·å–æ‰€æœ‰æœ‰æ•ˆ SKU"""
        df = self.get_all_cogs()
        if df.empty: return []
        return df['SKU'].dropna().unique().tolist()
==================== END FILE: core/repository/sku_repo.py ====================


==================== START FILE: core/repository/transaction_repo.py ====================
# core/repository/transaction_repo.py
"""
æ–‡ä»¶è¯´æ˜: äº¤æ˜“æ•°æ®é€šç”¨ä»“åº“ (Transaction Repository)
ä¸»è¦åŠŸèƒ½:
1. æä¾›å¯¹ Data_Clean_Log çš„æ ‡å‡†åŒ–æŸ¥è¯¢ã€‚
2. å¤„ç†æ—¥æœŸèŒƒå›´ç­›é€‰å’Œç±»å‹å¼ºåˆ¶è½¬æ¢ã€‚
"""

import pandas as pd
from datetime import date
from core.components.db.client import DBClient


class TransactionRepository:
    # æ ¸å¿ƒæ•°å€¼åˆ— (å¼ºåˆ¶è½¬æ¢ä»¥é˜²è®¡ç®—é”™è¯¯)
    NUMERIC_COLS = [
        'quantity', 'revenue', 'profit', 'Shipping and handling',
        'Seller collected tax', 'eBay collected tax', 'Refund',
        'Shipping label-Earning data', 'Shipping label-underpay',
        'Shipping label-overpay', 'Shipping label-Return',
        'Final Value Fee - fixed', 'Final Value Fee - variable',
        'Promoted Listings fee'
    ]

    def get_transactions_by_date(self, start_date: date, end_date: date) -> pd.DataFrame:
        """
        æŸ¥è¯¢æ¸…æ´—åçš„äº¤æ˜“æ•°æ® (Clean Log)
        """
        sql = """
              SELECT * \
              FROM Data_Clean_Log
              WHERE `order date` >= :start
                AND `order date` <= :end \
              """
        params = {
            "start": start_date.strftime("%Y-%m-%d"),
            "end": end_date.strftime("%Y-%m-%d")
        }

        df = DBClient.read_df(sql, params)
        if df.empty: return df

        # ç±»å‹æ¸…æ´—
        # 1. æ•°å€¼åˆ—
        for c in self.NUMERIC_COLS:
            # æ¨¡ç³ŠåŒ¹é…åˆ—å (å¿½ç•¥å¤§å°å†™)
            match = next((col for col in df.columns if col.lower() == c.lower()), None)
            if match:
                df[match] = pd.to_numeric(df[match], errors='coerce').fillna(0.0)

        # 2. æ—¥æœŸåˆ—
        if "order date" in df.columns:
            df["order date"] = pd.to_datetime(df["order date"], errors='coerce')

        return df
==================== END FILE: core/repository/transaction_repo.py ====================


==================== START FILE: core/components/security.py ====================
# core/components/security.py
"""
æ–‡ä»¶è¯´æ˜: å®‰å…¨åŠ å¯†å·¥å…·ç®± (Security Toolkit)
ä¸»è¦åŠŸèƒ½:
1. æä¾›æ— çŠ¶æ€çš„å¯†ç å“ˆå¸ŒåŠ å¯† (PBKDF2-SHA256)ã€‚
2. æä¾›å¯†ç æ ¡éªŒé€»è¾‘ (é˜²æ­¢æ—¶åºæ”»å‡»)ã€‚
3. ç”Ÿæˆå®‰å…¨çš„éšæœºä¼šè¯ä»¤ç‰Œ (UUID)ã€‚
"""

import os
import base64
import hashlib
import hmac
import uuid


class SecurityUtils:
    """
    [åº•å±‚ç»„ä»¶] å®‰å…¨åŠ å¯†å·¥å…·ç®±
    """

    @staticmethod
    def hash_password(password: str) -> str:
        """
        ä½¿ç”¨ PBKDF2-SHA256 ç®—æ³•å¯¹å¯†ç è¿›è¡Œå“ˆå¸ŒåŠ å¯†ã€‚
        æ ¼å¼: pbkdf2_sha256$iterations$salt$hash
        """
        if not password:
            raise ValueError("å¯†ç ä¸èƒ½ä¸ºç©º")

        # ç”Ÿæˆéšæœºç› (16 bytes)
        salt = os.urandom(16)
        iterations = 120000

        # è®¡ç®—å“ˆå¸Œ
        dk = hashlib.pbkdf2_hmac("sha256", password.encode("utf-8"), salt, iterations)

        # Base64 ç¼–ç ä»¥ä¾¿å­˜å‚¨
        salt_b64 = base64.b64encode(salt).decode("ascii")
        hash_b64 = base64.b64encode(dk).decode("ascii")

        return f"pbkdf2_sha256${iterations}${salt_b64}${hash_b64}"

    @staticmethod
    def verify_password(password: str, stored_hash: str) -> bool:
        """
        æ ¡éªŒæ˜æ–‡å¯†ç ä¸å­˜å‚¨çš„å“ˆå¸Œå€¼æ˜¯å¦åŒ¹é…ã€‚
        """
        try:
            if not stored_hash or "$" not in stored_hash:
                return False

            alg, iter_str, salt_b64, hash_b64 = stored_hash.split("$")

            if alg != "pbkdf2_sha256":
                return False

            salt = base64.b64decode(salt_b64.encode("ascii"))
            stored_dk = base64.b64decode(hash_b64.encode("ascii"))

            dk = hashlib.pbkdf2_hmac("sha256", password.encode("utf-8"), salt, int(iter_str))

            # ä½¿ç”¨æ’å®šæ—¶é—´æ¯”è¾ƒ (compare_digest)ï¼Œé˜²æ­¢æ—¶åºæ”»å‡»
            return hmac.compare_digest(dk, stored_dk)
        except Exception:
            return False

    @staticmethod
    def generate_token() -> str:
        """ç”Ÿæˆå”¯ä¸€ä¼šè¯ä»¤ç‰Œ"""
        return str(uuid.uuid4())
==================== END FILE: core/components/security.py ====================


==================== START FILE: core/components/algo/models.py ====================
# core/components/algo/models.py
"""
æ–‡ä»¶è¯´æ˜: æ ¸å¿ƒç®—æ³•æ¨¡å‹åº“ (Algorithm Models)
ä¸»è¦åŠŸèƒ½:
1. å®ç°å¤šç§æ—¶é—´åºåˆ—é¢„æµ‹ç®—æ³• (XGBoost, SARIMA, ETS, Croston)ã€‚
2. å°è£…å¤æ‚çš„ç‰¹å¾å·¥ç¨‹ (Feature Engineering)ã€‚
3. æä¾›ä¼˜é›…é™çº§æœºåˆ¶ (Try-Import)ï¼Œç¡®ä¿ç¼ºåº“æ—¶ä¸å´©æºƒã€‚
"""

import pandas as pd
import numpy as np
import warnings
from .base import BaseForecaster

warnings.filterwarnings("ignore")


# =========================================================
# 1. æœºå™¨å­¦ä¹ æ¨¡å‹ (XGBoost)
# =========================================================
class XGBoostForecaster(BaseForecaster):
    """
    XGBoost å›å½’é¢„æµ‹
    ç‰¹æ€§: è‡ªåŠ¨æ„å»ºæ»åç‰¹å¾(Lags)å’Œæ»šåŠ¨ç‰¹å¾(Rolling Window)ã€‚
    """

    def __init__(self):
        super().__init__("XGBoost")
        try:
            from xgboost import XGBRegressor
            self.model_cls = XGBRegressor
            self.available = True
        except ImportError:
            self.available = False

    def fit_predict(self, series: pd.Series, periods: int = 1) -> float:
        if not self.available or len(series) < 12: return -1.0

        # ç‰¹å¾å·¥ç¨‹
        df = pd.DataFrame({'y': series.values})

        # Lags
        for i in [1, 2, 3, 6, 12]:
            if len(df) > i: df[f'lag_{i}'] = df['y'].shift(i)

        # Rolling
        df['roll_mean_3'] = df['y'].shift(1).rolling(window=3).mean()

        # Calendar (Simple index based)
        months = np.arange(len(df)) % 12 + 1
        df['month'] = months

        df = df.dropna()
        if df.empty: return -1.0

        X = df.drop(columns=['y'])
        y = df['y']

        try:
            model = self.model_cls(n_estimators=100, max_depth=3, n_jobs=1, verbosity=0)
            model.fit(X, y)

            # æ„é€ ä¸‹ä¸€æœŸç‰¹å¾ (ç®€åŒ–é€»è¾‘: å–æœ€åä¸€è¡Œä½œä¸ºåŸºç¡€)
            last_row = X.iloc[-1].copy()

            # æ›´æ–° Lag ç‰¹å¾
            last_val = series.iloc[-1]
            if 'lag_1' in last_row: last_row['lag_1'] = last_val

            # é¢„æµ‹
            pred = model.predict(pd.DataFrame([last_row]))[0]
            return max(0.0, float(pred))
        except:
            return -1.0


# =========================================================
# 2. ç»Ÿè®¡å­¦æ¨¡å‹ (Statsmodels)
# =========================================================
class StatsModelForecaster(BaseForecaster):
    def __init__(self, name):
        super().__init__(name)
        try:
            import statsmodels.api as sm
            self.available = True
        except ImportError:
            self.available = False


class SarimaForecaster(StatsModelForecaster):
    def __init__(self):
        super().__init__("SARIMA")

    def fit_predict(self, series: pd.Series, periods: int = 1) -> float:
        if not self.available or len(series) < 6: return -1.0
        try:
            from statsmodels.tsa.statespace.sarimax import SARIMAX
            # ç®€å•å‚æ•° (1,1,1)ï¼Œç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ auto_arima
            model = SARIMAX(series, order=(1, 1, 1), enforce_stationarity=False, enforce_invertibility=False)
            res = model.fit(disp=False)
            return max(0.0, float(res.forecast(periods).iloc[-1]))
        except:
            return -1.0


class ETSForecaster(StatsModelForecaster):
    def __init__(self):
        super().__init__("ETS")

    def fit_predict(self, series: pd.Series, periods: int = 1) -> float:
        if not self.available or len(series) < 6: return -1.0
        try:
            from statsmodels.tsa.exponential_smoothing.ets import ETSModel
            model = ETSModel(series, error="add", trend="add")
            res = model.fit(disp=False)
            return max(0.0, float(res.forecast(periods).iloc[-1]))
        except:
            return -1.0


class HoltWintersForecaster(StatsModelForecaster):
    def __init__(self):
        super().__init__("HoltWinters")

    def fit_predict(self, series: pd.Series, periods: int = 1) -> float:
        if not self.available or len(series) < 12: return -1.0
        try:
            from statsmodels.tsa.holtwinters import ExponentialSmoothing
            model = ExponentialSmoothing(series, trend="add", seasonal="add", seasonal_periods=12)
            res = model.fit()
            return max(0.0, float(res.forecast(periods).iloc[-1]))
        except:
            return -1.0


# =========================================================
# 3. å¯å‘å¼æ¨¡å‹ (Heuristic)
# =========================================================
class WeightedCycleForecaster(BaseForecaster):
    """åŠ æƒç§»åŠ¨å¹³å‡ (å…œåº•æ–¹æ¡ˆ)"""

    def __init__(self):
        super().__init__("WeightedCycle")

    def fit_predict(self, series: pd.Series, periods: int = 1) -> float:
        if len(series) == 0: return 0.0
        if len(series) < 3: return float(series.mean())
        # 3ä¸ªæœˆåŠ æƒ: 50%, 30%, 20%
        val = (series.iloc[-1] * 0.5 + series.iloc[-2] * 0.3 + series.iloc[-3] * 0.2)
        return max(0.0, float(val))


class CrostonForecaster(BaseForecaster):
    """é—´æ­‡æ€§éœ€æ±‚é¢„æµ‹ (é€‚åˆé•¿å°¾/æ–­ç»­å‡ºå•çš„ SKU)"""

    def __init__(self):
        super().__init__("Croston")

    def fit_predict(self, series: pd.Series, periods: int = 1) -> float:
        y = series.values
        if np.count_nonzero(y) < 2: return float(np.mean(y)) if len(y) > 0 else 0.0

        alpha = 0.3
        demand = y[np.argmax(y > 0)]
        interval = 1.0
        last_idx = np.argmax(y > 0)

        for i in range(last_idx + 1, len(y)):
            if y[i] > 0:
                current_int = i - last_idx
                demand = alpha * y[i] + (1 - alpha) * demand
                interval = alpha * current_int + (1 - alpha) * interval
                last_idx = i

        if interval == 0: return 0.0
        return max(0.0, float(demand / interval))
==================== END FILE: core/components/algo/models.py ====================


==================== START FILE: core/components/algo/__init__.py ====================

==================== END FILE: core/components/algo/__init__.py ====================


==================== START FILE: core/components/algo/base.py ====================
# core/components/algo/base.py
"""
æ–‡ä»¶è¯´æ˜: ç®—æ³•æ¨¡å‹åŸºç±» (Algorithm Base Class)
ä¸»è¦åŠŸèƒ½:
1. å®šä¹‰æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹çš„ç»Ÿä¸€æ¥å£ (BaseForecaster)ã€‚
2. æä¾›é€šç”¨çš„æ•°æ®é¢„å¤„ç†æ–¹æ³• (ç¼ºå¤±å€¼å¡«å……ã€ç±»å‹è½¬æ¢)ã€‚
"""

from abc import ABC, abstractmethod
import pandas as pd
import numpy as np

class BaseForecaster(ABC):
    """
    [ç®—æ³•åŸºç±»] æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹æ¥å£
    """
    def __init__(self, name: str):
        self.name = name

    @abstractmethod
    def fit_predict(self, series: pd.Series, periods: int = 1) -> float:
        """
        [æ ¸å¿ƒå¥‘çº¦] è®­ç»ƒå¹¶é¢„æµ‹æœªæ¥ N æœŸ
        Args:
            series: å†å²æ•°æ®åºåˆ— (pandas Series)
            periods: é¢„æµ‹æ­¥é•¿ (é»˜è®¤ä¸º 1)
        Returns:
            float: é¢„æµ‹å€¼ (å¿…é¡»éè´Ÿ)
        """
        pass

    def preprocess(self, series: pd.Series) -> pd.Series:
        """é€šç”¨é¢„å¤„ç†"""
        return pd.to_numeric(series, errors='coerce').fillna(0)
==================== END FILE: core/components/algo/base.py ====================


==================== START FILE: core/components/utils/diff_engine.py ====================
# core/components/utils/diff_engine.py
"""
æ–‡ä»¶è¯´æ˜: æ™ºèƒ½å·®å¼‚æ¯”å¯¹å¼•æ“ (Diff Engine)
ä¸»è¦åŠŸèƒ½:
1. å¯¹æ¯”ä¸¤ä¸ª DataFrame (Old vs New) çš„å·®å¼‚ã€‚
2. è¯†åˆ«æ–°å¢ (Added)ã€åˆ é™¤ (Removed) å’Œ ä¿®æ”¹ (Modified) çš„è¡Œã€‚
3. é’ˆå¯¹ä¿®æ”¹è¡Œï¼Œç²¾ç¡®æŒ‡å‡ºæ˜¯å“ªä¸ªå­—æ®µå‘ç”Ÿäº†å˜åŒ– (OldVal -> NewVal)ã€‚
4. [æ¶æ„è®¾è®¡] è¿™æ˜¯ä¸€ä¸ªçº¯é€»è¾‘å…¬æœ‰ç±»ï¼Œå¯å¤ç”¨äºä»»ä½•è¡¨çš„å·®å¼‚è®¡ç®—ã€‚
"""

import pandas as pd
from typing import List, Dict, Any, Tuple


class DiffEngine:

    @staticmethod
    def compute_diff(df_old: pd.DataFrame, df_new: pd.DataFrame, key_col: str) -> Dict[str, Any]:
        """
        [æ ¸å¿ƒç®—æ³•] è®¡ç®—å·®å¼‚
        :param df_old: åŸå§‹æ•°æ® (DB State)
        :param df_new: ç¼–è¾‘åæ•°æ® (UI State)
        :param key_col: ä¸»é”®åˆ—å (å¦‚ 'SKU')
        :return: {
            "modified": [{key: 'A', changes: {'Cost': (10, 12)}}],
            "added": [],
            "removed": []
        }
        """

        # é¢„å¤„ç†ï¼šç»Ÿä¸€ç´¢å¼•ï¼Œç¡®ä¿å­—ç¬¦ä¸²å»ç©ºå¤§å†™
        def clean_df(df):
            d = df.copy()
            # å¼ºåˆ¶ä¸»é”®è½¬å­—ç¬¦ä¸²å¹¶å»ç©º
            d[key_col] = d[key_col].astype(str).str.strip().str.upper()
            return d.set_index(key_col)

        old_map = clean_df(df_old)
        new_map = clean_df(df_new)

        diff_result = {
            "modified": [],
            "added": [],  # æœ¬æ¬¡éœ€æ±‚ä¸»è¦å…³æ³¨ modifiedï¼Œä½†ä¿ç•™å®Œæ•´æ€§
            "removed": []
        }

        # 1. æ‰¾å‡ºäº¤é›† (å¯èƒ½è¢«ä¿®æ”¹çš„)
        common_keys = old_map.index.intersection(new_map.index)

        for key in common_keys:
            row_old = old_map.loc[key]
            row_new = new_map.loc[key]

            changes = {}
            # é€åˆ—æ¯”å¯¹
            for col in new_map.columns:
                if col not in row_old: continue  # å¿½ç•¥æ–°åŠ çš„ä¸´æ—¶åˆ—

                v_old = row_old[col]
                v_new = row_new[col]

                # ç±»å‹å®½æ¾è½¬æ¢å¯¹æ¯” (é˜²æ­¢ int 10 != float 10.0)
                try:
                    # å¦‚æœéƒ½æ˜¯æ•°å­—
                    if pd.api.types.is_number(v_old) and pd.api.types.is_number(v_new):
                        is_diff = abs(float(v_old) - float(v_new)) > 0.0001
                    else:
                        # å­—ç¬¦ä¸²å¯¹æ¯”
                        is_diff = str(v_old).strip() != str(v_new).strip()
                except:
                    is_diff = str(v_old) != str(v_new)

                if is_diff:
                    changes[col] = (v_old, v_new)

            if changes:
                diff_result["modified"].append({
                    "key": key,
                    "changes": changes
                })

        return diff_result

    @staticmethod
    def format_log_message(diff_result: Dict[str, Any], key_name: str = "SKU") -> List[str]:
        """[å·¥å…·] å°†å·®å¼‚å¯¹è±¡è½¬ä¸ºäººç±»å¯è¯»çš„æ—¥å¿—åˆ—è¡¨"""
        logs = []
        for item in diff_result["modified"]:
            key = item["key"]
            change_strs = []
            for col, (old, new) in item["changes"].items():
                change_strs.append(f"{col}: {old} -> {new}")

            logs.append(f"æ›´æ–° {key_name} [{key}]: " + ", ".join(change_strs))
        return logs
==================== END FILE: core/components/utils/diff_engine.py ====================


==================== START FILE: core/components/utils/csv_parser.py ====================
# core/components/utils/csv_parser.py
"""
æ–‡ä»¶è¯´æ˜: å¤æ‚ CSV è§£æå·¥å…·
ä¸»è¦åŠŸèƒ½:
1. è§£æåŒ…å«å¤šä¸ªå­è¡¨çš„å †å å¼ CSV æ–‡ä»¶ (Multi-table CSV)ã€‚
2. è‡ªåŠ¨è¯†åˆ«åˆ†éš”ç¬¦ (ç©ºè¡Œã€æ ‡é¢˜è¡Œ)ã€‚
3. è¿‡æ»¤æ³¨é‡Šè¡Œå’Œæ— å…³å…ƒæ•°æ®ã€‚
"""

import pandas as pd
import io
import re
from pathlib import Path
from typing import List, Tuple


def parse_compound_csv(file_path: Path) -> List[Tuple[str, pd.DataFrame]]:
    """
    [æ™ºèƒ½è§£æå™¨] è§£æåŒ…å«å¤šä¸ªå­è¡¨çš„å¤æ‚ CSV æ–‡ä»¶

    Returns:
        List of (Title, DataFrame)
    """
    if not file_path.exists():
        return []

    tables = []
    current_lines = []
    current_title = "Main Table"

    # æ ‡é¢˜ç‰¹å¾æ­£åˆ™: === xxx ===, è¡¨1, Table 1
    title_pattern = re.compile(r'^(===.+===|è¡¨\d+|Table\s*\d+)', re.IGNORECASE)

    try:
        with open(file_path, "r", encoding="utf-8-sig") as f:
            all_lines = f.readlines()

        for line in all_lines:
            stripped = line.strip()

            # Case 1: ç©ºè¡Œ -> å¯èƒ½æ˜¯åˆ†éš”ç¬¦
            if not stripped:
                if current_lines:
                    _try_parse_buffer(current_lines, current_title, tables)
                    current_lines = []
                continue

            # Case 2: æ ‡é¢˜è¡Œ -> å¼ºåˆ¶åˆ†éš”
            # ç‰¹å¾ï¼šæ²¡æœ‰é€—å·ï¼Œä¸”ç¬¦åˆæ ‡é¢˜æ­£åˆ™
            if "," not in stripped and title_pattern.match(stripped):
                # å…ˆä¿å­˜ä¹‹å‰çš„
                if current_lines:
                    _try_parse_buffer(current_lines, current_title, tables)
                    current_lines = []

                # æ›´æ–°æ ‡é¢˜ (å»é™¤è£…é¥°ç¬¦)
                current_title = stripped.replace("===", "").strip()
                continue

            # Case 3: åº•éƒ¨è¯´æ˜æ–‡å­— (Footer)
            if (stripped.startswith("è¯´æ˜") or stripped.startswith("å¤‡æ³¨") or
                    "é€»è¾‘è¯´æ˜" in stripped or stripped.startswith("ğŸ“˜")):
                # é‡åˆ° Footerï¼Œç»“æŸæ”¶é›†
                if current_lines:
                    _try_parse_buffer(current_lines, current_title, tables)
                    current_lines = []
                continue

            # Case 4: æ•°æ®è¡Œ
            current_lines.append(line)

        # Loop ç»“æŸï¼Œå¤„ç†æœ€åä¸€å—
        if current_lines:
            _try_parse_buffer(current_lines, current_title, tables)

    except Exception as e:
        print(f"è§£æ CSV å¤±è´¥: {e}")
        # å…œåº•ï¼šå°è¯•å…¨é‡è¯»å–
        try:
            df = pd.read_csv(file_path)
            return [("Raw Data", df)]
        except:
            return []

    return tables


def _try_parse_buffer(lines, title, tables_list):
    """è¾…åŠ©ï¼šå°è¯•å°†æ–‡æœ¬è¡Œè½¬ä¸º DF"""
    try:
        # è¿‡æ»¤çº¯æ³¨é‡Šè¡Œ (#)
        valid_lines = [l for l in lines if not l.startswith("#")]
        if not valid_lines: return

        csv_io = io.StringIO("".join(valid_lines))
        # å®½å®¹æ¨¡å¼è¯»å–
        df = pd.read_csv(csv_io, on_bad_lines='skip')

        # æœ‰æ•ˆæ€§æ£€æŸ¥ï¼šä¸ä»…è¦æœ‰åˆ—ï¼Œè¿˜å¾—æœ‰æ•°æ®
        if not df.empty and len(df.columns) > 1:
            tables_list.append((title, df))
    except:
        pass
==================== END FILE: core/components/utils/csv_parser.py ====================


==================== START FILE: core/components/utils/__init__.py ====================

==================== END FILE: core/components/utils/__init__.py ====================


==================== START FILE: core/components/db/client.py ====================
# core/components/db/client.py
"""
æ–‡ä»¶è¯´æ˜: æ•°æ®åº“å®¢æˆ·ç«¯ (Database Client) - Syntax Fix Edition
ä¸»è¦åŠŸèƒ½:
1. æä¾› SQLAlchemy å¼•æ“å•ä¾‹ã€‚
2. [Fix] read_df: å¼ºåˆ¶ä½¿ç”¨ sqlalchemy.text() åŒ…è£… SQLï¼Œè§£å†³ ':u' è¯­æ³•é”™è¯¯ã€‚
3. [Fix] execute_stmt: ä¼˜åŒ–å‚æ•°ç»‘å®šï¼Œç¡®ä¿å†™å…¥æ“ä½œç¨³å®šã€‚
4. æ·±åº¦å®¡è®¡: è®°å½•è¯¦ç»† SQL å’Œå‚æ•°ã€‚
"""

import pandas as pd
import re
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine
from typing import Optional, Dict, Any, Union

from config.settings import settings
from core.sys.logger import get_audit_logger

# ä¸“ç”¨å®¡è®¡æ—¥å¿—è®°å½•å™¨
audit_logger = get_audit_logger()


class DBClient:
    _engine: Optional[Engine] = None

    @classmethod
    def get_engine(cls) -> Engine:
        if cls._engine is None:
            cls._engine = create_engine(
                settings.SQLALCHEMY_URL,
                pool_recycle=3600,
                pool_pre_ping=True
            )
        return cls._engine

    @classmethod
    def read_df(cls, sql: str, params: dict = None) -> pd.DataFrame:
        """
        æ‰§è¡ŒæŸ¥è¯¢ (SELECT)
        [Fix]: ä½¿ç”¨ text() åŒ…è£… SQLï¼Œç¡®ä¿ ':param' è¯­æ³•èƒ½è¢« SQLAlchemy æ­£ç¡®ç¼–è¯‘ã€‚
        """
        try:
            # è·å–è¿æ¥å¯¹è±¡ (Connection) è€Œä¸æ˜¯ç›´æ¥ç”¨ Engine
            with cls.get_engine().connect() as conn:
                # æ˜¾å¼è½¬æ¢ SQL å­—ç¬¦ä¸²ä¸º TextClause
                stmt = text(sql)
                # Pandas ä¼šå°† params ä¼ é€’ç»™ SQLAlchemy çš„ execution_options æˆ–ç›´æ¥ç»‘å®š
                return pd.read_sql(stmt, conn, params=params)

        except Exception as e:
            # æ‰“å°é”™è¯¯åˆ°æ§åˆ¶å°ä»¥ä¾¿è°ƒè¯• (å¼€å‘é˜¶æ®µ)
            print(f"ğŸ”¥ DATABASE QUERY ERROR: {e}")
            audit_logger.error(f"Query Failed: {e}", extra=cls._parse_sql_meta(sql))
            return pd.DataFrame()

    @classmethod
    def execute_stmt(cls, sql: str, params: dict = None) -> bool:
        """
        æ‰§è¡Œå˜æ›´ (INSERT / UPDATE / DELETE / DDL)
        """
        meta = cls._parse_sql_meta(sql)

        try:
            with cls.get_engine().begin() as conn:
                # æ˜¾å¼è½¬æ¢
                stmt = text(sql) if isinstance(sql, str) else sql

                # æ‰§è¡Œ
                result = conn.execute(stmt, params if params else {})

                rows_affected = result.rowcount
                meta["rows"] = rows_affected

                # å®¡è®¡è®°å½•
                safe_params = cls._sanitize_params(params)
                audit_msg = f"[SUCCESS] SQL: {cls._normalize_sql(sql)} ;; Params: {safe_params}"
                audit_logger.info(audit_msg, extra=meta)
                return True

        except Exception as e:
            print(f"ğŸ”¥ DATABASE EXEC ERROR: {e}")
            audit_logger.critical(f"[FAILED] Error: {str(e)} ;; SQL: {cls._normalize_sql(sql)}", extra=meta)
            return False

    @classmethod
    def atomic_transaction(cls):
        return cls.get_engine().begin()

    @classmethod
    def truncate_table(cls, table_name: str):
        sql = f"TRUNCATE TABLE `{table_name}`"
        meta = {"action": "TRUNCATE", "table": table_name, "rows": -1}
        try:
            with cls.get_engine().begin() as conn:
                conn.execute(text(sql))
            audit_logger.warning(f"[WARNING] Table Truncated: {table_name}", extra=meta)
            return True
        except Exception as e:
            audit_logger.critical(f"[FAILED] Truncate Failed: {e}", extra=meta)
            return False

    # =========================================================================
    # ğŸ•µï¸â€â™€ï¸ å®¡è®¡è¾…åŠ©å·¥å…·
    # =========================================================================

    @staticmethod
    def _parse_sql_meta(sql: Union[str, Any]) -> Dict[str, Any]:
        raw_sql = str(sql).strip().upper()
        norm_sql = re.sub(r'\s+', ' ', raw_sql)

        meta = {"action": "SQL", "table": "-", "rows": 0}

        first_word = norm_sql.split(' ')[0]
        if first_word in ["INSERT", "UPDATE", "DELETE", "SELECT", "CREATE", "DROP", "TRUNCATE", "ALTER"]:
            meta["action"] = first_word

        try:
            target = "-"
            if meta["action"] == "UPDATE":
                m = re.search(r"UPDATE\s+[`']?([a-zA-Z0-9_]+)[`']?", norm_sql)
                if m: target = m.group(1)
            elif meta["action"] == "INSERT":
                m = re.search(r"INTO\s+[`']?([a-zA-Z0-9_]+)[`']?", norm_sql)
                if m: target = m.group(1)
            elif meta["action"] == "DELETE":
                m = re.search(r"FROM\s+[`']?([a-zA-Z0-9_]+)[`']?", norm_sql)
                if m: target = m.group(1)
            elif meta["action"] == "CREATE":
                m = re.search(r"TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?[`']?([a-zA-Z0-9_]+)[`']?", norm_sql)
                if m: target = m.group(1)

            if target in ["INTO", "FROM", "TABLE", "IF", "NOT", "EXISTS", "SET"]:
                target = "-"
            meta["table"] = target
        except:
            pass
        return meta

    @staticmethod
    def _sanitize_params(params: Optional[Dict]) -> str:
        if not params: return "{}"
        SENSITIVE_KEYS = {'password', 'passwd', 'pwd', 'secret', 'token', 'key', 'credential', 'auth', 'ph'}
        safe_copy = {}
        for k, v in params.items():
            key_lower = k.lower()
            if any(s in key_lower for s in SENSITIVE_KEYS):
                safe_copy[k] = "****** (MASKED)"
            else:
                str_v = str(v)
                if len(str_v) > 200:
                    safe_copy[k] = str_v[:200] + "..."
                else:
                    safe_copy[k] = v
        return str(safe_copy)

    @staticmethod
    def _normalize_sql(sql: Union[str, Any]) -> str:
        s = str(sql).replace('\n', ' ').replace('\r', ' ')
        return re.sub(r'\s+', ' ', s).strip()
==================== END FILE: core/components/db/client.py ====================


==================== START FILE: core/components/db/__init__.py ====================
# core/components/db/__init__.py
"""
æ–‡ä»¶è¯´æ˜: æ•°æ®åº“ç»„ä»¶åŒ…åˆå§‹åŒ–æ–‡ä»¶
ä¸»è¦åŠŸèƒ½:
1. æ ‡è¯† db æ–‡ä»¶å¤¹ä¸ºä¸€ä¸ª Python åŒ…ã€‚
2. (å¯é€‰)åœ¨æ­¤å¤„æš´éœ² DBClient ä»¥ç®€åŒ–å¯¼å…¥è·¯å¾„ (from core.components.db import DBClient)ã€‚
"""
# æš‚æ—¶ä¿æŒä¸ºç©ºï¼Œéµå¾ªæ˜¾å¼å¯¼å…¥åŸåˆ™
==================== END FILE: core/components/db/__init__.py ====================


==================== START FILE: core/sys/lock_manager.py ====================
# core/sys/lock_manager.py
"""
æ–‡ä»¶è¯´æ˜: åˆ†å¸ƒå¼å¹¶å‘é”ç®¡ç†å™¨ (Concurrency Lock Manager)
ä¸»è¦åŠŸèƒ½:
1. æä¾›åŸºäºæ•°æ®åº“çš„èµ„æºé”å®šæœºåˆ¶ (System_Locks)ã€‚
2. é˜²æ­¢å¤šç”¨æˆ·åŒæ—¶æ“ä½œå…³é”®æ•°æ®è¡¨ (Race Condition Prevention)ã€‚
3. æ”¯æŒé”çš„è‡ªåŠ¨è¿‡æœŸä¸å¼ºåˆ¶é‡Šæ”¾ï¼Œä»¥åŠç”¨æˆ·ç™»å‡ºæ—¶çš„æ‰¹é‡è§£é”ã€‚
"""

import pandas as pd
from typing import Optional, Tuple, List
from sqlalchemy import text

from core.components.db.client import DBClient
from core.sys.logger import get_logger


class LockManager:
    """
    [æ ¸å¿ƒç»„ä»¶] å…¨å±€é”ç®¡ç†å™¨
    å»ºè®®çš„èµ„æº Key (Resource Key) æ ‡å‡†:
    - 'Data_Transaction': äº¤æ˜“æ•°æ®è¡¨ (ETLä¸Šä¼ /æ¸…æ´—)
    - 'Data_Inventory': åº“å­˜æ•°æ®è¡¨ (ETLåŒæ­¥/æ‰‹åŠ¨ä¿®æ”¹)
    - 'Data_COGS': æ¡£æ¡ˆæ•°æ®è¡¨ (æ‰¹é‡ä¿®æ”¹/æ–°å¢SKU)
    """

    TABLE_NAME = "System_Locks"
    TIMEOUT_MINUTES = 30  # é”é»˜è®¤è¿‡æœŸæ—¶é—´ï¼Œé˜²æ­¢æ­»é”

    logger = get_logger("LockManager")

    @classmethod
    def initialize(cls):
        """[åˆå§‹åŒ–] åˆ›å»ºé”è¡¨ç»“æ„"""
        sql = f"""
        CREATE TABLE IF NOT EXISTS `{cls.TABLE_NAME}` (
            `resource_key` VARCHAR(50) NOT NULL,
            `locked_by` VARCHAR(64) NOT NULL,
            `locked_at` DATETIME DEFAULT CURRENT_TIMESTAMP,
            `module_name` VARCHAR(50),
            PRIMARY KEY (`resource_key`)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
        """
        DBClient.execute_stmt(sql)

    @classmethod
    def acquire_lock(cls, resource_key: str, user: str, module: str) -> Tuple[bool, str]:
        """
        [åŸå­æ“ä½œ] å°è¯•è·å–é”
        :param resource_key: èµ„æºæ ‡è¯† (å¦‚ Data_Transaction)
        :param user: å½“å‰ç”¨æˆ·å
        :param module: æ“ä½œæ¨¡å—æè¿° (ç”¨äºæç¤ºå…¶ä»–äºº)
        :return: (æ˜¯å¦æˆåŠŸ, æç¤ºä¿¡æ¯)
        """
        if not user: return False, "åŒ¿åç”¨æˆ·æ— æ³•è·å–é”"

        # 1. æ£€æŸ¥å½“å‰é”çŠ¶æ€
        current = cls.get_lock_info(resource_key)

        if current:
            owner = current['locked_by']
            time_str = str(current['locked_at'])

            # å¦‚æœæ˜¯è‡ªå·±æŒæœ‰çš„é”ï¼Œåˆ·æ–°æ—¶é—´å¹¶è¿”å›æˆåŠŸ (å¯é‡å…¥)
            if owner == user:
                cls._refresh_lock(resource_key)
                return True, "Lock Refreshed"

            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ (è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä¾èµ–DBæ—¶é—´æˆ³ï¼Œæš‚ä¸è‡ªåŠ¨è¸¢äººï¼Œç”±äººå·¥åˆ¤æ–­)
            return False, f"èµ„æº [{resource_key}] æ­£è¢«ç”¨æˆ· [{owner}] å ç”¨ (å¼€å§‹æ—¶é—´: {time_str})ã€‚è¯·ç­‰å¾…å…¶æ“ä½œå®Œæˆã€‚"

        # 2. å°è¯•å†™å…¥é” (Insert)
        try:
            sql = f"""
            INSERT INTO `{cls.TABLE_NAME}` (resource_key, locked_by, module_name, locked_at)
            VALUES (:k, :u, :m, NOW())
            """
            DBClient.execute_stmt(sql, {"k": resource_key, "u": user, "m": module})
            cls.logger.info(f"ğŸ”’ é”å®šèµ„æº: {resource_key} | User: {user}")
            return True, "Lock Acquired"
        except Exception as e:
            cls.logger.warning(f"åŠ é”ç«äº‰å¤±è´¥: {e}")
            return False, "èµ„æºæ­£å¿™ï¼Œè¯·ç¨åé‡è¯•ã€‚"

    @classmethod
    def release_lock(cls, resource_key: str, user: str) -> bool:
        """[åŸå­æ“ä½œ] é‡Šæ”¾é”"""
        try:
            # åªèƒ½é‡Šæ”¾è‡ªå·±çš„é”
            sql = f"DELETE FROM `{cls.TABLE_NAME}` WHERE resource_key=:k AND locked_by=:u"
            DBClient.execute_stmt(sql, {"k": resource_key, "u": user})
            cls.logger.info(f"ğŸ”“ é‡Šæ”¾èµ„æº: {resource_key} | User: {user}")
            return True
        except Exception as e:
            cls.logger.error(f"é‡Šæ”¾é”å¤±è´¥: {e}")
            return False

    @classmethod
    def release_all_user_locks(cls, user: str) -> List[str]:
        """
        [æ¸…ç†æ“ä½œ] é‡Šæ”¾æŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰é” (ç”¨äºç™»å‡º/æ–­çº¿/å¼‚å¸¸é€€å‡º)
        :return: è¢«é‡Šæ”¾çš„èµ„æºåˆ—è¡¨ (ä¾‹å¦‚ ['Data_Transaction'])ï¼Œä¾›ä¸Šå±‚åˆ¤æ–­æ˜¯å¦éœ€è¦å›æ»š
        """
        try:
            # 1. æŸ¥è¯¢è¯¥ç”¨æˆ·æŒæœ‰å“ªäº›é”
            df = DBClient.read_df(f"SELECT resource_key FROM `{cls.TABLE_NAME}` WHERE locked_by=:u", {"u": user})
            resources = df['resource_key'].tolist() if not df.empty else []

            if resources:
                # 2. åˆ é™¤
                sql = f"DELETE FROM `{cls.TABLE_NAME}` WHERE locked_by=:u"
                DBClient.execute_stmt(sql, {"u": user})
                cls.logger.warning(f"ğŸ§¹ å¼ºåˆ¶é‡Šæ”¾ç”¨æˆ· [{user}] çš„æ‰€æœ‰é”: {resources}")

            return resources
        except Exception as e:
            cls.logger.error(f"æ‰¹é‡é‡Šæ”¾å¤±è´¥: {e}")
            return []

    @classmethod
    def get_lock_info(cls, resource_key: str) -> Optional[dict]:
        """æŸ¥è¯¢é”ä¿¡æ¯"""
        sql = f"SELECT * FROM `{cls.TABLE_NAME}` WHERE resource_key=:k"
        df = DBClient.read_df(sql, {"k": resource_key})
        if not df.empty:
            return df.iloc[0].to_dict()
        return None

    @classmethod
    def check_access(cls, resource_key: str, user: str) -> Tuple[bool, str]:
        """
        [æƒé™æ£€æŸ¥] åˆ¤æ–­å½“å‰ç”¨æˆ·æ˜¯å¦æœ‰æƒæ“ä½œèµ„æº
        é€»è¾‘: èµ„æºæœªé”å®š OR èµ„æºè¢«è‡ªå·±é”å®š -> True
        """
        info = cls.get_lock_info(resource_key)
        if not info:
            return True, "Available"

        if info['locked_by'] == user:
            return True, "Owned by you"

        return False, f"ğŸš« ç³»ç»Ÿé”å®š: ç”¨æˆ· [{info['locked_by']}] æ­£åœ¨ä½¿ç”¨è¯¥åŠŸèƒ½ [{info['module_name']}]ï¼Œè¯·ç¨å€™ã€‚"

    @classmethod
    def _refresh_lock(cls, resource_key: str):
        """åˆ·æ–°é”çš„å¿ƒè·³æ—¶é—´"""
        DBClient.execute_stmt(f"UPDATE `{cls.TABLE_NAME}` SET locked_at=NOW() WHERE resource_key=:k",
                              {"k": resource_key})
==================== END FILE: core/sys/lock_manager.py ====================


==================== START FILE: core/sys/logger.py ====================
# core/sys/logger.py
"""
æ–‡ä»¶è¯´æ˜: ç»Ÿä¸€æ—¥å¿—ç®¡ç†ç³»ç»Ÿ (Unified Logging System) - V5.0 Fix Recursion & Duplication
ä¸»è¦åŠŸèƒ½:
1. [Fix] å¼ºåˆ¶æ¸…ç†æ‰€æœ‰ Logger çš„ Handlersï¼Œå½»åº•è§£å†³ Streamlit Rerun å¯¼è‡´çš„æ—¥å¿—é‡å¤åˆ·å±é—®é¢˜ã€‚
2. [Fix] ä¿®å¤ RelativePathFilter çš„æ— é™é€’å½’ ([LIB] [LIB]...) é—®é¢˜ã€‚
3. æä¾› ContextFilter æ³¨å…¥ä¸Šä¸‹æ–‡ã€‚
"""

import logging
import logging.handlers
import sys
import os
from pathlib import Path
from typing import Optional

from config.settings import settings
from core.sys.context import get_context


# ==============================================================================
# 1. è¿‡æ»¤å™¨å®šä¹‰
# ==============================================================================
class ContextFilter(logging.Filter):
    """è‡ªåŠ¨æ³¨å…¥ Context ä¸­çš„ User, IP, Func ä¿¡æ¯"""

    def filter(self, record: logging.LogRecord) -> bool:
        ctx = get_context()

        if not hasattr(record, "user"): record.user = ctx.username or "System"
        if not hasattr(record, "ip"): record.ip = ctx.ip or "-"
        if not hasattr(record, "func"): record.func = ctx.function or "System"

        # å­—æ®µå…œåº•
        for attr in ["action", "table", "rows", "error_path", "error_func"]:
            if not hasattr(record, attr): setattr(record, attr, "-")

        if not hasattr(record, "module_name"): record.module_name = record.name
        return True


class AuditNoiseFilter(logging.Filter):
    def filter(self, record: logging.LogRecord) -> bool:
        user = getattr(record, "user", "System")
        if user == "System" and record.levelno < logging.CRITICAL:
            return False
        return True


class RelativePathFilter(logging.Filter):
    """
    [å®‰å…¨] å°†ç»å¯¹è·¯å¾„è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
    [Fix] å¢åŠ å¹‚ç­‰æ€§æ£€æŸ¥ï¼Œé˜²æ­¢ [LIB] å‰ç¼€æ— é™å åŠ 
    """

    def filter(self, record: logging.LogRecord) -> bool:
        if hasattr(record, "error_path") and record.error_path != "-":
            raw_path = str(record.error_path)

            # [Fix] å¦‚æœå·²ç»å¤„ç†è¿‡ï¼ˆåŒ…å« [LIB]ï¼‰ï¼Œç›´æ¥è¿”å›ï¼Œä¸å†å¤„ç†
            if "[LIB]" in raw_path:
                return True

            # [Fix] å¦‚æœå·²ç»æ˜¯ç›¸å¯¹è·¯å¾„ï¼ˆä¸åŒ…å« Base Dir ä¸”ä¸æ˜¯ç»å¯¹è·¯å¾„ï¼‰ï¼Œä¹Ÿä¸å¤„ç†
            if str(settings.BASE_DIR) not in raw_path and not os.path.isabs(raw_path):
                return True

            try:
                abs_path = Path(raw_path)
                try:
                    # å°è¯•è½¬ä¸ºç›¸å¯¹è·¯å¾„
                    rel = abs_path.relative_to(settings.BASE_DIR)
                    record.error_path = str(rel)
                except ValueError:
                    # ä¸åœ¨é¡¹ç›®è·¯å¾„ä¸‹ï¼Œæ ‡è®°ä¸º LIB
                    record.error_path = f"[LIB] {abs_path.name}"
            except:
                pass
        return True


# ==============================================================================
# 2. åˆå§‹åŒ–é€»è¾‘
# ==============================================================================
_LOG_INITIALIZED = False


def _clear_handlers(logger_name: str = None):
    """[Helper] å¼ºåˆ¶æ¸…ç©ºæŒ‡å®š Logger çš„æ‰€æœ‰ Handlers"""
    l = logging.getLogger(logger_name)
    if l.hasHandlers():
        l.handlers.clear()


def init_logging() -> None:
    # ç§»é™¤ _LOG_INITIALIZED æ£€æŸ¥ï¼Œæˆ–è€…ç¡®ä¿å®ƒçœŸçš„æœ‰æ•ˆ
    # åœ¨ Streamlit ä¸­ï¼Œå»ºè®®æ¯æ¬¡è¿è¡Œéƒ½æ£€æŸ¥å¹¶æ¸…ç† Handlers

    log_dir = settings.LOG_DIR
    log_dir.mkdir(parents=True, exist_ok=True)
    date_fmt = "%Y-%m-%d %H:%M:%S"

    ctx_filter = ContextFilter()
    rel_path_filter = RelativePathFilter()
    noise_filter = AuditNoiseFilter()

    # --- 0. æ¸…ç†æ—§ Handlers (å…³é”®ä¿®å¤: é˜²æ­¢åˆ·å±) ---
    _clear_handlers()  # Root
    _clear_handlers("db.audit")
    _clear_handlers("sys.error")

    # --- 1. ä¸šåŠ¡æ—¥å¿— (app.log) ---
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)

    app_fmt = (
        "%(asctime)s | %(levelname)-8s | %(module_name)s | "
        "user=%(user)s | ip=%(ip)s | func=%(func)s | "
        "%(message)s"
    )
    app_formatter = logging.Formatter(fmt=app_fmt, datefmt=date_fmt)

    # Console
    console = logging.StreamHandler(sys.stdout)
    console.setFormatter(app_formatter)
    console.addFilter(ctx_filter)
    root_logger.addHandler(console)

    # File
    app_h = logging.handlers.RotatingFileHandler(
        filename=str(log_dir / "app.log"), maxBytes=10 * 1024 * 1024, backupCount=5, encoding="utf-8"
    )
    app_h.setFormatter(app_formatter)
    app_h.addFilter(ctx_filter)
    root_logger.addHandler(app_h)

    # --- 2. æ•°æ®åº“å®¡è®¡ (audit.log) ---
    audit_logger = logging.getLogger("db.audit")
    audit_logger.propagate = False
    audit_logger.setLevel(logging.INFO)

    audit_fmt = (
        "%(asctime)s | %(levelname)-8s | %(module_name)s | "
        "user=%(user)s | ip=%(ip)s | func=%(func)s | "
        "action=%(action)s | table=%(table)s | rows=%(rows)s | "
        "%(message)s"
    )
    audit_h = logging.handlers.RotatingFileHandler(
        filename=str(log_dir / "audit.log"), maxBytes=20 * 1024 * 1024, backupCount=10, encoding="utf-8"
    )
    audit_h.setFormatter(logging.Formatter(fmt=audit_fmt, datefmt=date_fmt))
    audit_h.addFilter(ctx_filter)
    audit_h.addFilter(noise_filter)
    audit_logger.addHandler(audit_h)

    # --- 3. ç³»ç»Ÿé”™è¯¯æ—¥å¿— (error.log) ---
    error_logger = logging.getLogger("sys.error")
    error_logger.propagate = False
    error_logger.setLevel(logging.ERROR)

    # [Fix] æè‡´ç²¾ç®€çš„ä¸€è¡Œæ ¼å¼ (ç”¨æˆ·æ“ä½œ + é”™è¯¯æ‘˜è¦ + ä½ç½®)
    # è¯¦ç»†å †æ ˆé€šè¿‡ ErrorHandler æ”¾å…¥ message ä¸­ï¼Œæˆ–è€…è¿™é‡Œåªè®°å½•æ‘˜è¦
    # æ—¢ç„¶ä½ è¦æ±‚â€œä¸€è¡Œè®°å½•â€ï¼Œæˆ‘ä»¬å°† Traceback å‹ç¼©æˆ–ä»…è®°å½• Headerï¼Œè¯¦æƒ…ç”±å¼€å‘äººå‘˜æŸ¥é˜…
    # ä½†ä¸ºäº†è°ƒè¯•ï¼Œé€šå¸¸è¿˜æ˜¯éœ€è¦ Tracebackã€‚
    # è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ï¼šHeader Line + Traceback Block
    error_fmt = (
        "%(asctime)s | %(levelname)-8s | User: %(user)s | Func: %(func)s | Loc: %(error_path)s:%(error_func)s\n"
        "   >>> %(message)s"
    )
    error_h = logging.handlers.RotatingFileHandler(
        filename=str(log_dir / "error.log"), maxBytes=10 * 1024 * 1024, backupCount=10, encoding="utf-8"
    )
    error_h.setFormatter(logging.Formatter(fmt=error_fmt, datefmt=date_fmt))
    error_h.addFilter(ctx_filter)
    error_h.addFilter(rel_path_filter)
    error_logger.addHandler(error_h)

    global _LOG_INITIALIZED
    _LOG_INITIALIZED = True


# ==============================================================================
# 3. è·å–æ¥å£
# ==============================================================================
class UserLoggerAdapter(logging.LoggerAdapter):
    def process(self, msg, kwargs):
        return msg, kwargs


def get_logger(name: str = None) -> logging.Logger:
    init_logging()
    return UserLoggerAdapter(logging.getLogger(name if name else "app"), {})


def get_audit_logger() -> logging.Logger:
    init_logging()
    return UserLoggerAdapter(logging.getLogger("db.audit"), {})


def get_error_logger() -> logging.Logger:
    init_logging()
    return UserLoggerAdapter(logging.getLogger("sys.error"), {})
==================== END FILE: core/sys/logger.py ====================


==================== START FILE: core/sys/context.py ====================
# core/sys/context.py
"""
æ–‡ä»¶è¯´æ˜: å…¨å±€è¯·æ±‚/ç”¨æˆ·ä¸Šä¸‹æ–‡ç®¡ç† (Request / User Context)
ä¸»è¦åŠŸèƒ½:
1. ä½¿ç”¨ contextvars å®ç°çº¿ç¨‹/åç¨‹å®‰å…¨çš„çŠ¶æ€éš”ç¦»ã€‚
2. å­˜å‚¨å½“å‰è¯·æ±‚çš„ User, IP, SessionIDã€‚
3. **[New] å­˜å‚¨ Function Name (å½“å‰åŠŸèƒ½æ¨¡å—)**ï¼Œå®ç°æ“ä½œæº¯æºã€‚
"""

from dataclasses import dataclass, field
from typing import Optional, Dict
import contextvars


@dataclass
class RequestContext:
    """
    [ä¸Šä¸‹æ–‡æ¨¡å‹]
    å­˜å‚¨å½“å‰è¯·æ±‚çš„å…ƒæ•°æ®ï¼Œç”¨äºå…¨é“¾è·¯è¿½è¸ªã€‚
    """
    username: Optional[str] = None
    ip: Optional[str] = None
    session_id: Optional[str] = None
    function: Optional[str] = "System"  # [New] é»˜è®¤ä¸º Systemï¼ŒUIå±‚ä¼šè¦†ç›–æ­¤å€¼
    extra: Dict[str, str] = field(default_factory=dict)


# ä½¿ç”¨ contextvars å®ç°çº¿ç¨‹/åç¨‹å®‰å…¨çš„çŠ¶æ€éš”ç¦»
_current_ctx: contextvars.ContextVar[RequestContext] = contextvars.ContextVar(
    "current_request_context",
    default=RequestContext()
)


def set_context(username: Optional[str], ip: Optional[str] = None,
                session_id: Optional[str] = None,
                function: Optional[str] = None,  # [New]
                extra: Optional[Dict[str, str]] = None) -> None:
    """
    [å…¥å£] è®¾ç½®å½“å‰ä¸Šä¸‹æ–‡
    æ”¯æŒå¢é‡æ›´æ–°ï¼šå¦‚æœå‚æ•°ä¸º Noneï¼Œåˆ™ä¿ç•™æ—§å€¼ã€‚
    """
    old = _current_ctx.get()

    ctx = RequestContext(
        username=username if username is not None else old.username,
        ip=ip if ip is not None else old.ip,
        session_id=session_id if session_id is not None else old.session_id,
        function=function if function is not None else old.function,  # [New]
        extra=extra or old.extra
    )
    _current_ctx.set(ctx)


def set_current_function(name: str):
    """
    [å¿«æ·æ–¹å¼] è®¾ç½®å½“å‰åŠŸèƒ½æ¨¡å—åç§°
    UI é¡µé¢åœ¨ render() å¼€å¤´è°ƒç”¨æ­¤æ–¹æ³•ï¼Œæ ‡è®°å½“å‰æ“ä½œæ‰€å±çš„åŠŸèƒ½åŒºã€‚
    """
    set_context(None, function=name)


def get_context() -> RequestContext:
    """[è·å–] è·å–å½“å‰å®Œæ•´çš„ä¸Šä¸‹æ–‡å¯¹è±¡"""
    return _current_ctx.get()


def get_current_user() -> str:
    """[å¿«æ·æ–¹å¼] è·å–å½“å‰ç”¨æˆ·å"""
    user = _current_ctx.get().username
    return user if user else "System"


def clear_context() -> None:
    """[æ¸…ç†] é‡ç½®ä¸Šä¸‹æ–‡"""
    _current_ctx.set(RequestContext())
==================== END FILE: core/sys/context.py ====================


==================== START FILE: core/services/crm.py ====================
# core/services/crm.py
"""
æ–‡ä»¶è¯´æ˜: å®¢æˆ·å…³ç³»ç®¡ç†ä¸šåŠ¡æœåŠ¡ (CRM Service)
ä¸»è¦åŠŸèƒ½:
1. åŠ è½½è¿‡å» 365 å¤©çš„å†å²äº¤æ˜“æ•°æ®ã€‚
2. è®¡ç®— RFM æ¨¡å‹ (Recency, Frequency, Monetary)ã€‚
3. è°ƒç”¨ CustomerDiagnostician è¿›è¡Œåˆ†å±‚è¯Šæ–­ã€‚
4. **å…³é”®ä¿®å¤**: å¼ºåˆ¶æ•°å€¼ç±»å‹è½¬æ¢ï¼Œé˜²æ­¢ 'float'+'str' æŠ¥é”™ã€‚
"""

import pandas as pd
import numpy as np
from datetime import timedelta
from core.services.finance.base import ProfitAnalyzerBase
from core.repository.transaction_repo import TransactionRepository
from core.services.diagnostics.crm import CustomerDiagnostician


class CustomerAnalyzer(ProfitAnalyzerBase):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.trans_repo = TransactionRepository()

    def _calculate_rfm_1y(self, df_full: pd.DataFrame) -> pd.DataFrame:
        """
        [æ ¸å¿ƒé€»è¾‘] åŸºäºè¿‡å» 1 å¹´çš„æ•°æ®è®¡ç®— RFM
        """
        if df_full.empty: return pd.DataFrame()

        # 1. æ—¶é—´çª—å£
        # end_date åœ¨åŸºç±»ä¸­å·²ç»æ˜¯ date å¯¹è±¡ï¼Œéœ€è½¬ä¸º timestamp ä»¥ä¾¿è®¡ç®—
        analysis_end_dt = pd.to_datetime(self.end_date)
        one_year_ago = analysis_end_dt - timedelta(days=365)

        # 2. ç­›é€‰
        # ç¡®ä¿ order date ä¹Ÿæ˜¯ timestamp
        df_full["order date"] = pd.to_datetime(df_full["order date"])
        df_1y = df_full[df_full["order date"] >= one_year_ago].copy()

        if df_1y.empty:
            return pd.DataFrame()

        # 3. æ•°æ®æ¸…æ´—ä¸é¢„è®¡ç®—
        bad_actions = ['CA', 'RE', 'CR', 'CC', 'PD']
        dispute_actions = ['CC', 'PD']

        # [å…³é”®ä¿®å¤] å¼ºåˆ¶æ•°å€¼è½¬æ¢ï¼Œé˜²æ­¢å­—ç¬¦ä¸²æ‹¼æ¥é”™è¯¯
        for col in ['revenue', 'Refund']:
            # å°†éæ•°å€¼è½¬ä¸º NaNï¼Œç„¶åå¡« 0
            df_1y[col] = pd.to_numeric(df_1y[col], errors='coerce').fillna(0.0)

        df_1y['is_bad'] = df_1y['action'].isin(bad_actions).astype(int)
        df_1y['is_dispute'] = df_1y['action'].isin(dispute_actions).astype(int)

        # 4. èšåˆè®¡ç®— RFM
        # æ³¨æ„: è¿™é‡Œçš„ sum æ˜¯æ•°å€¼åŠ æ³•ï¼Œå› ä¸ºä¸Šé¢å·²ç»å¼ºåˆ¶è½¬æ¢äº†
        rfm = df_1y.groupby("buyer username").agg({
            "order number": "nunique",  # Frequency
            "revenue": "sum",  # Gross Monetary
            "Refund": "sum",  # Refund Amount (æ³¨æ„ï¼šæ•°æ®åº“é‡Œé€šå¸¸æ˜¯è´Ÿæ•°æˆ–æ­£æ•°ï¼Œéœ€ç¡®è®¤)
            "order date": "max",  # LastDate
            "is_bad": "sum",
            "is_dispute": "sum"
        }).rename(columns={
            "order number": "Frequency",
            "revenue": "Gross_Monetary",
            "order date": "LastDate",
            "is_bad": "BadCount",
            "is_dispute": "DisputeCount"
        })

        # 5. è®¡ç®—é«˜é˜¶æŒ‡æ ‡
        # å‡è®¾ Refund åœ¨æ•°æ®åº“é‡Œæ˜¯è´Ÿæ•° (Clean Log çš„æ ‡å‡†)ã€‚
        # å¦‚æœæ˜¯æ­£æ•°ä»£è¡¨é€€æ¬¾é¢ï¼Œåˆ™åº”è¯¥å‡å»ã€‚
        # ç¨³å¦¥èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾ Net = Gross + Refund (å¦‚æœRefundæ˜¯è´Ÿçš„)
        # æ ¹æ® V1.5.3 é€»è¾‘ï¼ŒRefund æ˜¯è´Ÿå€¼ã€‚
        rfm["Net_Monetary"] = rfm["Gross_Monetary"] + rfm["Refund"]

        rfm["Recency"] = (analysis_end_dt - rfm["LastDate"]).dt.days

        # é¿å…é™¤ä»¥0
        rfm["AOV"] = rfm.apply(lambda x: x["Net_Monetary"] / x["Frequency"] if x["Frequency"] > 0 else 0, axis=1)

        total_lines = df_1y.groupby("buyer username").size()
        rfm["Total_Lines"] = total_lines
        rfm["ReturnRate"] = (rfm["BadCount"] / rfm["Total_Lines"]).fillna(0)

        return rfm.reset_index()

    def run(self):
        self.log(f"ğŸš€ å¯åŠ¨ R-F-P-L-D å®¢æˆ·èšç±»åˆ†æ...")

        # 1. åŠ è½½å…¨é‡æ•°æ® (ä¸ºäº†è®¡ç®— RFMï¼Œéœ€è¦æ›´é•¿çš„æ—¶é—´çª—å£ï¼Œæ¯”å¦‚1å¹´)
        self.log("åŠ è½½è¿‡å»ä¸€å¹´ (365å¤©) äº¤æ˜“æ•°æ®...")

        end_dt = pd.to_datetime(self.end_date)
        start_dt = end_dt - timedelta(days=365)

        # è°ƒç”¨ repo è·å–
        df_raw = self.trans_repo.get_transactions_by_date(start_dt.date(), end_dt.date())

        if df_raw.empty:
            self.log("âš ï¸ è¿‡å»ä¸€å¹´æ— äº¤æ˜“æ•°æ®")
            return

        # 2. è®¡ç®— RFM
        self.log("æ­£åœ¨è®¡ç®—åŠ¨æ€å‡€å€¼ RFM æ¨¡å‹...")
        df_rfm = self._calculate_rfm_1y(df_raw)

        if df_rfm.empty:
            self.log("âš ï¸ è®¡ç®—åæ— æœ‰æ•ˆå®¢æˆ·æ•°æ®")
            return

        # 3. è¯Šæ–­
        self.log("æ­£åœ¨æ‰§è¡Œå®¢æˆ·åˆ†å±‚ (åŸºäºå‡€å€¼ä¸é£é™©)...")
        # ä¼ å…¥ DataFrame
        diagnostician = CustomerDiagnostician(metrics_cur=df_rfm, metrics_prev=None)
        df_final = diagnostician.diagnose()

        if df_final.empty:
            self.log("âš ï¸ æœªå‘ç°ç‰¹å¾æ˜¾è‘—çš„å®¢æˆ·")
            return

        df_final = df_final.sort_values("å‡€æ¶ˆè´¹é¢(Net LTV)", ascending=False)

        # 4. ä¿å­˜
        filename = f"Analysis_Customer_RFM_{self.file_suffix}.csv"
        footer = diagnostician.get_tag_definitions()

        self.save_csv(df_final, filename, footer)
==================== END FILE: core/services/crm.py ====================


==================== START FILE: core/services/logistics.py ====================
# core/services/logistics.py
import os
import pandas as pd
from collections import defaultdict
import tqdm
from core.services.finance.base import ProfitAnalyzerBase
from core.services.diagnostics.logistics import LogisticsDiagnostician
from core.repository.transaction_repo import TransactionRepository
from core.sys.context import get_current_user
from config.settings import settings


class ShippingAnalyzer(ProfitAnalyzerBase):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.trans_repo = TransactionRepository()
        self.df_curr = pd.DataFrame()
        self.df_prev = pd.DataFrame()

    def run(self):
        self.log(f"ğŸš€ å¼€å§‹ç‰©æµåˆ†æ: {self.start_date} -> {self.end_date}")

        self.df_curr = self.trans_repo.get_transactions_by_date(self.start_date, self.end_date)

        delta = self.end_date - self.start_date
        prev_end = self.start_date - pd.Timedelta(days=1)
        prev_start = prev_end - delta
        self.df_prev = self.trans_repo.get_transactions_by_date(prev_start, prev_end)

        # [Fix] å³ä½¿æ— æ•°æ®ï¼Œä¹Ÿç”Ÿæˆç©ºè¡¨
        if self.df_curr.empty:
            self.log("âš ï¸ æœ¬æœŸæ— æ•°æ®ï¼Œç”Ÿæˆç©ºç‰©æµæŠ¥è¡¨ã€‚")
            df3_curr = pd.DataFrame(columns=["Combo", "åŸå§‹é‚®è´¹", "è¶…æ”¯é‚®è´¹", "é‚®è´¹ç½šæ¬¾", "æ€»è®¢å•æ•°"])
            df3_prev = pd.DataFrame()
        else:
            df3_curr = self._compute_df3(self.df_curr)
            df3_prev = self._compute_df3(self.df_prev)

        t1 = self._table1(df3_curr, df3_prev)
        t2 = self._table2(self.df_curr, self.df_prev)
        t3 = df3_curr
        t4 = self._table4(df3_curr)
        t5 = self._table5(df3_curr)

        diag = LogisticsDiagnostician(metrics_cur=df3_curr, metrics_prev=None)
        df_diag = diag.diagnose()

        self._save_suite([t1, t2, t3, t4, t5, df_diag], diag.get_tag_definitions())
        self.log(f" ç‰©æµæŠ¥è¡¨å·²ç”Ÿæˆ: Analysis_Shipping_{self.file_suffix}.csv")

    def _compute_df3(self, df: pd.DataFrame) -> pd.DataFrame:
        if df.empty: return pd.DataFrame()
        order_meta = df.groupby("order number")["full sku"].first()
        combo_map = order_meta.fillna("Unknown").to_dict()

        cols = ["Shipping label-Earning data", "Shipping label-underpay", "Shipping label-overpay",
                "Shipping label-Return"]
        for c in cols:
            if c not in df.columns: df[c] = 0.0

        df_grouped = df.groupby("order number")[cols].sum()
        money = {}
        orders_by_combo = defaultdict(set)

        for order_num, row in df_grouped.iterrows():
            combo = combo_map.get(order_num, "Unknown")
            if combo not in money:
                money[combo] = {"åŸå§‹é‚®è´¹": 0.0, "è¶…æ”¯é‚®è´¹": 0.0, "é‚®è´¹ç½šæ¬¾": 0.0, "åŒ…é‚®é€€è´§é‚®è´¹": 0.0}
            rec = money[combo]
            current_total = row["Shipping label-Earning data"] + row["Shipping label-underpay"] + row[
                "Shipping label-overpay"]
            rec["åŸå§‹é‚®è´¹"] += current_total
            rec["è¶…æ”¯é‚®è´¹"] += row["Shipping label-overpay"]
            rec["é‚®è´¹ç½šæ¬¾"] += row["Shipping label-underpay"]
            rec["åŒ…é‚®é€€è´§é‚®è´¹"] += row["Shipping label-Return"]
            orders_by_combo[combo].add(order_num)

        over_set = set(df_grouped[df_grouped["Shipping label-overpay"] > 0.001].index)
        penal_set = set(df_grouped[df_grouped["Shipping label-underpay"].abs() > 0.001].index)
        ret_set = set(df_grouped[df_grouped["Shipping label-Return"] > 0.001].index)

        rows = []
        for combo, vals in money.items():
            ords = orders_by_combo[combo]
            rows.append({
                "Combo": combo,
                "åŸå§‹é‚®è´¹": round(vals["åŸå§‹é‚®è´¹"], 2),
                "è¶…æ”¯é‚®è´¹": round(vals["è¶…æ”¯é‚®è´¹"], 2),
                "é‚®è´¹ç½šæ¬¾": round(vals["é‚®è´¹ç½šæ¬¾"], 2),
                "åŒ…é‚®é€€è´§é‚®è´¹": round(vals["åŒ…é‚®é€€è´§é‚®è´¹"], 2),
                "åŸå§‹å•æ•°": len(ords),
                "è¶…æ”¯å•æ•°": len(ords & over_set),
                "ç½šæ¬¾å•æ•°": len(ords & penal_set),
                "åŒ…é‚®é€€è´§å•æ•°": len(ords & ret_set),
            })

        if not rows: return pd.DataFrame()
        df3 = pd.DataFrame(rows).sort_values("åŸå§‹é‚®è´¹", ascending=False)
        df3["ç½šæ¬¾æ¯”ä¾‹"] = (df3["é‚®è´¹ç½šæ¬¾"] / df3["åŸå§‹é‚®è´¹"]).fillna(0).apply(lambda x: f"{x:.2%}")
        df3["ç½šæ¬¾å•æ•°æ¯”ä¾‹"] = (df3["ç½šæ¬¾å•æ•°"] / df3["åŸå§‹å•æ•°"]).fillna(0).apply(lambda x: f"{x:.2%}")
        df3["æ€»è®¢å•æ•°"] = df3["åŸå§‹å•æ•°"]

        cols_order = ["Combo", "åŸå§‹é‚®è´¹", "è¶…æ”¯é‚®è´¹", "é‚®è´¹ç½šæ¬¾", "åŒ…é‚®é€€è´§é‚®è´¹", "åŸå§‹å•æ•°", "è¶…æ”¯å•æ•°", "ç½šæ¬¾å•æ•°",
                      "åŒ…é‚®é€€è´§å•æ•°", "ç½šæ¬¾æ¯”ä¾‹", "ç½šæ¬¾å•æ•°æ¯”ä¾‹", "æ€»è®¢å•æ•°"]
        for c in cols_order:
            if c not in df3.columns: df3[c] = 0
        return df3[cols_order]

    def _table1(self, cur, prev):
        if cur.empty: return pd.DataFrame(columns=["é¡¹ç›®", "è´¹ç”¨", "æ¯”ä¾‹", "ç¯æ¯”"])
        c_vals = [cur["åŸå§‹é‚®è´¹"].sum(), cur["è¶…æ”¯é‚®è´¹"].sum(), cur["é‚®è´¹ç½šæ¬¾"].sum()]
        p_vals = [prev["åŸå§‹é‚®è´¹"].sum(), prev["è¶…æ”¯é‚®è´¹"].sum(), prev["é‚®è´¹ç½šæ¬¾"].sum()] if not prev.empty else [0, 0,
                                                                                                                  0]
        total_c = c_vals[0]
        rows = [
            ["æ€»é‚®è´¹(Total)", total_c, "100.00%", self._diff(total_c, p_vals[0])],
            ["è¶…æ”¯é‚®è´¹(Over)", c_vals[1], self._pct(c_vals[1], total_c), self._diff(c_vals[1], p_vals[1])],
            ["ç½šæ¬¾é‚®è´¹(Fine)", c_vals[2], self._pct(c_vals[2], total_c), self._diff(c_vals[2], p_vals[2])]
        ]
        return pd.DataFrame(rows, columns=["é¡¹ç›®", "è´¹ç”¨", "æ¯”ä¾‹", "ç¯æ¯”"])

    def _table2(self, df_c, df_p):
        if df_c.empty: return pd.DataFrame(columns=["é¡¹ç›®", "å•æ•°", "æ¯”ä¾‹", "ç¯æ¯”"])
        c_cnt = df_c['order number'].nunique()
        p_cnt = df_p['order number'].nunique() if not df_p.empty else 0
        diff = (c_cnt - p_cnt) / p_cnt if p_cnt != 0 else 0
        return pd.DataFrame([["æ€»è®¢å•æ•°", c_cnt, "100%", f"{diff:.2%}"]], columns=["é¡¹ç›®", "å•æ•°", "æ¯”ä¾‹", "ç¯æ¯”"])

    def _table4(self, df3):
        if df3.empty: return pd.DataFrame()
        t = df3[df3["æ€»è®¢å•æ•°"] > 5].copy()
        if t.empty: return pd.DataFrame()
        t["_sort_val"] = t["ç½šæ¬¾æ¯”ä¾‹"].astype(str).str.rstrip("%").astype(float)
        return t.nlargest(10, "_sort_val")[["Combo", "åŸå§‹é‚®è´¹", "é‚®è´¹ç½šæ¬¾", "ç½šæ¬¾æ¯”ä¾‹"]]

    def _table5(self, df3):
        if df3.empty: return pd.DataFrame()
        t = df3[df3["æ€»è®¢å•æ•°"] > 5].copy()
        if t.empty: return pd.DataFrame()
        t["_sort_val"] = t["ç½šæ¬¾å•æ•°æ¯”ä¾‹"].astype(str).str.rstrip("%").astype(float)
        return t.nlargest(10, "_sort_val")[["Combo", "åŸå§‹å•æ•°", "ç½šæ¬¾å•æ•°", "ç½šæ¬¾å•æ•°æ¯”ä¾‹"]]

    def _save_suite(self, tables, footer=None):
        filename = f"Analysis_Shipping_{self.file_suffix}.csv"
        user = get_current_user()
        safe_user = "".join([c for c in user if c.isalnum() or c in ('_', '-')])
        sub_dir = safe_user if safe_user else "default"
        user_output_dir = settings.OUTPUT_DIR / sub_dir
        if not user_output_dir.exists(): user_output_dir.mkdir(parents=True, exist_ok=True)
        save_path = user_output_dir / filename

        try:
            with open(save_path, "w", encoding="utf-8-sig") as f:
                names = ["è¡¨1_è´¹ç”¨æ±‡æ€»", "è¡¨2_å•æ•°æ±‡æ€»", "è¡¨3_Comboè¯¦æƒ…", "è¡¨4_ç½šæ¬¾é‡‘é¢Top10", "è¡¨5_ç½šæ¬¾å•æ•°Top10",
                         "C1_æ™ºèƒ½è¯Šæ–­"]
                for i, df in enumerate(tables):
                    if i < len(names):
                        f.write(f"=== {names[i]} ===\n")
                    else:
                        f.write(f"=== Table {i + 1} ===\n")
                    df.to_csv(f, index=False)
                    f.write("\n\n")
                if footer:
                    f.write("\n")
                    for line in footer: f.write(f"{line}\n")
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¤±è´¥: {e}")

    def _pct(self, v, total):
        return f"{v / total:.2%}" if total else "0.00%"

    def _diff(self, cur, prev):
        if not prev: return "0.00%"
        return f"{(cur - prev) / prev:.2%}"
==================== END FILE: core/services/logistics.py ====================


==================== START FILE: core/services/report_manager.py ====================
# core/services/report_manager.py
"""
æ–‡ä»¶è¯´æ˜: æŠ¥è¡¨æ–‡ä»¶ç®¡ç†å™¨ (Report File Manager)
ä¸»è¦åŠŸèƒ½:
1. ç®¡ç† output/ ç›®å½•ä¸‹çš„æŠ¥è¡¨æ–‡ä»¶ã€‚
2. å®ç°ç”¨æˆ·éš”ç¦»ï¼šæ ¹æ®å½“å‰ç™»å½•ç”¨æˆ·æ“ä½œå¯¹åº”çš„å­ç›®å½•ã€‚
3. æä¾›æ–‡ä»¶åˆ—è¡¨æŸ¥è¯¢ã€æ¸…ç©ºã€ZIP æ‰“åŒ…ä¸‹è½½åŠŸèƒ½ã€‚
"""

import os
import shutil
import zipfile
from io import BytesIO
from typing import List, Optional
from pathlib import Path

from config.settings import settings
from core.sys.context import get_current_user


class ReportFileManager:

    def __init__(self):
        # [æ ¸å¿ƒ] åŠ¨æ€å®šä½ç”¨æˆ·ç›®å½•
        user = get_current_user()
        # ç®€å•æ¸…æ´—ç”¨æˆ·åï¼Œé˜²æ­¢éæ³•å­—ç¬¦
        sub_dir = user if user and user != "System" else "default"

        self.output_dir = settings.OUTPUT_DIR / sub_dir

        if not self.output_dir.exists():
            self.output_dir.mkdir(parents=True, exist_ok=True)

    def get_generated_files(self) -> List[str]:
        """è·å–æ‰€æœ‰ç”Ÿæˆçš„ CSV æ–‡ä»¶å"""
        if not self.output_dir.exists(): return []
        # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åˆ—
        files = [f for f in os.listdir(self.output_dir) if f.lower().endswith('.csv')]
        files.sort(key=lambda x: os.path.getmtime(os.path.join(self.output_dir, x)), reverse=True)
        return files

    def get_file_path(self, filename: str) -> Path:
        """è·å–ç»å¯¹è·¯å¾„"""
        return self.output_dir / filename

    def clear_all_reports(self) -> None:
        """æ¸…ç©ºå½“å‰ç”¨æˆ·çš„è¾“å‡ºç›®å½•"""
        for filename in os.listdir(self.output_dir):
            file_path = self.output_dir / filename
            try:
                if os.path.isfile(file_path) or os.path.islink(file_path):
                    os.unlink(file_path)
                elif os.path.isdir(file_path):
                    shutil.rmtree(file_path)
            except Exception as e:
                print(f"Error deleting {file_path}: {e}")

    def create_zip_archive(self, file_list: Optional[List[str]] = None) -> bytes:
        """å°†æ–‡ä»¶æ‰“åŒ…ä¸º ZIP å­—èŠ‚æµ (ç”¨äºä¸‹è½½)"""
        buffer = BytesIO()
        target_files = file_list if file_list is not None else self.get_generated_files()

        with zipfile.ZipFile(buffer, "w", zipfile.ZIP_DEFLATED) as zf:
            for fname in target_files:
                fpath = self.output_dir / fname
                if fpath.exists():
                    zf.write(fpath, arcname=fname)

        return buffer.getvalue()
==================== END FILE: core/services/report_manager.py ====================


==================== START FILE: core/services/visual_service.py ====================
# core/services/visual_service.py
"""
æ–‡ä»¶è¯´æ˜: å¯è§†åŒ–æ•°æ®èšåˆæœåŠ¡ (Visual Analytics Service)
ä¸»è¦åŠŸèƒ½:
1. ä» Data_Clean_Log è¯»å–æ¸…æ´—åçš„æ•°æ®ã€‚
2. æ‰§è¡Œå¤æ‚çš„å‘é‡åŒ–è®¡ç®—ï¼š
   - è¿˜åŸçœŸå®ç‰©ç†é”€é‡ (Real Qty): é”€å”®å• x2, é€€è´§å• x-2 (ç‰¹æ®Šä¸šåŠ¡è§„åˆ™)ã€‚
   - è®¡ç®—æ€»æˆæœ¬ (COGS): å…³è” SKU æˆæœ¬ã€‚
   - å½’ç±»è´¹ç”¨: è¿è´¹ã€å¹³å°è´¹ã€ç½šæ¬¾ç­‰ã€‚
3. æŒ‰æ—¶é—´ç²’åº¦ (æ—¥/å‘¨/æœˆ) èšåˆæ•°æ®ï¼Œä¾›å‰ç«¯ Altair ç»˜å›¾ã€‚
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
from datetime import date

from core.components.db.client import DBClient
from core.services.inventory.repository import InventoryRepository
from core.services.finance.base import ProfitAnalyzerBase
from core.sys.logger import get_logger


class VisualService:

    def __init__(self):
        self.db = DBClient()
        self.repo_inv = InventoryRepository()
        self.logger = get_logger("VisualService")

        # å¼•ç”¨åŸºç±»çš„è´¹ç”¨å®šä¹‰ï¼Œä¿æŒé€»è¾‘ä¸€è‡´
        self.PLATFORM_FEE_COLS = list(ProfitAnalyzerBase.PLATFORM_FEE_GROUP)

        self.ACTION_GROUPS = {
            'CA': 'Cancel', 'RE': 'Return', 'CC': 'Case',
            'CR': 'Request', 'PD': 'Dispute'
        }

        # åº—é“ºåç§°æ˜ å°„ (UI -> DB)
        self.STORE_MAP = {
            "88": "esparts88",
            "esplus": "espartsplus"
        }

        # ç‰¹æ®Š SKU é›†åˆ (ç”¨äºåŠ æƒè°ƒæ•´)
        self.KEY_SKUS = {"NU1C8E51C", "NU1C8E51K"}

    def _get_date_grain(self, start: date, end: date) -> str:
        """æ ¹æ®æ—¶é—´è·¨åº¦è‡ªåŠ¨å†³å®šèšåˆç²’åº¦"""
        delta = (end - start).days
        if delta <= 35:
            return 'D'  # 1ä¸ªæœˆå†…çœ‹å¤©
        elif delta <= 180:
            return 'W'  # åŠå¹´å†…çœ‹å‘¨
        else:
            return 'ME'  # åŠå¹´ä»¥ä¸Šçœ‹æœˆ

    def _safe_numeric(self, series: pd.Series) -> pd.Series:
        """[å®‰å…¨] å¼ºåˆ¶è½¬æ•°å€¼å¹¶å¡«0"""
        return pd.to_numeric(series, errors='coerce').fillna(0.0)

    def _vectorized_cogs_calc(self, df: pd.DataFrame, sku_cost_map: Dict[str, float]) -> pd.Series:
        """[é«˜æ€§èƒ½] å‘é‡åŒ–è®¡ç®—æ¯è¡Œæ€»æˆæœ¬"""
        total_cogs = pd.Series(0.0, index=df.index)
        base_qty = self._safe_numeric(df['quantity'])

        # éå† sku1 - sku10
        for i in range(1, 11):
            s_col = f'sku{i}'
            q_col = f'qty{i}'
            if s_col not in df.columns: continue

            # è·å– SKU å¯¹åº”çš„æˆæœ¬ (Map)
            # æ³¨æ„: map ä¹‹å‰è¦ç¡®ä¿ key æ ¼å¼ä¸€è‡´ (å¤§å†™å»ç©º)
            current_skus = df[s_col].astype(str).str.strip().str.upper()
            unit_costs = current_skus.map(sku_cost_map).fillna(0.0)

            per_qtys = self._safe_numeric(df[q_col])

            # ç´¯åŠ : å•ä¸ªæˆæœ¬ * å¥—å†…æ•°é‡ * å¥—æ•°
            line_cost = unit_costs * per_qtys * base_qty
            total_cogs += line_cost

        return total_cogs

    def load_and_aggregate(self, start_date: date, end_date: date, stores: List[str]) -> Tuple[pd.DataFrame, str]:
        """
        [ä¸»å…¥å£] åŠ è½½æ•°æ®å¹¶èšåˆ
        Returns: (Aggregated DataFrame, Debug SQL)
        """
        if not stores:
            return pd.DataFrame(), "No stores selected"

        # 1. æ„é€ æŸ¥è¯¢
        db_stores = [self.STORE_MAP.get(s, s) for s in stores]
        store_str = "', '".join(db_stores)

        sql = f"""
            SELECT * FROM Data_Clean_Log 
            WHERE `order date` >= :start
              AND `order date` <= :end
              AND `seller` IN ('{store_str}')
        """
        params = {
            "start": start_date.strftime('%Y-%m-%d'),
            "end": end_date.strftime('%Y-%m-%d')
        }

        self.logger.info(f"Visual Query: {start_date} -> {end_date} | Stores: {stores}")
        df = self.db.read_df(sql, params)

        if df.empty:
            return pd.DataFrame(), sql

        # --- 2. é¢„å¤„ç† (Preprocessing) ---
        # å½’ä¸€åŒ–åˆ—å
        df.columns = [c.strip().lower() for c in df.columns]

        # [å…³é”®ä¿®å¤] å¼ºåˆ¶ç±»å‹è½¬æ¢ï¼Œé˜²æ­¢ Object ç±»å‹å¯¼è‡´è®¡ç®—å´©æºƒ
        df['quantity'] = self._safe_numeric(df.get('quantity', 0))
        df['revenue'] = self._safe_numeric(df.get('revenue', 0))

        # æ˜ å°„ Action (ç”¨äºåˆ†ç»„)
        # è¿™é‡Œçš„ Action ä»£ç è½¬ä¸ºäººç±»å¯è¯»çš„ Sales/Return...
        df['ui_action'] = df['action'].astype(str).str.strip().str.upper().map(self.ACTION_GROUPS).fillna('Sales')

        # è®¡ç®—çœŸå®ç‰©ç†æ¶ˆè€— (Business Rule)
        # ç®€åŒ–é€»è¾‘ï¼šè¿™é‡Œç›´æ¥ç”¨ quantityï¼Œå¦‚æœéœ€è¦ç‰¹æ®Š SKU æƒé‡ (NU1C8 * 2)ï¼Œå¯ä»¥åœ¨è¿™é‡ŒåŠ é€»è¾‘
        # å¤åˆ» V1.5.3: Sales=+2, Return=-2 ç­‰æƒé‡é€»è¾‘
        # ä¸ºäº†é€šç”¨æ€§ï¼Œæš‚æ—¶ç®€åŒ–ä¸ºç›´æ¥å– quantityï¼Œè§†ä½œ"å½±å“åº“å­˜çš„æ•°é‡"
        df['calc_real_qty'] = df['quantity']

        # 3. è´¹ç”¨å½’ç±» (Fees)
        # è¿è´¹ç›¸å…³
        ship_map = {
            'shipping label-earning data': 'calc_ship_regular',
            'shipping label-underpay': 'calc_ship_under',
            'shipping label-overpay': 'calc_ship_over',
            'shipping label-return': 'calc_ship_return'
        }
        for db_c, calc_c in ship_map.items():
            if db_c in df.columns:
                df[calc_c] = self._safe_numeric(df[db_c]).abs()  # å–ç»å¯¹å€¼æ–¹ä¾¿å±•ç¤º
            else:
                df[calc_c] = 0.0

        # å¹³å°è´¹ç›¸å…³
        valid_plat = [c.lower() for c in self.PLATFORM_FEE_COLS if c.lower() in df.columns]
        if valid_plat:
            # sum(axis=1) è¡Œæ±‚å’Œ
            df['calc_platform_fee'] = df[valid_plat].apply(pd.to_numeric, errors='coerce').fillna(0.0).sum(axis=1).abs()
        else:
            df['calc_platform_fee'] = 0.0

        # 4. æˆæœ¬è®¡ç®— (COGS)
        df_cogs = self.repo_inv.get_all_cogs()
        sku_cost_map = dict(zip(
            df_cogs['SKU'].astype(str).str.strip().str.upper(),
            pd.to_numeric(df_cogs['Cog'], errors='coerce').fillna(0)
        ))
        df['calc_cogs'] = self._vectorized_cogs_calc(df, sku_cost_map).abs()

        # --- 5. èšåˆ (Aggregation) ---
        df['dt'] = pd.to_datetime(df['order date'])
        grain = self._get_date_grain(start_date, end_date)

        # å®šä¹‰èšåˆè§„åˆ™
        agg_rules = {
            'revenue': 'sum',  # é‡‘é¢
            'calc_real_qty': 'sum',  # æ•°é‡
            'order number': 'nunique',  # è®¢å•æ•°
            'calc_cogs': 'sum',  # æˆæœ¬
            'calc_platform_fee': 'sum',  # å¹³å°è´¹
            'calc_ship_regular': 'sum',  # æ­£å¸¸è¿è´¹
            'calc_ship_under': 'sum',  # ç½šæ¬¾
            'calc_ship_over': 'sum',  # è¶…æ”¯
            'calc_ship_return': 'sum'  # é€€è´§è¿è´¹
        }

        # A. æŒ‰ Action åˆ†ç»„ (Sales, Return, Cancel...)
        # GroupBy [Time, Action]
        grouped = df.groupby([pd.Grouper(key='dt', freq=grain), 'ui_action']).agg(agg_rules).reset_index()

        # Pivot: è¡Œè½¬åˆ—
        # Index=dt, Columns=Action, Values=Metrics
        # ç»“æœåˆ—åä¼šå˜æˆ: revenue_Sales, revenue_Return ...
        pivot_df = grouped.pivot(index='dt', columns='ui_action', values=list(agg_rules.keys()))
        # å±•å¹³å¤šçº§åˆ—å (e.g., ('revenue', 'Sales') -> 'Sales_revenue')
        # æˆ‘ä»¬å¸Œæœ›æ ¼å¼: {Action}_{Metric} (e.g., Sales_Amount)
        pivot_df.columns = [f"{col[1]}_{col[0]}" for col in pivot_df.columns]

        # B. è®¡ç®—æ€»é‡ (Total)
        total_agg = df.groupby(pd.Grouper(key='dt', freq=grain)).agg(agg_rules)
        total_agg.columns = [f"Total_{c}" for c in total_agg.columns]

        # C. åˆå¹¶
        final_df = pd.concat([pivot_df, total_agg], axis=1).fillna(0)

        # D. é‡å‘½åä¸º UI å‹å¥½æ ¼å¼
        rename_map = {}
        for col in final_df.columns:
            # æ›¿æ¢åŸå§‹åˆ—åä¸ºæ ‡å‡†æŒ‡æ ‡å
            new = col.replace('revenue', 'Amount') \
                .replace('calc_real_qty', 'Quantity') \
                .replace('order number', 'Order') \
                .replace('calc_cogs', 'COGS') \
                .replace('calc_platform_fee', 'PlatformFee') \
                .replace('calc_ship_regular', 'ShipRegular') \
                .replace('calc_ship_under', 'ShipUnder') \
                .replace('calc_ship_over', 'ShipOver') \
                .replace('calc_ship_return', 'ShipReturn')
            rename_map[col] = new

        final_df = final_df.rename(columns=rename_map)

        # å¢åŠ æ—¥æœŸå­—ç¬¦ä¸²åˆ—ä¾› Altair ä½¿ç”¨
        final_df['DateStr'] = final_df.index.strftime('%Y-%m-%d')

        return final_df.sort_index(), sql
==================== END FILE: core/services/visual_service.py ====================


==================== START FILE: core/services/prediction.py ====================
# core/services/prediction.py

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
import tqdm
from collections import defaultdict
from typing import Dict, Any

from core.services.finance.base import ProfitAnalyzerBase
from core.repository.transaction_repo import TransactionRepository
from core.repository.sku_repo import SkuRepository
from config.settings import settings

# å¼•å…¥ç®—æ³•æ¨¡å‹
from core.components.algo.models import (
    XGBoostForecaster, SarimaForecaster, HoltWintersForecaster,
    ETSForecaster, CrostonForecaster, WeightedCycleForecaster
)


class PredictionService(ProfitAnalyzerBase):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.trans_repo = TransactionRepository()
        self.sku_repo = SkuRepository()

        self.models = [
            XGBoostForecaster(), SarimaForecaster(), HoltWintersForecaster(),
            ETSForecaster(), CrostonForecaster(), WeightedCycleForecaster()
        ]

    def _get_loss_rate(self, action: str) -> float:
        action = str(action).strip().upper()
        R = settings.LOSS_RATES
        if action == 'CA': return 1.0
        if action == 'RE': return R.get('RETURN', 0.3)
        if action == 'CC': return R.get('CASE', 0.6)
        if action == 'CR': return R.get('REQUEST', 0.5)
        if action == 'PD': return R.get('DISPUTE', 1.0)
        return 0.0

    def _aggregate_monthly_sales(self) -> pd.DataFrame:
        end_dt = datetime.now().replace(day=1) - timedelta(days=1)
        start_dt = end_dt - relativedelta(months=24)

        self.log(f"ğŸ“¥ æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®: {start_dt.date()} -> {end_dt.date()}")
        df_raw = self.trans_repo.get_transactions_by_date(start_dt.date(), end_dt.date())

        if df_raw.empty:
            self.log("âš ï¸ è­¦å‘Š: æŒ‡å®šèŒƒå›´å†…æ— äº¤æ˜“æ•°æ®ã€‚")
            return pd.DataFrame()

        self.log(f"ğŸ“Š åŸå§‹è®°å½•åŠ è½½å®Œæˆ: {len(df_raw)} æ¡ï¼Œå¼€å§‹èšåˆå¤„ç†...")
        monthly_data = defaultdict(lambda: defaultdict(int))
        records = df_raw.to_dict('records')

        SPECIAL_SOURCE_SKUS = {"NU1C8E51C", "NU1C8E51K"}
        SPECIAL_TARGET_SKU = "NU1C8SKT7"

        for row in tqdm.tqdm(records, desc="èšåˆé”€é‡æ•°æ®"):
            date_val = row.get("order date")
            if pd.isna(date_val): continue
            month_key = date_val.strftime("%Y-%m")

            action = row.get("action", "")
            loss_rate = self._get_loss_rate(action)
            if loss_rate >= 1.0: continue
            effective_ratio = 1.0 - loss_rate

            try:
                base_qty = int(float(row.get("quantity", 0)))
            except:
                base_qty = 0
            if base_qty <= 0: continue

            for i in range(1, 21):
                s_key = f"sku{i}";
                q_key = f"qty{i}"
                if s_key not in row: break
                raw_sku = str(row.get(s_key))
                if not raw_sku or raw_sku.lower() in ['nan', 'none', '', '0']: continue

                sku = raw_sku.strip().upper()
                try:
                    per_qty = float(row.get(q_key, 0))
                except:
                    per_qty = 0
                if per_qty <= 0: continue

                net_qty = base_qty * per_qty * effective_ratio
                monthly_data[sku][month_key] += int(net_qty)

                if sku in SPECIAL_SOURCE_SKUS:
                    special_qty = base_qty * 2 * effective_ratio
                    monthly_data[SPECIAL_TARGET_SKU][month_key] += int(special_qty)

        if not monthly_data: return pd.DataFrame()
        df = pd.DataFrame.from_dict(monthly_data, orient='index').fillna(0)
        df = df[sorted(df.columns)]
        return df

    def _evaluate_models(self, series: pd.Series, horizon: int = 3) -> Dict[str, float]:
        scores = {m.name: 0.0 for m in self.models}
        if len(series) < 6: return scores
        y = pd.to_numeric(series, errors='coerce').fillna(0)

        for model in self.models:
            try:
                abs_errors = []
                actuals = []
                for i in range(horizon, 0, -1):
                    train = y.iloc[:-i]
                    actual = y.iloc[-i]
                    pred = model.fit_predict(train, periods=1)
                    if pred < 0: pred = 0
                    abs_errors.append(abs(actual - pred))
                    actuals.append(actual)

                sum_actual = sum(actuals)
                sum_error = sum(abs_errors)
                if sum_actual == 0:
                    accuracy = 100.0 if sum_error < 1.0 else 0.0
                else:
                    wmape = sum_error / sum_actual
                    accuracy = max(0, 1.0 - wmape) * 100
                scores[model.name] = round(accuracy, 2)
            except:
                scores[model.name] = 0.0
        return scores

    def run(self):
        self.log("ğŸš€ å¯åŠ¨ AI é”€é‡é¢„æµ‹å¼•æ“ (Tournament Mode)...")

        # 1. è·å–æ•°æ®
        df_matrix = self._aggregate_monthly_sales()

        # [Fix] å‡†å¤‡è¾“å‡ºç»“æ„ (å³ä½¿ä¸ºç©ºä¹Ÿè¦ç”Ÿæˆæ–‡ä»¶)
        output_cols = ["SKU", "Best_Algo", "BestForecast"]
        for m in self.models:
            output_cols.extend([f"{m.name}_Forecast", f"{m.name}_Score"])

        results = []

        if not df_matrix.empty:
            for sku, row in tqdm.tqdm(df_matrix.iterrows(), total=len(df_matrix), desc="æ¨¡å‹ç«å¤‡ä¸­"):
                series = row
                valid_len = np.count_nonzero(series.values)
                is_new = valid_len < 6

                if is_new:
                    wc_model = self.models[-1]
                    best_val = wc_model.fit_predict(series)
                    best_algo = "NewProduct_Rule"
                    record = {"SKU": sku, "Best_Algo": best_algo, "BestForecast": round(best_val, 2)}
                    for m in self.models:
                        record[f"{m.name}_Forecast"] = 0
                        record[f"{m.name}_Score"] = 0
                    record["WeightedCycle_Forecast"] = round(best_val, 2)
                    record["WeightedCycle_Score"] = 100
                else:
                    preds = {}
                    for m in self.models:
                        try:
                            p = m.fit_predict(series)
                            preds[m.name] = round(p, 2) if p >= 0 else 0.0
                        except:
                            preds[m.name] = 0.0

                    scores = self._evaluate_models(series)
                    best_algo = max(self.models, key=lambda m: scores.get(m.name, 0)).name
                    best_val = preds.get(best_algo, 0.0)

                    record = {"SKU": sku, "Best_Algo": best_algo, "BestForecast": best_val}
                    for m in self.models:
                        record[f"{m.name}_Forecast"] = preds.get(m.name, 0)
                        record[f"{m.name}_Score"] = scores.get(m.name, 0)
                results.append(record)

        # [Fix] æ— è®ºæ˜¯å¦æœ‰ç»“æœï¼Œéƒ½ä¿å­˜æ–‡ä»¶
        df_res = pd.DataFrame(results) if results else pd.DataFrame(columns=output_cols)

        footer = [
            "ğŸ“˜ AI é¢„æµ‹é€»è¾‘è¯´æ˜:",
            "1. é”¦æ ‡èµ›æœºåˆ¶: è¿è¡Œ 6 ç§ç®—æ³•å¹¶é€‰æ‹©å† å†›ã€‚",
            f"2. ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        ]

        filename = "Estimated_Monthly_SKU.csv"
        # åŸºç±»çš„ save_csv å·²ç»ä¿®æ”¹ä¸ºå…è®¸ä¿å­˜ç©ºè¡¨
        self.save_csv(df_res, filename, footer)
        if df_res.empty:
            self.log(f"âš ï¸ æ•°æ®ä¸è¶³ï¼Œç”Ÿæˆç©ºé¢„æµ‹è¡¨: {filename}")
        else:
            self.log(f" é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {filename}")
==================== END FILE: core/services/prediction.py ====================


==================== START FILE: core/services/log_service.py ====================
# core/services/log_service.py
"""
æ–‡ä»¶è¯´æ˜: æ—¥å¿—ç®¡ç†ä¸šåŠ¡æœåŠ¡ (Log Service) - Core Logic
ä¸»è¦åŠŸèƒ½:
1. æ—¥å¿—æ–‡ä»¶è¯»å–ä¸è§£æ (Parsing)ã€‚
2. DataFrame è½¬æ¢ã€‚
3. ç‰¹æƒæ—¥å¿—æ¸…æ´— (Purging) ä¸ ä¸å¯ç£¨ç­å®¡è®¡è®°å½•ã€‚
"""

import os
import shutil
import pandas as pd
import datetime
from pathlib import Path
from typing import List, Dict, Any

from config.settings import settings
from core.sys.logger import get_logger

# ä¸šåŠ¡æ—¥å¿—è®°å½•å™¨
app_logger = get_logger("LogService")
IMMUTABLE_MARKER = "[[SECURITY_AUDIT]]"


class LogService:

    @staticmethod
    def _parse_line(line: str) -> Dict[str, Any]:
        """
        [æ ¸å¿ƒè§£æå™¨]
        æ ¼å¼: TIME | LEVEL | MODULE | user=X | ip=Y | func=Z | ... | Message
        """
        line = line.strip()
        if not line: return {}

        parts = line.split(" | ")
        if len(parts) < 3: return {}

        try:
            meta = {
                "time": parts[0].strip(),
                "level": parts[1].strip(),
                "raw_msg": line
            }

            # åŠ¨æ€è§£æé”®å€¼å¯¹
            msg_parts = []
            for part in parts[2:]:
                if "=" in part and " " not in part.split("=")[0]:
                    k, v = part.split("=", 1)
                    meta[k.strip()] = v.strip()
                else:
                    msg_parts.append(part)

            meta["message"] = " | ".join(msg_parts).strip()

            # å­—æ®µå…œåº• (ä¿è¯ DataFrame ç»“æ„ä¸€è‡´)
            for key in ["user", "ip", "func", "action", "table", "loc", "trace"]:
                if key not in meta: meta[key] = "-"

            return meta
        except:
            return {}

    @classmethod
    def get_logs_as_df(cls, filename: str, max_lines: int = 5000) -> pd.DataFrame:
        """è¯»å–æ—¥å¿—å¹¶è¿”å› DataFrame"""
        path = settings.LOG_DIR / filename
        if not path.exists():
            return pd.DataFrame()

        try:
            with path.open("r", encoding="utf-8", errors="ignore") as f:
                # è¯»å–æœ€å N è¡Œ
                lines = f.readlines()[-max_lines:]

            # å€’åºå±•ç¤º (æœ€æ–°çš„åœ¨æœ€å‰)
            lines.reverse()

            data = [cls._parse_line(l) for l in lines]
            # è¿‡æ»¤è§£æå¤±è´¥çš„è¡Œ
            data = [d for d in data if d and d.get("time")]

            return pd.DataFrame(data)
        except Exception as e:
            app_logger.error(f"æ—¥å¿—è¯»å–å¤±è´¥: {e}")
            return pd.DataFrame()

    @classmethod
    def purge_logs_by_range(cls, filename: str, start_dt: datetime.datetime, end_dt: datetime.datetime,
                            operator: str, reason: str) -> int:
        """
        [ç‰¹æƒæ“ä½œ] ç‰©ç†æ¸…æ´—æ—¥å¿—
        :param operator: æ“ä½œäººç”¨æˆ·å
        :param reason: æ¸…æ´—åŸå› 
        :return: åˆ é™¤è¡Œæ•°
        """
        path = settings.LOG_DIR / filename
        if not path.exists(): return 0

        temp_path = path.with_suffix(".tmp")
        deleted_count = 0

        s_str = start_dt.strftime("%Y-%m-%d %H:%M:%S")
        e_str = end_dt.strftime("%Y-%m-%d %H:%M:%S")

        try:
            with path.open("r", encoding="utf-8") as fin, \
                    temp_path.open("w", encoding="utf-8") as fout:

                for line in fin:
                    # 1. è§„åˆ™: ä¸å¯ç£¨ç­æ ‡è®° (æ°¸è¿œä¿ç•™)
                    if IMMUTABLE_MARKER in line:
                        fout.write(line)
                        continue

                    # 2. è§„åˆ™: æ—¶é—´æ ¼å¼æ ¡éªŒ
                    if len(line) < 19:
                        fout.write(line)
                        continue

                    t_str = line[:19]

                    # 3. è§„åˆ™: å‘½ä¸­æ—¶é—´åŒºé—´ -> åˆ é™¤ (è·³è¿‡å†™å…¥)
                    if s_str <= t_str <= e_str:
                        deleted_count += 1
                        continue

                    fout.write(line)

            # åŸå­æ›¿æ¢
            shutil.move(str(temp_path), str(path))

            # [Core] è®°å½•æ“ä½œç—•è¿¹ (åˆ°ä¸šåŠ¡æ—¥å¿—)
            # å³ä½¿æ¸…æ´—çš„æ˜¯ audit.logï¼Œç—•è¿¹ä¹Ÿä¼šç•™åœ¨ app.log ä¸­ï¼Œäº’ä¸ºå¤‡ä»½
            audit_msg = f"{IMMUTABLE_MARKER} [LOG PURGE] Target: {filename} | Rows: {deleted_count} | Range: {s_str}~{e_str} | Reason: {reason}"
            app_logger.critical(audit_msg, extra={"user": operator, "action": "LOG_PURGE", "func": "å®‰å…¨å®¡è®¡"})

            return deleted_count

        except Exception as e:
            if temp_path.exists(): os.remove(temp_path)
            app_logger.error(f"æ—¥å¿—æ¸…æ´—å¤±è´¥: {e}")
            raise e
==================== END FILE: core/services/log_service.py ====================


==================== START FILE: core/services/database_service.py ====================
# core/services/database_service.py
"""
æ–‡ä»¶è¯´æ˜: æ•°æ®åº“ç”Ÿå‘½å‘¨æœŸç®¡ç†æœåŠ¡ (Database Service) - Fix DateTime Parsing
ä¸»è¦åŠŸèƒ½:
1. å¤‡ä»½ç®¡ç† (Backup/Restore).
2. [Fix] æ–‡ä»¶åè§£æ: ä¿®æ­£ datetime è°ƒç”¨é”™è¯¯ï¼Œç¡®ä¿èƒ½æ­£ç¡®æ˜¾ç¤º 'ğŸ•’ æ—¶é—´ | ğŸ·ï¸ å¤‡æ³¨'ã€‚
3. æ•°æ®æ¸…æ´—ä¸å®¡è®¡ (è¡¨åè„±æ•)ã€‚
"""

import os
import subprocess
import datetime
import shutil
import pandas as pd
from pathlib import Path
from typing import List, Tuple, Callable, Optional
from sqlalchemy import text

from config.settings import settings
from core.components.db.client import DBClient
from core.sys.logger import get_logger, get_audit_logger
from core.sys.context import get_current_user

IMMUTABLE_MARKER = "[[SECURITY_AUDIT]]"


class DatabaseService:

    def __init__(self):
        self.logger = get_logger("DatabaseService")
        self.audit_logger = get_audit_logger()
        self.db = DBClient()

        self.host = settings.DB_HOST
        self.port = settings.DB_PORT
        self.user = settings.DB_USER
        self.password = settings.DB_PASS
        self.db_name = settings.DB_NAME

        self.backup_dir = settings.BACKUP_DIR

    # =========================================================================
    # [Fix] æ–‡ä»¶åè§£æé€»è¾‘ (å…³é”®ä¿®å¤)
    # =========================================================================
    @staticmethod
    def parse_filename_to_display(filename: str) -> str:
        """
        Input:  20251209_175224_Init_Backup.sql
        Output: ğŸ•’ 2025-12-09 17:52:24 | ğŸ·ï¸ Init_Backup
        """
        try:
            # 1. å»æ‰è·¯å¾„å’Œåç¼€
            clean_name = os.path.basename(filename)
            if clean_name.lower().endswith('.sql'):
                clean_name = clean_name[:-4]

            # 2. åˆ†å‰² (æœ€å¤šåˆ†å‰²2æ¬¡: Date, Time, Rest)
            parts = clean_name.split("_", 2)

            if len(parts) >= 2:
                date_part = parts[0]  # 20251209
                time_part = parts[1]  # 175224
                tag_part = parts[2] if len(parts) > 2 else ""  # Init_Backup

                # 3. æ ¡éªŒæ•°å­—æ ¼å¼
                if len(date_part) == 8 and len(time_part) == 6 and date_part.isdigit() and time_part.isdigit():
                    # [Fix] ä½¿ç”¨ datetime.datetime.strptime
                    dt_obj = datetime.datetime.strptime(f"{date_part}{time_part}", "%Y%m%d%H%M%S")
                    dt_str = dt_obj.strftime("%Y-%m-%d %H:%M:%S")

                    if tag_part:
                        return f"ğŸ•’ {dt_str} | ğŸ·ï¸ {tag_part}"
                    else:
                        return f"ğŸ•’ {dt_str}"
        except Exception:
            pass

        # è§£æå¤±è´¥å…œåº•
        return f"â“ {filename}"

    # --- Backup & Restore Logic ---
    def list_backups(self) -> List[str]:
        return sorted([f.name for f in self.backup_dir.glob("*.sql")], reverse=True)

    def create_backup(self, tag: str = "") -> Tuple[bool, str]:
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        # å¤„ç† Tag ä¸­çš„éæ³•å­—ç¬¦
        safe_tag = "".join([c for c in tag if c.isalnum() or c in ('_', '-')])
        tag_part = f"_{safe_tag}" if safe_tag else ""

        filename = f"{timestamp}{tag_part}.sql"
        filepath = self.backup_dir / filename

        cmd = ['mysqldump', '-h', self.host, '-P', str(self.port), '-u', self.user, f'-p{self.password}',
               '--result-file', str(filepath), self.db_name]
        try:
            res = subprocess.run(cmd, capture_output=True, text=True)
            if res.returncode == 0:
                size_kb = os.path.getsize(filepath) / 1024
                return True, f"å¤‡ä»½æˆåŠŸ: {filename} ({size_kb:.1f} KB)"
            if filepath.exists(): os.remove(filepath)
            return False, f"å¤‡ä»½å¤±è´¥: {res.stderr}"
        except Exception as e:
            return False, str(e)

    def restore_backup_with_progress(self, filename: str, callback=None) -> Tuple[bool, str]:
        filepath = self.backup_dir / filename
        if not filepath.exists(): return False, "æ–‡ä»¶ä¸å­˜åœ¨"

        cmd = ['mysql', '-h', self.host, '-P', str(self.port), '-u', self.user, f'-p{self.password}', self.db_name]
        try:
            file_size = os.path.getsize(filepath)
            bytes_read = 0
            proc = subprocess.Popen(cmd, stdin=subprocess.PIPE, stderr=subprocess.PIPE)

            with open(filepath, 'rb') as f:
                while True:
                    chunk = f.read(1024 * 1024)
                    if not chunk: break
                    proc.stdin.write(chunk)
                    proc.stdin.flush()
                    bytes_read += len(chunk)
                    if callback and file_size > 0: callback(bytes_read / file_size)

            proc.stdin.close()
            proc.wait()

            if proc.returncode == 0:
                if callback: callback(1.0)
                return True, "è¿˜åŸæˆåŠŸ"
            return False, proc.stderr.read().decode()
        except Exception as e:
            return False, str(e)

    def delete_backup(self, filename: str) -> Tuple[bool, str]:
        try:
            p = self.backup_dir / filename
            if p.exists(): os.remove(p); return True, "å·²åˆ é™¤"
            return False, "æ–‡ä»¶ä¸å­˜åœ¨"
        except Exception as e:
            return False, str(e)

    # --- Data Deletion Logic (Masked Report) ---
    def delete_business_data_by_range(self, start_date: datetime.date, end_date: datetime.date, reason: str) -> Tuple[
        bool, str]:
        user = get_current_user()
        start_str = start_date.strftime("%Y-%m-%d")
        end_str = end_date.strftime("%Y-%m-%d")

        audit_msg = f"{IMMUTABLE_MARKER} æ•°æ®æ¸…ç†è¯·æ±‚ | Range: {start_str} to {end_str} | Reason: {reason}"
        self.audit_logger.critical(audit_msg, extra={"user": user, "action": "DATA_DELETION", "table": "ALL"})

        TABLE_ALIAS = {
            "Data_Transaction": "åŸå§‹æ•°æ®æº",
            "Data_Transaction_Done": "åŸå§‹æ•°æ®æº",
            "Data_Clean_Log": "å®Œæˆæ¸…æ´—æ•°æ®æº",
            "Data_Inventory": "åº“å­˜æ•°æ®æº"
        }

        log_lines = []
        try:
            with self.db.atomic_transaction() as conn:
                target_tables = ["Data_Transaction", "Data_Clean_Log"]
                for tbl in target_tables:
                    display_name = TABLE_ALIAS.get(tbl, tbl)
                    exists = conn.execute(text(f"SHOW TABLES LIKE '{tbl}'")).first()
                    if not exists: continue

                    cols = [r[0] for r in conn.execute(text(f"SHOW COLUMNS FROM `{tbl}`")).fetchall()]
                    date_col = 'order date' if 'order date' in cols else cols[0]

                    del_sql = text(f"DELETE FROM `{tbl}` WHERE `{date_col}` >= :s AND `{date_col}` <= :e")
                    res = conn.execute(del_sql, {"s": start_str, "e": end_str})
                    log_lines.append(f" {display_name}: åˆ é™¤äº† {res.rowcount} è¡Œã€‚")

                inv_tbl = "Data_Inventory"
                inv_display = TABLE_ALIAS.get(inv_tbl, inv_tbl)
                exists_inv = conn.execute(text(f"SHOW TABLES LIKE '{inv_tbl}'")).first()
                if exists_inv:
                    inv_cols = [r[0] for r in conn.execute(text(f"SHOW COLUMNS FROM `{inv_tbl}`")).fetchall()]
                    cols_to_drop = []
                    for col in inv_cols:
                        if len(col) == 10 and col[4] == '-' and col[7] == '-':
                            if start_str <= col <= end_str: cols_to_drop.append(col)

                    if cols_to_drop:
                        drop_parts = [f"DROP COLUMN `{c}`" for c in cols_to_drop]
                        alter_sql = f"ALTER TABLE `{inv_tbl}` " + ", ".join(drop_parts)
                        conn.execute(text(alter_sql))
                        log_lines.append(f" {inv_display}: åˆ é™¤äº† {len(cols_to_drop)} ä¸ªæ—¥æœŸåˆ—ã€‚")
                    else:
                        log_lines.append(f"ï¸ {inv_display}: æœªå‘ç°èŒƒå›´å†…çš„æ•°æ®åˆ—ã€‚")

            final_msg = "\n".join(log_lines)
            self.logger.warning(f"æ•°æ®æ¸…ç†å®Œæˆ:\n{final_msg}")
            self.audit_logger.critical(f"{IMMUTABLE_MARKER} æ¸…ç†ç»“æœ: {log_lines}",
                                       extra={"user": user, "action": "DATA_DELETION_RESULT"})

            return True, final_msg
        except Exception as e:
            self.logger.error(f"æ•°æ®æ¸…ç†å¤±è´¥: {e}")
            return False, str(e)
==================== END FILE: core/services/database_service.py ====================


==================== START FILE: core/services/ordering.py ====================
# core/services/ordering.py

import os
import numpy as np
import pandas as pd
from math import ceil, floor, sqrt
from typing import Tuple
from tqdm import tqdm

from config.settings import settings
from core.services.finance.base import ProfitAnalyzerBase
from core.repository.sku_repo import SkuRepository


class OrderingService(ProfitAnalyzerBase):
    Z_SCORES = {0.98: 2.05, 0.95: 1.65, 0.90: 1.28}

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.sku_repo = SkuRepository()
        self.lead_time = float(settings.LEAD_MONTH)
        self.min_safety = float(settings.MIN_SAFETY_MONTH)

    def _load_data_sources(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        self.log("ğŸ“¥ [Ordering] æ­£åœ¨åŠ è½½æ•°æ®æº...")

        # 1. Prediction
        user = self.etl_repo.get_transactions_by_date  # Mock call to ensure context? No, use file path
        # è¿™é‡Œçš„ output_dir å·²ç»åœ¨åŸºç±»åˆå§‹åŒ–æ—¶ç¡®å®šäº†
        pred_path = os.path.join(self.output_dir, "Estimated_Monthly_SKU.csv")

        df_pred = pd.DataFrame()
        if os.path.exists(pred_path):
            try:
                df_pred = pd.read_csv(pred_path)
                if "SKU" in df_pred.columns:
                    mask = ~df_pred["SKU"].astype(str).str.contains("è¯´æ˜|ç”Ÿæˆ|:", regex=True, na=False)
                    df_pred = df_pred[mask]
                    df_pred["SKU"] = df_pred["SKU"].astype(str).str.strip().str.upper()
                    df_pred.rename(columns={"BestForecast": "é¢„æµ‹æœˆæ¶ˆè€—"}, inplace=True)
            except Exception as e:
                self.log(f" è¯»å–é¢„æµ‹å¤±è´¥: {e}")

        # 2. Inventory
        df_inv = self.sku_repo.get_inventory_latest()
        if not df_inv.empty:
            df_inv["SKU"] = df_inv["SKU"].astype(str).str.strip().str.upper()
            df_inv["Quantity"] = pd.to_numeric(df_inv["Quantity"], errors='coerce').fillna(0)
            df_inv = df_inv.groupby("SKU", as_index=False)["Quantity"].sum()

        # 3. COGS
        df_cogs = self.sku_repo.get_all_cogs()[["SKU", "Cog"]]
        if not df_cogs.empty:
            df_cogs["SKU"] = df_cogs["SKU"].astype(str).str.strip().str.upper()
            df_cogs["Cog"] = pd.to_numeric(df_cogs["Cog"], errors='coerce').fillna(0)
            df_cogs = df_cogs.groupby("SKU", as_index=False)["Cog"].max()

        return df_pred, df_inv, df_cogs

    def _calc_abc_classification(self, df: pd.DataFrame) -> pd.DataFrame:
        df["é¢„æµ‹æœˆæ¶ˆè€—"] = pd.to_numeric(df["é¢„æµ‹æœˆæ¶ˆè€—"], errors='coerce').fillna(0)
        df["Cog"] = pd.to_numeric(df["Cog"], errors='coerce').fillna(0)
        df["é¢„ä¼°é”€å”®é¢"] = df["é¢„æµ‹æœˆæ¶ˆè€—"] * df["Cog"]
        df = df.sort_values("é¢„ä¼°é”€å”®é¢", ascending=False).reset_index(drop=True)

        total_val = df["é¢„ä¼°é”€å”®é¢"].sum()
        if total_val <= 0:
            df["ABCç­‰çº§"] = "C";
            df["ç›®æ ‡æœåŠ¡æ°´å¹³"] = 0.90;
            return df

        df["ç´¯è®¡å æ¯”"] = df["é¢„ä¼°é”€å”®é¢"].cumsum() / total_val
        conditions = [(df["ç´¯è®¡å æ¯”"] <= 0.80), (df["ç´¯è®¡å æ¯”"] <= 0.95)]
        df["ABCç­‰çº§"] = np.select(conditions, ["A", "B"], default="C")
        df["ç›®æ ‡æœåŠ¡æ°´å¹³"] = np.select(conditions, [0.98, 0.95], default=0.90)
        return df

    def _calculate_logic_row(self, row: pd.Series) -> pd.Series:
        forecast = float(row["é¢„æµ‹æœˆæ¶ˆè€—"])
        current_inv = float(row["Quantity"])
        sl = float(row["ç›®æ ‡æœåŠ¡æ°´å¹³"])

        volatility = forecast * 0.5
        z_score = self.Z_SCORES.get(sl, 1.28)
        ss_stat = z_score * sqrt(self.lead_time) * volatility
        ss_min = self.min_safety * forecast
        safety_stock = max(ss_stat, ss_min)

        target_stock = (self.lead_time * forecast) + safety_stock
        gap = target_stock - current_inv

        sku = str(row["SKU"])
        moq = 1000 if sku.startswith(('NE', 'NU', 'BE', 'BU')) else 100

        if gap <= 0:
            suggest_qty = 0; note = "åº“å­˜å……è¶³"
        elif (forecast * 6) < moq:
            suggest_qty = 0; note = f"é”€é‡è¿‡ä½ (< {moq})"
        else:
            factor = gap / moq
            remainder = factor - int(factor)
            rounds = ceil(factor) if remainder >= 0.33 else floor(factor)
            suggest_qty = rounds * moq
            note = "å»ºè®®è¡¥è´§" if suggest_qty > 0 else "ç¼ºå£å¾®å°"

        return pd.Series({
            "å®‰å…¨åº“å­˜": round(safety_stock, 1),
            "ç›®æ ‡åº“å­˜": round(target_stock, 1),
            "ç†è®ºåº“å­˜": round(current_inv, 1),
            "ç¼ºå£": round(gap, 1),
            "å»ºè®®è®¢è´§": int(suggest_qty),
            "å¤‡æ³¨": note
        })

    def run(self):
        self.log(f"ğŸš€ å¯åŠ¨æ™ºèƒ½è¡¥è´§è®¡ç®— (Lead={self.lead_time}, Safety={self.min_safety})...")

        df_pred, df_inv, df_cogs = self._load_data_sources()

        output_cols = [
            "SKU", "ABCç­‰çº§", "å»ºè®®è®¢è´§", "å¤‡æ³¨",
            "é¢„æµ‹æœˆæ¶ˆè€—", "ç›®æ ‡æœåŠ¡æ°´å¹³", "å®‰å…¨åº“å­˜",
            "ç›®æ ‡åº“å­˜", "ç†è®ºåº“å­˜", "ç¼ºå£", "Cog"
        ]

        # [Fix] å³ä½¿é¢„æµ‹ä¸ºç©ºï¼Œä¹Ÿç»§ç»­æ‰§è¡Œä»¥ç”Ÿæˆç©ºè¡¨
        if df_pred.empty:
            self.log("âš ï¸ é¢„æµ‹æ•°æ®ä¸ºç©ºï¼Œç”Ÿæˆç©ºè¡¥è´§è¡¨ã€‚")
            df_final = pd.DataFrame(columns=output_cols)
        else:
            self.log("ğŸ”— å…³è”åº“å­˜ä¸æˆæœ¬...")
            df_main = pd.merge(df_pred, df_inv, on="SKU", how="left")
            df_main = pd.merge(df_main, df_cogs, on="SKU", how="left")
            df_main.fillna(0, inplace=True)

            self.log("ğŸ“Š æ‰§è¡Œ ABC åˆ†çº§ä¸è¡¥è´§è®¡ç®—...")
            df_main = self._calc_abc_classification(df_main)

            tqdm.pandas(desc="Computing")
            logic_results = df_main.progress_apply(self._calculate_logic_row, axis=1)
            df_final = pd.concat([df_main, logic_results], axis=1)
            df_final = df_final.sort_values(["ABCç­‰çº§", "å»ºè®®è®¢è´§"], ascending=[True, False])

        # è¡¥å…¨åˆ—
        for c in output_cols:
            if c not in df_final.columns: df_final[c] = 0

        filename = f"Smart_Ordering_Plan_{self.file_suffix}.csv"
        footer = [
            "ğŸ“˜ æ™ºèƒ½è¡¥è´§é€»è¾‘è¯´æ˜:",
            f"1. å‚æ•°: Lead={self.lead_time}, Safety={self.min_safety}",
            "2. å…¬å¼: Target = (Forecast * Lead) + SafetyStock"
        ]

        self.save_csv(df_final[output_cols], filename, footer)
==================== END FILE: core/services/ordering.py ====================


==================== START FILE: core/services/correction.py ====================
# core/services/correction.py
"""
æ–‡ä»¶è¯´æ˜: SKU çº é”™ä¸è®°å¿†æœåŠ¡ (Correction Service)
ä¸»è¦åŠŸèƒ½:
1. ç®¡ç† "SKU è‡ªåŠ¨ä¿®æ­£è®°å¿†åº“" (CSVæ–‡ä»¶)ï¼Œè®°å½•ç”¨æˆ·çš„ä¿®æ­£æ“ä½œ (Learn)ã€‚
2. æä¾›æ™ºèƒ½æ¨è (Fuzzy Match) å’Œè‡ªåŠ¨åŒ¹é…å»ºè®® (Suggest)ã€‚
3. æä¾›åŸå­æ€§çš„ä¿®å¤æ¥å£ï¼ŒåŒæ—¶æ›´æ–°æ•°æ®åº“å’Œè®°å¿†åº“ã€‚
"""

import pandas as pd
import difflib
from typing import List, Optional, Tuple, Dict, Any

from config.settings import settings
from core.components.db.client import DBClient
from core.sys.logger import get_logger
from core.sys.context import get_current_user
# å¼•å…¥åº“å­˜ä»“åº“
from core.services.inventory.repository import InventoryRepository


class CorrectionService:

    def __init__(self):
        self.logger = get_logger("CorrectionService")
        self.db = DBClient()

        # è®°å¿†æ–‡ä»¶è·¯å¾„
        self.memory_file = settings.KNOWLEDGE_BASE_DIR / "sku_correction_memory.csv"

        # [Fix] å®ä¾‹åŒ– InventoryRepositoryï¼Œè€Œä¸æ˜¯ç›´æ¥è°ƒç”¨ç±»æ–¹æ³•
        self.inv_repo = InventoryRepository()
        self.valid_skus = set(self.inv_repo.get_valid_skus())

        # åŠ è½½è®°å¿†åº“
        self.memory_df = self._load_memory()

    def _load_memory(self) -> pd.DataFrame:
        """åŠ è½½è®°å¿†åº“ CSV"""
        if self.memory_file.exists():
            try:
                # å…¨éƒ¨æŒ‰å­—ç¬¦ä¸²è¯»å–ï¼Œé˜²æ­¢ '001' å˜æˆ 1
                return pd.read_csv(self.memory_file, dtype=str).fillna("")
            except Exception:
                return pd.DataFrame(columns=["CustomLabel", "BadSKU", "BadQty", "CorrectSKU", "CorrectQty"])
        else:
            return pd.DataFrame(columns=["CustomLabel", "BadSKU", "BadQty", "CorrectSKU", "CorrectQty"])

    def save_correction_memory(self, custom_label: str, bad_sku: str, bad_qty: str,
                               correct_sku: str, correct_qty: str):
        """
        [å­¦ä¹ ] è®°å½•ç”¨æˆ·çš„ä¿®æ­£æ“ä½œ
        """
        new_row = {
            "CustomLabel": str(custom_label).strip(),
            "BadSKU": str(bad_sku).strip().upper(),
            "BadQty": str(bad_qty).strip(),
            "CorrectSKU": str(correct_sku).strip().upper(),
            "CorrectQty": str(correct_qty).strip()
        }

        # è¿½åŠ å¹¶å»é‡ (ä¿ç•™æœ€æ–°çš„ä¿®æ­£è®°å½•)
        self.memory_df = pd.concat([self.memory_df, pd.DataFrame([new_row])], ignore_index=True)
        self.memory_df.drop_duplicates(subset=["CustomLabel", "BadSKU"], keep='last', inplace=True)

        # æŒä¹…åŒ–
        try:
            self.memory_df.to_csv(self.memory_file, index=False, encoding='utf-8-sig')
        except Exception as e:
            self.logger.error(f"è®°å¿†åº“ä¿å­˜å¤±è´¥: {e}")

        # åŠ¨æ€æ›´æ–°å†…å­˜ä¸­çš„æœ‰æ•ˆ SKU åˆ—è¡¨ï¼Œæ— éœ€æŸ¥åº“
        if correct_sku not in self.valid_skus:
            self.valid_skus.add(correct_sku)

    def find_auto_fix(self, custom_label: str, bad_sku: str) -> Tuple[Optional[str], Optional[str]]:
        """
        [å›å¿†] ä»è®°å¿†åº“ä¸­å¯»æ‰¾å†å²è§£å†³æ–¹æ¡ˆ
        Returns: (CorrectSKU, CorrectQty)
        """
        if self.memory_df.empty: return None, None

        custom_label = str(custom_label).strip()
        bad_sku = str(bad_sku).strip().upper()

        # ç²¾ç¡®åŒ¹é… Label å’Œ BadSKU
        match = self.memory_df[
            (self.memory_df["CustomLabel"] == custom_label) &
            (self.memory_df["BadSKU"] == bad_sku)
            ]

        if not match.empty:
            row = match.iloc[-1]
            return row["CorrectSKU"], row["CorrectQty"]
        return None, None

    def get_fuzzy_suggestions(self, bad_sku: str, n: int = 5) -> List[str]:
        """
        [å»ºè®®] æ¨¡ç³Šæœç´¢æœ‰æ•ˆ SKU
        """
        bad_sku = str(bad_sku).upper()
        # 1. åŒ…å«åŒ¹é…
        contains = [s for s in self.valid_skus if bad_sku in s]
        # 2. æ¨¡ç³ŠåŒ¹é… (Levenshtein Distance)
        fuzzy = difflib.get_close_matches(bad_sku, self.valid_skus, n=n, cutoff=0.4)

        results = sorted(list(set(contains + fuzzy)))
        return results[:n]

    def is_valid_sku(self, sku: str) -> bool:
        """æ ¡éªŒ SKU æ˜¯å¦å­˜åœ¨äºç³»ç»Ÿæ¡£æ¡ˆä¸­"""
        return str(sku).strip().upper() in self.valid_skus

    def validate_quantity(self, val: str) -> bool:
        """[å·¥å…·] éªŒè¯æ•°é‡æ˜¯å¦ä¸ºæ­£æ•´æ•°"""
        try:
            v = float(val)
            return v > 0 and v.is_integer()
        except:
            return False

    def apply_fix_transactional(self, order_id: str, col_idx: int,
                                custom_label: str, bad_sku: str, bad_qty: str,
                                new_sku: str, new_qty: str) -> bool:
        """
        [åŸå­æ“ä½œ] åº”ç”¨äººå·¥ä¿®å¤ï¼šåŒæ—¶æ›´æ–° Data_Transaction å’Œ è®°å¿†åº“
        """
        user = get_current_user()

        # 1. ä¿å­˜è®°å¿†
        self.save_correction_memory(custom_label, bad_sku, bad_qty, new_sku, new_qty)

        # 2. æ›´æ–°æ•°æ®åº“ (Raw Table)
        try:
            sku_col = f"P_SKU{col_idx}"
            qty_col = f"P_Quantity{col_idx}"

            # ä»…æ›´æ–°å€¼ï¼ŒFlag äº¤ç”± Parser åœ¨ä¸‹ä¸€æ¬¡è¿è¡Œæ—¶è‡ªåŠ¨æ ¡éªŒæ›´æ–°
            sql = f"""
            UPDATE Data_Transaction 
            SET `{sku_col}` = :ns, `{qty_col}` = :nq
            WHERE `Order number` = :oid
            """
            DBClient.execute_stmt(sql, {"ns": new_sku, "nq": new_qty, "oid": order_id})

            self.logger.info(f"äººå·¥ä¿®å¤: Order[{order_id}] {bad_sku}->{new_sku}",
                             extra={"action": "MANUAL_FIX_SKU", "user": user})
            return True
        except Exception as e:
            self.logger.error(f"ä¿®å¤æäº¤å¤±è´¥: {e}")
            return False

    def get_next_pending_issue(self) -> Optional[pd.Series]:
        """è·å–ä¸‹ä¸€æ¡å¾…å¤„ç†å¼‚å¸¸ (P_Flag=99)"""
        try:
            # P_Flag=99 æ˜¯ Parser æ ‡è®°çš„â€œæ ¡éªŒå¤±è´¥â€çŠ¶æ€
            sql = "SELECT * FROM Data_Transaction WHERE P_Flag = 99 LIMIT 1"
            df = DBClient.read_df(sql)
            if df.empty: return None
            return df.iloc[0]
        except Exception:
            return None

    def mark_as_skipped(self, order_id: str) -> None:
        """è·³è¿‡è®°å½• (å¼ºåˆ¶æ ‡è®°ä¸ºé€šè¿‡ P_Flag=5)"""
        user = get_current_user()
        sql = "UPDATE Data_Transaction SET P_Flag = 5 WHERE `Order number` = :oid"
        DBClient.execute_stmt(sql, {"oid": order_id})
        self.logger.warning(f"è·³è¿‡å¼‚å¸¸è®°å½•: Order[{order_id}]", extra={"action": "SKIP_FIX", "user": user})

    def run_auto_parser(self) -> Dict[str, Any]:
        """
        [ç¼–æ’] è§¦å‘ ETL Parser è¿›è¡Œé‡æ‰«
        (ä¸»è¦ç”¨äº UI ä¸Šçš„ 'é‡æ–°æ‰«æ' æŒ‰é’®)
        """
        # å±€éƒ¨å¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
        from core.services.etl.parser import TransactionParser
        self.logger.info("å¯åŠ¨è‡ªåŠ¨è§£æä¸ä¿®å¤æµç¨‹...")
        parser = TransactionParser()
        return parser.run()
==================== END FILE: core/services/correction.py ====================


==================== START FILE: core/services/data_manager.py ====================
# core/services/data_manager.py
"""
æ–‡ä»¶è¯´æ˜: æ•°æ®ä¿®æ”¹ä¸­å¿ƒä¸šåŠ¡æœåŠ¡ (Data Manager Service) - V5.0 Smart Update
ä¸»è¦åŠŸèƒ½:
1. åº“å­˜ä¿®æ”¹: è¯»å–/å•ç‚¹ä¿®æ”¹/æ•´åˆ—åˆ é™¤ (å«å®¡è®¡)ã€‚
2. [Mod] æ¡£æ¡ˆç²¾å‡†ç»´æŠ¤: ä½¿ç”¨ DiffEngine è¯†åˆ«å˜æ›´ï¼Œä»…æ›´æ–°å—å½±å“è¡Œï¼Œè®°å½•ç²¾å‡†æ—¥å¿—ã€‚
3. [New] SKU æ–°å¢: åŸå­åŒ–åˆ›å»º SKU æ¡£æ¡ˆåŠåº“å­˜è®°å½•ã€‚
"""

import pandas as pd
from typing import List, Tuple, Any, Dict
from sqlalchemy import text

from config.settings import settings
from core.components.db.client import DBClient
from core.sys.logger import get_logger, get_audit_logger
from core.sys.context import get_current_user
from core.components.utils.diff_engine import DiffEngine  # [New] å¼•å…¥å·®å¼‚å¼•æ“

IMMUTABLE_MARKER = "[[SECURITY_AUDIT]]"


class DataManager:

    def __init__(self):
        self.db = DBClient()
        self.logger = get_logger("DataManager")
        self.audit_logger = get_audit_logger()
        self.table_inv = "Data_Inventory"
        self.table_cogs = "Data_COGS"

    # =========================================================================
    # åº“å­˜ç®¡ç† (Inventory) - ä¿æŒä¸å˜
    # =========================================================================

    def get_inventory_columns(self) -> List[str]:
        try:
            df = self.db.read_df(f"SELECT * FROM `{self.table_inv}` LIMIT 0")
            cols = [c for c in df.columns if c.lower() not in ['id', 'sku', 'created_at', 'updated_at']]
            return sorted(cols, reverse=True)
        except Exception as e:
            self.logger.error(f"è·å–åº“å­˜åˆ—å¤±è´¥: {e}")
            return []

    def get_all_skus(self) -> List[str]:
        try:
            df = self.db.read_df(f"SELECT DISTINCT SKU FROM `{self.table_inv}` ORDER BY SKU")
            return df["SKU"].dropna().astype(str).tolist()
        except Exception:
            return []

    def get_inventory_value(self, date_col: str, sku: str) -> int:
        try:
            sql = f"SELECT `{date_col}` FROM `{self.table_inv}` WHERE SKU = :sku"
            df = self.db.read_df(sql, {"sku": sku})
            if df.empty: return 0
            val = df.iloc[0, 0]
            return int(float(val)) if pd.notna(val) else 0
        except Exception:
            return 0

    def update_inventory_qty(self, date_col: str, sku: str, new_qty: int) -> Tuple[bool, str]:
        user = get_current_user()
        old_qty = self.get_inventory_value(date_col, sku)
        if old_qty == new_qty: return True, "æ•°å€¼æœªå˜åŒ–ã€‚"

        try:
            sql = f"UPDATE `{self.table_inv}` SET `{date_col}` = :qty WHERE SKU = :sku"
            success = self.db.execute_stmt(sql, {"qty": new_qty, "sku": sku})
            if success:
                msg = f"åº“å­˜ä¿®æ­£: {sku} [{date_col}] | {old_qty} -> {new_qty}"
                self.logger.warning(msg, extra={"action": "UPDATE_INVENTORY", "user": user})
                return True, f" æ›´æ–°æˆåŠŸ: {msg}"
            return False, "æ•°æ®åº“å†™å…¥å¤±è´¥"
        except Exception as e:
            self.logger.error(f"åº“å­˜æ›´æ–°å¼‚å¸¸: {e}")
            return False, str(e)

    def drop_inventory_column(self, date_col: str, reason: str) -> Tuple[bool, str]:
        user = get_current_user()
        if date_col.lower() in ['id', 'sku', 'created_at', 'updated_at']:
            return False, "ç³»ç»Ÿæ ¸å¿ƒåˆ—ä¸å¯åˆ é™¤"
        try:
            sql = f"ALTER TABLE `{self.table_inv}` DROP COLUMN `{date_col}`"
            success = self.db.execute_stmt(sql)
            if success:
                msg = f"{IMMUTABLE_MARKER} [DROP COLUMN] Target: {self.table_inv}.{date_col} | Reason: {reason}"
                self.audit_logger.critical(msg,
                                           extra={"action": "DROP_INV_COLUMN", "user": user, "table": self.table_inv})
                return True, f" å·²æˆåŠŸåˆ é™¤åˆ—: {date_col}"
            return False, "åˆ é™¤åˆ—å¤±è´¥"
        except Exception as e:
            self.logger.error(f"åˆ é™¤åˆ—å¼‚å¸¸: {e}")
            return False, f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"

    # =========================================================================
    # æ¡£æ¡ˆç»´æŠ¤ (COGS) - å‡çº§ç‰ˆ
    # =========================================================================

    def get_cogs_data(self) -> pd.DataFrame:
        """è·å–å…¨é‡ SKU æ¡£æ¡ˆæ•°æ®"""
        return self.db.read_df(f"SELECT * FROM `{self.table_cogs}`")

    def get_distinct_options(self, column: str) -> List[str]:
        """è·å–é€‰é¡¹"""
        allowed = ['Category', 'SubCategory', 'Type']
        if column not in allowed: return []
        try:
            sql = f"SELECT DISTINCT `{column}` FROM `{self.table_cogs}` WHERE `{column}` IS NOT NULL AND `{column}` != '' ORDER BY `{column}`"
            df = self.db.read_df(sql)
            return df[column].tolist()
        except:
            return []

    def update_cogs_smart(self, df_new: pd.DataFrame) -> Tuple[bool, str]:
        """
        [ç²¾å‡†æ›´æ–°] ä»…æ›´æ–°å‘ç”Ÿå˜åŒ–çš„è¡Œ
        """
        if df_new.empty: return False, "æ•°æ®ä¸ºç©º"

        user = get_current_user()

        # 1. è·å– DB ç°çŠ¶
        df_old = self.get_cogs_data()

        # 2. è®¡ç®—å·®å¼‚
        diff = DiffEngine.compute_diff(df_old, df_new, key_col="SKU")
        modifications = diff["modified"]

        if not modifications:
            return True, "æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•å˜æ›´ï¼Œæ— éœ€æ›´æ–°ã€‚"

        try:
            updated_count = 0
            # 3. é€è¡Œç”Ÿæˆ SQL å¹¶æ‰§è¡Œ (äº‹åŠ¡åŒ…è£¹)
            with self.db.atomic_transaction() as conn:
                for item in modifications:
                    sku = item["key"]
                    changes = item["changes"]  # dict {col: (old, new)}

                    # æ„é€  UPDATE è¯­å¥
                    set_clauses = []
                    params = {"sku": sku}

                    for col, (old_val, new_val) in changes.items():
                        # å®‰å…¨å¤„ç†åˆ—å
                        safe_col = f"`{col}`"
                        param_key = f"val_{col}"
                        set_clauses.append(f"{safe_col} = :{param_key}")
                        params[param_key] = new_val

                    if set_clauses:
                        sql = text(f"UPDATE `{self.table_cogs}` SET {', '.join(set_clauses)} WHERE SKU = :sku")
                        conn.execute(sql, params)
                        updated_count += 1

            # 4. ç”Ÿæˆç²¾å‡†æ—¥å¿—
            log_msgs = DiffEngine.format_log_message(diff, key_name="SKU")
            # ä¸ºé¿å…æ—¥å¿—åˆ·å±ï¼Œå¦‚æœæ¡æ•°å¤ªå¤šï¼Œåªè®°å½•æ‘˜è¦
            if len(log_msgs) > 10:
                log_summary = f"æ‰¹é‡æ›´æ–° {len(log_msgs)} æ¡ SKU èµ„æ–™ (è¯¦æƒ…è§ audit.log)"
                self.logger.warning(log_summary, extra={"action": "BATCH_UPDATE_COGS", "user": user})
                # å®Œæ•´æ—¥å¿—å»ºè®®å†™å…¥ audit æˆ– fileï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œæ‰“å°å‰5æ¡
                for msg in log_msgs[:5]:
                    self.logger.info(msg, extra={"action": "UPDATE_DETAIL", "user": user})
            else:
                for msg in log_msgs:
                    self.logger.warning(msg, extra={"action": "BATCH_UPDATE_COGS", "user": user})

            return True, f" æˆåŠŸæ›´æ–°äº† {updated_count} æ¡è®°å½•ã€‚"

        except Exception as e:
            self.logger.error(f"ç²¾å‡†æ›´æ–°å¤±è´¥: {e}")
            return False, f"æ›´æ–°å¤±è´¥: {e}"

    def batch_create_skus(self, sku_list: List[Dict[str, Any]]) -> Tuple[bool, str]:
        """
        [æ–°å¢ SKU] æ‰¹é‡åˆ›å»º SKU
        :param sku_list: [{SKU, Category, ..., Initial_Qty}, ...]
        """
        if not sku_list: return False, "æ²¡æœ‰æœ‰æ•ˆçš„ SKU æ•°æ®"

        user = get_current_user()

        # 1. è·å–åº“å­˜è¡¨æœ€æ–°æœˆä»½åˆ— (ç”¨äºå¡«å…¥ Initial_Qty)
        date_cols = self.get_inventory_columns()
        latest_date_col = date_cols[0] if date_cols else None

        # 2. è·å–åº“å­˜è¡¨æ‰€æœ‰åˆ— (ç”¨äºè¡¥ 0)
        # è¿™é‡Œéœ€è¦è¯» schema
        try:
            inv_schema = self.db.read_df(f"SELECT * FROM `{self.table_inv}` LIMIT 0")
            inv_all_cols = inv_schema.columns.tolist()
        except:
            return False, "æ— æ³•è¯»å–åº“å­˜è¡¨ç»“æ„"

        success_count = 0
        try:
            with self.db.atomic_transaction() as conn:
                for row in sku_list:
                    sku = str(row['SKU']).strip().upper()

                    # A. æ’å…¥ Data_COGS
                    # å‰”é™¤ Initial_Qty å­—æ®µï¼Œå› ä¸ºå®ƒä¸å±äº COGS è¡¨
                    cogs_data = {k: v for k, v in row.items() if k != 'Initial_Qty'}
                    cogs_data['SKU'] = sku

                    # ç®€å•æŸ¥é‡ (è™½ç„¶ DB æœ‰ Unique çº¦æŸï¼Œä½†ä¸ºäº†å‹å¥½æç¤º)
                    # åœ¨äº‹åŠ¡å†…æŸ¥é‡æ¯”è¾ƒå¤æ‚ï¼Œç›´æ¥ try-catch IntegrityError æ›´ç¨³

                    cols = ", ".join([f"`{k}`" for k in cogs_data.keys()])
                    vals = ", ".join([f":{k}" for k in cogs_data.keys()])
                    sql_cogs = text(f"INSERT INTO `{self.table_cogs}` ({cols}) VALUES ({vals})")
                    conn.execute(sql_cogs, cogs_data)

                    # B. æ’å…¥ Data_Inventory
                    # æ„é€ æ’å…¥å­—å…¸
                    inv_data = {}
                    initial_qty = int(row.get('Initial_Qty', 0))

                    for col in inv_all_cols:
                        c_lower = col.lower()
                        if c_lower == 'sku':
                            inv_data[col] = sku
                        elif c_lower == 'id':
                            continue  # Auto Inc
                        elif c_lower in ['created_at', 'updated_at']:
                            continue  # Auto
                        elif latest_date_col and col == latest_date_col:
                            # æœ€æ–°æœˆä»½å¡«åˆå§‹å€¼
                            inv_data[col] = initial_qty
                        else:
                            # å†å²æœˆä»½å¡« 0
                            inv_data[col] = 0

                    i_cols = ", ".join([f"`{k}`" for k in inv_data.keys()])
                    i_vals = ", ".join([f":val_{i}" for i in range(len(inv_data))])
                    # å‚æ•°åŒ– key éœ€è¦å”¯ä¸€
                    i_params = {f"val_{i}": v for i, v in enumerate(inv_data.values())}

                    sql_inv = text(f"INSERT INTO `{self.table_inv}` ({i_cols}) VALUES ({i_vals})")
                    conn.execute(sql_inv, i_params)

                    success_count += 1

            # å®¡è®¡æ—¥å¿—
            self.logger.warning(f"æ‰¹é‡æ–°å¢ SKU | Count: {success_count}",
                                extra={"action": "CREATE_SKU", "user": user})
            return True, f" æˆåŠŸåˆ›å»º {success_count} ä¸ª SKUã€‚"

        except Exception as e:
            if "Duplicate entry" in str(e):
                return False, f"éƒ¨åˆ† SKU å·²å­˜åœ¨ï¼Œåˆ›å»ºå¤±è´¥: {e}"
            self.logger.error(f"åˆ›å»º SKU å¤±è´¥: {e}")
            return False, f"åˆ›å»ºå¤±è´¥: {e}"
==================== END FILE: core/services/data_manager.py ====================


==================== START FILE: core/services/inventory_snapshot.py ====================
# core/services/inventory_snapshot.py
import pandas as pd
from core.services.finance.base import ProfitAnalyzerBase
from core.services.inventory.repository import InventoryRepository


class InventorySnapshot(ProfitAnalyzerBase):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.repo = InventoryRepository()

    def run(self):
        self.log("ğŸ“¸ å¼€å§‹æ‰§è¡Œåº“å­˜èµ„äº§å¿«ç…§åˆ†æ...")

        df_inv = self.repo.get_inventory_latest()

        # [Fix] å³ä½¿ä¸ºç©ºä¹Ÿç»§ç»­æ‰§è¡Œï¼Œä»¥ç”Ÿæˆç©ºè¡¨
        if df_inv.empty:
            self.log("âš ï¸ æ— åº“å­˜æ•°æ®ï¼Œç”Ÿæˆç©ºå¿«ç…§è¡¨ã€‚")
            df_inv = pd.DataFrame(columns=["SKU", "Quantity"])

        df_cogs = self.repo.get_all_cogs()

        df_inv['SKU'] = df_inv['SKU'].astype(str).str.strip().str.upper()
        df_inv['Quantity'] = pd.to_numeric(df_inv['Quantity'], errors='coerce').fillna(0)

        if not df_cogs.empty:
            df_cogs['SKU'] = df_cogs['SKU'].astype(str).str.strip().str.upper()
            df_cogs['Cog'] = pd.to_numeric(df_cogs['Cog'], errors='coerce').fillna(0.0)
        else:
            df_cogs = pd.DataFrame(columns=["SKU", "Cog", "Category"])

        df_merge = pd.merge(df_inv, df_cogs[['SKU', 'Cog', 'Category']], on='SKU', how='left')
        df_merge['Cog'] = df_merge['Cog'].fillna(0.0)
        df_merge['Asset_Value'] = df_merge['Quantity'] * df_merge['Cog']
        df_merge = df_merge.sort_values('Asset_Value', ascending=False)

        total_qty = df_merge['Quantity'].sum()
        total_val = df_merge['Asset_Value'].sum()

        filename = f"Inventory_Asset_Snapshot_{self.file_suffix}.csv"
        footer = [
            "ğŸ“˜ åº“å­˜èµ„äº§è¯´æ˜:",
            f"1. æ€»åº“å­˜æ•°é‡: {int(total_qty):,}",
            f"2. æ€»èµ„äº§ä»·å€¼: ${total_val:,.2f}",
            "3. ä»·å€¼å…¬å¼: Quantity * Cog (Cost + Freight)"
        ]

        df_out = df_merge[['SKU', 'Category', 'Quantity', 'Cog', 'Asset_Value']].copy()
        if not df_out.empty:
            df_out['Cog'] = df_out['Cog'].apply(lambda x: f"{x:.2f}")
            df_out['Asset_Value'] = df_out['Asset_Value'].apply(lambda x: f"{x:.2f}")

        self.save_csv(df_out, filename, footer)
        self.log(f" åº“å­˜å¿«ç…§å·²ç”Ÿæˆ (Total Value: ${total_val:,.2f})")
==================== END FILE: core/services/inventory_snapshot.py ====================


==================== START FILE: core/services/security/policy_manager.py ====================
# core/services/security/policy_manager.py
"""
æ–‡ä»¶è¯´æ˜: å®‰å…¨ç­–ç•¥ç®¡ç†å™¨ (Security Policy Manager) - V4.0 Four Codes System
ä¸»è¦åŠŸèƒ½:
1. è¯»å– action_registry.json (é»˜è®¤ç­–ç•¥) ä¸ security_overrides.json (è¦†ç›–ç­–ç•¥)ã€‚
2. [æ ¸å¿ƒ] ç»Ÿä¸€éªŒè¯æ¥å£: å¯¹æ¥ 4 ç±»å®‰ä¿ç  (Query/Modify/DB/System) + ç”¨æˆ·å¯†ç ã€‚
3. ç®¡ç†å‘˜èŒèƒ½å¼€å…³ç®¡ç†ã€‚
"""

import json
import os
from pathlib import Path
from typing import List, Dict, Any, Optional

from config.settings import settings
from core.services.auth.service import AuthService
from core.sys.logger import get_logger

logger = get_logger("SecurityPolicy")


class SecurityPolicyManager:
    _registry_cache: Dict[str, Any] = {}
    _overrides_cache: Dict[str, List[str]] = {}
    _capabilities_cache: Dict[str, bool] = {}
    _is_loaded = False

    OVERRIDES_FILE = settings.DATA_DIR / "security_overrides.json"
    CAPABILITIES_FILE = settings.DATA_DIR / "admin_capabilities.json"

    @classmethod
    def load_registry(cls):
        if cls._is_loaded: return

        # 1. Load Registry
        data = settings.load_action_registry()
        flat_map = {}
        if "modules" in data:
            for mod in data["modules"]:
                for tab in mod.get("tabs", []):
                    for act in tab.get("actions", []):
                        key = act.get("key")
                        if key: flat_map[key] = act
        cls._registry_cache = flat_map

        # 2. Load Overrides
        if cls.OVERRIDES_FILE.exists():
            try:
                with open(cls.OVERRIDES_FILE, "r", encoding="utf-8") as f:
                    cls._overrides_cache = json.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½ç­–ç•¥è¦†ç›–å¤±è´¥: {e}")
                cls._overrides_cache = {}

        # 3. Load Capabilities
        cls._capabilities_cache = {
            "can_create_user": True,
            "can_lock_user": True,
            "can_reset_pwd": True,
            "can_manage_perms": True
        }
        if cls.CAPABILITIES_FILE.exists():
            try:
                with open(cls.CAPABILITIES_FILE, "r", encoding="utf-8") as f:
                    saved = json.load(f)
                    cls._capabilities_cache.update(saved)
            except:
                pass

        cls._is_loaded = True
        logger.info(f"å®‰å…¨ç­–ç•¥åŠ è½½å®Œæ¯•: 4ç ä½“ç³»ç”Ÿæ•ˆ")

    # -------------------------------------------------------------------------
    # ç­–ç•¥ä¸éªŒè¯
    # -------------------------------------------------------------------------
    @classmethod
    def get_required_tokens(cls, action_key: str) -> List[str]:
        cls.load_registry()

        # ä¼˜å…ˆè¯»å–è¦†ç›–é…ç½®
        if action_key in cls._overrides_cache:
            return cls._overrides_cache[action_key]

        config = cls._registry_cache.get(action_key)
        if not config: return ["user"]  # é»˜è®¤å…œåº•

        # ç›´æ¥è¿”å› JSON ä¸­å®šä¹‰çš„ token list (user, query, modify, db, system)
        return config.get("default_security", ["user"])

    @classmethod
    def save_policy_override(cls, action_key: str, tokens: List[str]) -> bool:
        try:
            cls.load_registry()
            cls._overrides_cache[action_key] = tokens
            with open(cls.OVERRIDES_FILE, "w", encoding="utf-8") as f:
                json.dump(cls._overrides_cache, f, indent=2, ensure_ascii=False)
            logger.warning(f"å®‰å…¨ç­–ç•¥æ›´æ–°: {action_key} -> {tokens}")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜ç­–ç•¥å¤±è´¥: {e}")
            return False

    @classmethod
    def validate_token(cls, token_type: str, input_value: str, username: str = None) -> bool:
        """
        [æ ¸å¿ƒéªŒè¯é€»è¾‘]
        å¯¹æ¥ .env ä¸­çš„ç¯å¢ƒå˜é‡é…ç½®
        """
        if not input_value: return False

        # 1. èº«ä»½éªŒè¯ (Current User)
        if token_type == "user":
            if not username: return False
            ok, _, _ = AuthService.authenticate(username, input_value)
            return ok

        # 2. å››çº§å®‰ä¿ç éªŒè¯ (Authorization Codes)
        # Note: å¯†ç ç›´æ¥ä» settings (å³ .env) è¯»å–ï¼Œä¸åœ¨ä»£ç ä¸­ç¡¬ç¼–ç 

        if token_type == "query":
            return input_value == settings.SEC_CODE_QUERY

        if token_type == "modify":
            return input_value == settings.SEC_CODE_MODIFY

        if token_type == "db":
            return input_value == settings.SEC_CODE_DB

        if token_type == "system":
            return input_value == settings.SEC_CODE_SYSTEM

        logger.warning(f"æœªçŸ¥çš„ä»¤ç‰Œç±»å‹: {token_type}")
        return False

    @classmethod
    def get_action_metadata(cls, action_key: str) -> Dict[str, str]:
        cls.load_registry()
        config = cls._registry_cache.get(action_key, {})
        return {
            "name": config.get("name", action_key),
            "description": config.get("description", "")
        }

    # -------------------------------------------------------------------------
    # èŒèƒ½ç®¡ç†
    # -------------------------------------------------------------------------
    @classmethod
    def get_admin_capability(cls, cap_key: str) -> bool:
        cls.load_registry()
        return cls._capabilities_cache.get(cap_key, True)

    @classmethod
    def update_admin_capability(cls, cap_key: str, enabled: bool) -> bool:
        try:
            cls.load_registry()
            cls._capabilities_cache[cap_key] = enabled
            with open(cls.CAPABILITIES_FILE, "w", encoding="utf-8") as f:
                json.dump(cls._capabilities_cache, f, indent=2, ensure_ascii=False)
            return True
        except:
            return False
==================== END FILE: core/services/security/policy_manager.py ====================


==================== START FILE: core/services/security/__init__.py ====================
# core/services/security/__init__.py
"""
æ–‡ä»¶è¯´æ˜: å®‰å…¨æœåŠ¡åŒ…åˆå§‹åŒ–
ä¸»è¦åŠŸèƒ½:
1. åŒ…å«å®‰å…¨ç­–ç•¥ç®¡ç† (Policy Manager)ã€‚
2. æœªæ¥å¯æ‰©å±•åŠ å¯†æœåŠ¡ã€å®¡è®¡æœåŠ¡ç­‰ã€‚
"""
==================== END FILE: core/services/security/__init__.py ====================


==================== START FILE: core/services/security/inventory.py ====================
# core/services/security/inventory.py
"""
æ–‡ä»¶è¯´æ˜: æƒé™èµ„äº§ç›˜ç‚¹æœåŠ¡ (Security Inventory Service) - V2.1 Fix Duplicates
ä¸»è¦åŠŸèƒ½:
1. [æ•´åˆæ•°æ®æº] å°† modules.json (å¯¼èˆª) å’Œ action_registry.json (åŠ¨ä½œ) åˆå¹¶ã€‚
2. [æ„å»ºå±‚çº§] ç”Ÿæˆ Module -> Tab -> Action çš„ä¸‰çº§æƒé™æ ‘ã€‚
3. [Fix] è¿‡æ»¤å†²çª: ä¸¥æ ¼æ’é™¤ 'admin_only' å’Œ 'public'ï¼Œé˜²æ­¢ UI Duplicate Key æŠ¥é”™ã€‚
"""

from typing import List, Dict, Any
from config.settings import settings


class SecurityInventory:
    # [æ ¸å¿ƒä¿®å¤] å®šä¹‰ä¸éœ€è¦é…ç½®ç»†ç²’åº¦æƒé™çš„ Key
    # public: æ‰€æœ‰äººå¯è§
    # admin_only: ä»…ç®¡ç†å‘˜å¯è§ (Role-Based)ï¼Œä¸åº”å‡ºç°åœ¨ Permission Tree ä¸­
    IGNORED_PERMS = {"public", "admin_only"}

    @staticmethod
    def get_full_permission_tree() -> List[Dict[str, Any]]:
        """
        æ„å»ºå®Œæ•´çš„ä¸‰çº§æƒé™æ ‘
        """
        # 1. åŠ è½½é…ç½®
        nav_modules = settings.load_modules_config()
        action_registry = settings.load_action_registry()

        # å‡†å¤‡åŠ¨ä½œæ˜ å°„
        registry_map = {}
        if "modules" in action_registry:
            for m in action_registry["modules"]:
                m_key = m.get("key")
                registry_map[m_key] = {}
                for t in m.get("tabs", []):
                    t_key = t.get("key")
                    registry_map[m_key][t_key] = t.get("actions", [])

        tree = []

        # 2. éå†å¯¼èˆªé…ç½®
        for mod in nav_modules:
            perm_key = mod.get("permission", "public")

            # [Fix] è¿‡æ»¤æ‰ admin_only å’Œ public
            if perm_key in SecurityInventory.IGNORED_PERMS:
                continue

            mod_node = {
                "key": perm_key,
                "name": mod.get("name", "Unknown"),
                "type": "module",
                "children": []
            }

            raw_mod_key = mod.get("key")

            # éå† Tabs
            for tab in mod.get("tabs", []):
                tab_perm_key = tab.get("permission")

                # [Fix] Tab çº§è¿‡æ»¤
                if not tab_perm_key or tab_perm_key in SecurityInventory.IGNORED_PERMS:
                    continue

                raw_tab_key = tab.get("key")

                tab_node = {
                    "key": tab_perm_key,
                    "name": tab.get("name", "Unknown"),
                    "type": "tab",
                    "children": []
                }

                # 3. æ³¨å…¥ Actions
                if raw_mod_key in registry_map and raw_tab_key in registry_map[raw_mod_key]:
                    actions = registry_map[raw_mod_key][raw_tab_key]
                    for act in actions:
                        act_node = {
                            "key": act.get("key"),
                            "name": act.get("name"),
                            "type": "action",
                            "desc": act.get("description")
                        }
                        tab_node["children"].append(act_node)

                mod_node["children"].append(tab_node)

            # åªæœ‰å½“æ¨¡å—ä¸‹æœ‰å­èŠ‚ç‚¹æ—¶æ‰æ·»åŠ  (å¯é€‰)
            # è¿™é‡Œä¿ç•™æ¨¡å—èŠ‚ç‚¹ï¼Œå³ä½¿å®ƒæ˜¯ç©ºçš„ï¼Œä»¥ä¾¿å°†æ¥æ‰©å±•
            tree.append(mod_node)

        return tree

    @staticmethod
    def get_flat_action_list() -> List[Dict]:
        """è·å–æ‰å¹³åŒ–çš„æ‰€æœ‰åŠ¨ä½œåˆ—è¡¨"""
        action_registry = settings.load_action_registry()
        flat_list = []
        if "modules" in action_registry:
            for m in action_registry["modules"]:
                for t in m.get("tabs", []):
                    for a in t.get("actions", []):
                        a["module_name"] = m.get("name")
                        a["tab_name"] = t.get("name")
                        flat_list.append(a)
        return flat_list
==================== END FILE: core/services/security/inventory.py ====================


==================== START FILE: core/services/auth/service.py ====================
# core/services/auth/service.py
"""
æ–‡ä»¶è¯´æ˜: è®¤è¯ä¸ç”¨æˆ·ç®¡ç†æœåŠ¡ (Authentication Service) - Traceability Enhanced
ä¸»è¦åŠŸèƒ½:
1. ç”¨æˆ·ç™»å½•éªŒè¯ä¸ä¼šè¯ç®¡ç†ã€‚
2. ç”¨æˆ·è´¦å·ç®¡ç† (CRUD)ã€‚
3. [Fix] æ·±åº¦æº¯æº: åœ¨ç³»ç»Ÿåˆå§‹åŒ–(Bootstrap)æ—¶ï¼Œè¯¦ç»†è®°å½•æƒé™å˜æ›´çš„å·®å¼‚(Diff)ã€‚
"""

import os
from dataclasses import dataclass
from typing import Optional, Tuple, Dict, Any, List
import pandas as pd
from sqlalchemy import text

from config.settings import settings
from core.components.db.client import DBClient
from core.components.security import SecurityUtils
from core.sys.logger import get_logger


@dataclass
class User:
    username: str
    is_admin: bool
    is_locked: bool
    failed_attempts: int
    session_token: str = ""


class AuthService:
    USER_TABLE = "User_Account"
    PERM_TABLE = "User_Permission"
    LOGIN_HISTORY_TABLE = "User_Login_History"
    MAX_FAILED_ATTEMPTS = 10

    _logger = get_logger("AuthService")

    @classmethod
    def initialize(cls) -> None:
        cls._ensure_schema()
        cls._bootstrap_default_users()

    @classmethod
    def _ensure_schema(cls) -> None:
        charset = settings.DB_CHARSET
        # (Schema creation SQL kept concise for brevity, logic unchanged)
        sqls = [
            f"""CREATE TABLE IF NOT EXISTS `{cls.USER_TABLE}` (
                id INT AUTO_INCREMENT PRIMARY KEY, 
                username VARCHAR(64) NOT NULL UNIQUE, 
                password_hash VARCHAR(255) NOT NULL, 
                is_admin TINYINT(1) DEFAULT 0, 
                is_locked TINYINT(1) DEFAULT 0, 
                failed_attempts INT DEFAULT 0, 
                session_token VARCHAR(64), 
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP, 
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
            ) ENGINE=InnoDB DEFAULT CHARSET={charset};""",
            f"""CREATE TABLE IF NOT EXISTS `{cls.PERM_TABLE}` (
                id INT AUTO_INCREMENT PRIMARY KEY, 
                username VARCHAR(64) NOT NULL, 
                permission_key VARCHAR(128) NOT NULL, 
                allowed TINYINT(1) DEFAULT 1, 
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP, 
                UNIQUE KEY uniq_user_perm (username, permission_key)
            ) ENGINE=InnoDB DEFAULT CHARSET={charset};""",
            f"""CREATE TABLE IF NOT EXISTS `{cls.LOGIN_HISTORY_TABLE}` (
                id INT AUTO_INCREMENT PRIMARY KEY, 
                username VARCHAR(64) NOT NULL, 
                ip_address VARCHAR(45) NOT NULL, 
                login_at DATETIME DEFAULT CURRENT_TIMESTAMP, 
                INDEX idx_user (username)
            ) ENGINE=InnoDB DEFAULT CHARSET={charset};"""
        ]
        for sql in sqls: DBClient.execute_stmt(sql)

    @classmethod
    def _bootstrap_default_users(cls) -> None:
        au = os.getenv("DEFAULT_ADMIN_USERNAME", "admin")
        ap = os.getenv("DEFAULT_ADMIN_PASSWORD")
        if ap: cls._ensure_user(au, ap, is_admin=True)

        nu = os.getenv("DEFAULT_USER_USERNAME", "user")
        np = os.getenv("DEFAULT_USER_PASSWORD")
        if np: cls._ensure_user(nu, np, is_admin=False)

    @classmethod
    def _ensure_user(cls, u: str, p: str, is_admin: bool):
        df = DBClient.read_df(f"SELECT id, is_admin FROM `{cls.USER_TABLE}` WHERE username=:u", {"u": u})
        if df.empty:
            cls._logger.info(f"Bootstrap: åˆ›å»ºåˆå§‹ç”¨æˆ· {u} | Role: {'Admin' if is_admin else 'User'}")
            ph = SecurityUtils.hash_password(p)
            sql = f"INSERT INTO `{cls.USER_TABLE}` (username, password_hash, is_admin) VALUES (:u, :ph, :a)"
            DBClient.execute_stmt(sql, {"u": u, "ph": ph, "a": 1 if is_admin else 0})
        else:
            current_role = bool(df.iloc[0]["is_admin"])
            if current_role != is_admin:
                # [Fix] è¯¦ç»†è®°å½•å˜æ›´å‰åçš„çŠ¶æ€
                cls._logger.warning(
                    f"Bootstrap: ä¿®æ­£ç”¨æˆ· {u} æƒé™ | is_admin: {current_role} -> {is_admin}",
                    extra={"user": "System", "action": "AUTO_FIX_ROLE"}
                )
                DBClient.execute_stmt(f"UPDATE `{cls.USER_TABLE}` SET is_admin=:a WHERE username=:u",
                                      {"u": u, "a": 1 if is_admin else 0})

    @classmethod
    def authenticate(cls, username: str, password: str, ip: str = "-") -> Tuple[bool, Optional[User], str]:
        u = username.strip()
        if not u or not password: return False, None, "è¯·è¾“å…¥è´¦å·å¯†ç "

        df = DBClient.read_df(f"SELECT * FROM `{cls.USER_TABLE}` WHERE username=:u LIMIT 1", {"u": u})

        if df.empty:
            SecurityUtils.verify_password("dummy", SecurityUtils.hash_password("dummy"))  # Timing attack protection
            cls._logger.warning(f"ç™»å½•å¤±è´¥: ç”¨æˆ·ä¸å­˜åœ¨ | User: {u}")
            return False, None, "ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯"

        row = df.iloc[0]
        user = User(
            username=row["username"],
            is_admin=bool(row["is_admin"]),
            is_locked=bool(row["is_locked"]),
            failed_attempts=int(row["failed_attempts"]),
            session_token=row.get("session_token") or ""
        )

        if user.is_locked:
            cls._logger.warning(f"ç™»å½•æ‹¦æˆª: é”å®šè´¦å·å°è¯•ç™»å½• | User: {u}")
            return False, None, "è´¦å·å·²é”å®š"

        if not SecurityUtils.verify_password(password, row["password_hash"]):
            new_f = user.failed_attempts + 1
            is_l = 1 if new_f >= cls.MAX_FAILED_ATTEMPTS else 0

            # è®°å½•é”å®šäº‹ä»¶
            if is_l and not user.is_locked:
                cls._logger.warning(f"è´¦å·è§¦å‘è‡ªåŠ¨é”å®š | User: {u} | Failed: {new_f}")

            DBClient.execute_stmt(f"UPDATE `{cls.USER_TABLE}` SET failed_attempts=:f, is_locked=:l WHERE username=:u",
                                  {"f": new_f, "l": is_l, "u": u})
            return False, None, "ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯"

        DBClient.execute_stmt(f"UPDATE `{cls.USER_TABLE}` SET failed_attempts=0 WHERE username=:u", {"u": u})
        cls.record_login_event(u, ip)
        cls._logger.info(f"ç™»å½•æˆåŠŸ: {u} | IP: {ip}")
        return True, user, "ç™»å½•æˆåŠŸ"

    @classmethod
    def record_login_event(cls, username: str, ip: str) -> None:
        DBClient.execute_stmt(f"INSERT INTO `{cls.LOGIN_HISTORY_TABLE}` (username, ip_address) VALUES (:u, :ip)",
                              {"u": username, "ip": ip})

    @classmethod
    def refresh_session_token(cls, username: str) -> str:
        token = SecurityUtils.generate_token()
        DBClient.execute_stmt(f"UPDATE `{cls.USER_TABLE}` SET session_token=:t WHERE username=:u",
                              {"t": token, "u": username})
        return token

    @classmethod
    def verify_session_token(cls, username: str, local_token: str) -> bool:
        if not username or not local_token: return False
        try:
            df = DBClient.read_df(f"SELECT session_token FROM `{cls.USER_TABLE}` WHERE username=:u", {"u": username})
            if df.empty: return False
            db_token = df.iloc[0]["session_token"]
            return db_token == local_token
        except:
            return False

    @classmethod
    def get_permissions(cls, u: str) -> Dict[str, bool]:
        df = DBClient.read_df(f"SELECT permission_key FROM `{cls.PERM_TABLE}` WHERE username=:u AND allowed=1",
                              {"u": u})
        return {r["permission_key"]: True for _, r in df.iterrows()}

    @classmethod
    def list_users(cls) -> pd.DataFrame:
        return DBClient.read_df(
            f"SELECT username, is_admin, is_locked, failed_attempts FROM `{cls.USER_TABLE}` ORDER BY is_admin DESC")

    @classmethod
    def create_user(cls, u, p, is_admin=False) -> Tuple[bool, str]:
        if not DBClient.read_df(f"SELECT 1 FROM `{cls.USER_TABLE}` WHERE username=:u", {"u": u}).empty:
            return False, "ç”¨æˆ·å·²å­˜åœ¨"
        ph = SecurityUtils.hash_password(p)
        DBClient.execute_stmt(
            f"INSERT INTO `{cls.USER_TABLE}` (username, password_hash, is_admin) VALUES (:u, :ph, :a)",
            {"u": u, "ph": ph, "a": 1 if is_admin else 0})
        return True, "åˆ›å»ºæˆåŠŸ"

    @classmethod
    def reset_password(cls, u, np) -> Tuple[bool, str]:
        ph = SecurityUtils.hash_password(np)
        row = DBClient.execute_stmt(
            f"UPDATE `{cls.USER_TABLE}` SET password_hash=:ph, failed_attempts=0 WHERE username=:u", {"ph": ph, "u": u})
        return (True, "é‡ç½®æˆåŠŸ") if row else (False, "ç”¨æˆ·ä¸å­˜åœ¨")

    @classmethod
    def set_lock_state(cls, u, lock) -> Tuple[bool, str]:
        row = DBClient.execute_stmt(f"UPDATE `{cls.USER_TABLE}` SET is_locked=:l WHERE username=:u",
                                    {"l": 1 if lock else 0, "u": u})
        return (True, "æ›´æ–°æˆåŠŸ") if row else (False, "ç”¨æˆ·ä¸å­˜åœ¨")

    @classmethod
    def set_permissions(cls, u, pmap):
        with DBClient.atomic_transaction() as conn:
            conn.execute(text(f"DELETE FROM `{cls.PERM_TABLE}` WHERE username=:u"), {"u": u})
            if pmap:
                vals = ", ".join([f"(:u, :k{i}, 1)" for i in range(len(pmap))])
                params = {"u": u}
                for i, k in enumerate(pmap): params[f"k{i}"] = k
                sql = f"INSERT INTO `{cls.PERM_TABLE}` (username, permission_key, allowed) VALUES {vals}"
                conn.execute(text(sql), params)

    @classmethod
    def get_user_login_stats(cls, username: str) -> Dict[str, Any]:
        cnt_df = DBClient.read_df(f"SELECT COUNT(*) FROM `{cls.LOGIN_HISTORY_TABLE}` WHERE username=:u",
                                  {"u": username})
        cnt = cnt_df.iloc[0, 0] if not cnt_df.empty else 0
        hist = DBClient.read_df(
            f"SELECT ip_address, COUNT(*) as count, MAX(login_at) as last_seen FROM `{cls.LOGIN_HISTORY_TABLE}` WHERE username=:u GROUP BY ip_address ORDER BY count DESC",
            {"u": username})
        return {"total_logins": int(cnt), "ip_history": hist}
==================== END FILE: core/services/auth/service.py ====================


==================== START FILE: core/services/auth/__init__.py ====================

==================== END FILE: core/services/auth/__init__.py ====================


==================== START FILE: core/services/etl/ingest.py ====================
# core/services/etl/ingest.py
"""
æ–‡ä»¶è¯´æ˜: æ•°æ®æ‘„å…¥æœåŠ¡ (Ingest Service)
ä¸»è¦åŠŸèƒ½:
1. è¯»å– Transaction å’Œ Earning çš„ CSV æ–‡ä»¶ã€‚
2. æ™ºèƒ½è¯†åˆ« Seller (é€šè¿‡æ–‡ä»¶å†…å®¹æˆ–æ–‡ä»¶å) å’Œ Header è¡Œã€‚
3. **æ•°æ®æ¸…æ´—**: å…³é”®ä¿®å¤ - å¯¹æ‰€æœ‰å­—ç¬¦ä¸²åˆ—æ‰§è¡Œ strip()ï¼Œå»é™¤éšå½¢ç©ºæ ¼ã€‚
4. **æ•°æ®å…¥åº“**: ä½¿ç”¨ 'replace' æ¨¡å¼å…¨é‡é‡å»º Raw è¡¨ï¼Œé€‚åº”åˆ—ç»“æ„å˜åŒ–ã€‚
"""

import pandas as pd
import numpy as np
import csv
import io
from typing import List, Tuple, Optional, Any
from pathlib import Path
from sqlalchemy.types import Text

from core.components.db.client import DBClient
from core.sys.logger import get_logger
from core.services.etl.repository import ETLRepository

# æŠ‘åˆ¶ Pandas è­¦å‘Š
pd.set_option('future.no_silent_downcasting', True)


class IngestService:

    def __init__(self):
        self.logger = get_logger("IngestService")
        self.repo = ETLRepository()

    def run_ingest_pipeline(self, transaction_files: List[Any], earning_files: List[Any]) -> str:
        """
        [ä¸»å…¥å£] æ‰§è¡Œå®Œæ•´çš„æ‘„å…¥æµç¨‹
        files å‚æ•°æ”¯æŒ: Streamlit UploadedFile (BytesIO) æˆ– pathlib.Path
        """
        if not transaction_files and not earning_files:
            return "âš ï¸ æ²¡æœ‰æ”¶åˆ°ä»»ä½•æ–‡ä»¶ã€‚"

        # 1. åˆå§‹åŒ–åŸå§‹è¡¨
        self.logger.info("æ­£åœ¨åˆå§‹åŒ–åŸå§‹æ•°æ®è¡¨...")
        self.repo.truncate_raw_tables()

        msg = []

        # 2. å¤„ç† Transaction
        if transaction_files:
            count = self._process_files(transaction_files, "Data_Transaction", "Order number")
            msg.append(f"Transaction: åŠ è½½ {count} æ¡")

        # 3. å¤„ç† Earning
        if earning_files:
            count = self._process_files(earning_files, "Data_Order_Earning", "Order number")
            msg.append(f"Earning: åŠ è½½ {count} æ¡")

        final_msg = " | ".join(msg)
        self.logger.info(f" æ•°æ®æ‘„å…¥å®Œæˆ: {final_msg}")
        return final_msg

    def _process_files(self, files: List[Any], table_name: str, key_col: str) -> int:
        """é€šç”¨æ–‡ä»¶å¤„ç†é€»è¾‘"""
        all_chunks = []

        for file_obj in files:
            try:
                # 1. æ¢æµ‹å…ƒæ•°æ® (Seller, Skiprows)
                seller, skiprows = self._detect_metadata(file_obj, key_col)

                # 2. å…œåº• Seller
                if not seller and hasattr(file_obj, 'name'):
                    seller = self._infer_seller_from_name(file_obj.name)

                if not seller:
                    name = getattr(file_obj, 'name', 'Unknown')
                    self.logger.warning(f"æ— æ³•è¯†åˆ« Sellerï¼Œè·³è¿‡æ–‡ä»¶: {name}")
                    continue

                # 3. è¯»å–å†…å®¹ (é‡ç½®æŒ‡é’ˆ)
                if hasattr(file_obj, 'seek'): file_obj.seek(0)

                df = pd.read_csv(
                    file_obj,
                    skiprows=skiprows,
                    dtype=str,
                    encoding='utf-8-sig',
                    on_bad_lines='skip'
                )

                if df.empty: continue

                # [å…³é”®ä¿®å¤] å…¨å±€å»ç©ºæ ¼ï¼šå¯¹æ‰€æœ‰ object ç±»å‹åˆ—æ‰§è¡Œ strip
                # è¿™è§£å†³äº† " 20001" != "20001" çš„åŒ¹é…é—®é¢˜
                df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)

                # 4. æ³¨å…¥å…ƒæ•°æ®ä¸æ¸…æ´—
                df["Seller"] = seller
                df.columns = [str(c).strip() for c in df.columns]

                # æ ¡éªŒå…³é”®åˆ—
                if key_col not in df.columns:
                    self.logger.warning(f"æ–‡ä»¶ç»“æ„å¼‚å¸¸ (ç¼ºå¤± {key_col})")
                    continue

                # æ¸…ç†ç©ºå€¼
                df = df.replace(['--', '-', 'N/A', 'null', 'nan', 'None'], np.nan)
                df = df.dropna(how='all')

                if not df.empty:
                    all_chunks.append(df)

            except Exception as e:
                name = getattr(file_obj, 'name', 'Unknown')
                self.logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥ ({name}): {e}")

        if not all_chunks:
            return 0

        # 5. åˆå¹¶å¹¶å†™å…¥æ•°æ®åº“
        try:
            final_df = pd.concat(all_chunks, ignore_index=True)
            # ä½¿ç”¨ Text ç±»å‹é˜²æ­¢æˆªæ–­
            dtype_map = {c: Text() for c in final_df.columns}

            # ä½¿ç”¨ replace æ¨¡å¼ï¼Œè‡ªåŠ¨é€‚åº”åˆ—ç»“æ„å˜åŒ–
            final_df.to_sql(
                table_name,
                DBClient.get_engine(),
                if_exists='replace',
                index=False,
                chunksize=2000,
                dtype=dtype_map
            )
            return len(final_df)
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“å†™å…¥å¤±è´¥ ({table_name}): {e}")
            return 0

    def _detect_metadata(self, file_obj: Any, keyword: str) -> Tuple[Optional[str], int]:
        """æ™ºèƒ½å—…æ¢ Seller å’Œ Header è¡Œ"""
        seller = None
        skiprows = 0
        target_key = keyword.lower()

        if hasattr(file_obj, 'seek'): file_obj.seek(0)

        try:
            # å…¼å®¹ Path å’Œ BytesIO
            if isinstance(file_obj, (str, Path)):
                f = open(file_obj, 'r', encoding='utf-8-sig', errors='replace')
                should_close = True
            else:
                # BytesIO éœ€è¦åŒ…è£…
                f = io.TextIOWrapper(file_obj, encoding='utf-8-sig', errors='replace')
                should_close = False

            reader = csv.reader(f)
            lines = []
            for _ in range(30):
                try:
                    lines.append(next(reader))
                except StopIteration:
                    break

            if should_close:
                f.close()
            else:
                f.detach()

            for i, row in enumerate(lines):
                if not row: continue
                clean_row = [str(x).lower().replace('"', '').strip() for x in row]

                # æ‰¾ Seller
                if len(clean_row) >= 2 and clean_row[0] == "seller":
                    if clean_row[1]: seller = clean_row[1]

                # æ‰¾ Header
                if target_key in clean_row:
                    skiprows = i

            return seller, skiprows

        except Exception as e:
            self.logger.warning(f"å…ƒæ•°æ®æ¢æµ‹å¼‚å¸¸: {e}")
            return None, 0

    def _infer_seller_from_name(self, filename: str) -> Optional[str]:
        fname = filename.lower()
        if "88" in fname:
            return "esparts88"
        elif "plus" in fname:
            return "espartsplus"
        return None
==================== END FILE: core/services/etl/ingest.py ====================


==================== START FILE: core/services/etl/__init__.py ====================
# core/services/etl/__init__.py
"""
æ–‡ä»¶è¯´æ˜: ETL æœåŠ¡åŒ…åˆå§‹åŒ–
ä¸»è¦åŠŸèƒ½:
1. åŒ…å«æ•°æ®æ‘„å…¥ (Ingest)ã€è§£æ (Parser)ã€è½¬æ¢ (Transformer) ä¸‰å¤§æ ¸å¿ƒç»„ä»¶ã€‚
2. è´Ÿè´£å¤„ç† Data_Transaction, Data_Order_Earning, Data_Clean_Log ç­‰æ ¸å¿ƒè¡¨ã€‚
"""
==================== END FILE: core/services/etl/__init__.py ====================


==================== START FILE: core/services/etl/parser.py ====================
# core/services/etl/parser.py
"""
æ–‡ä»¶è¯´æ˜: äº¤æ˜“æ•°æ®è§£æå™¨ (Transaction Parser)
ä¸»è¦åŠŸèƒ½:
1. ç»“æ„åŒ–è§£æ: ä» 'Custom label' å­—æ®µä¸­æå– SKU å’Œ Quantityã€‚
2. æ¨¡å¼è¯†åˆ«: æ”¯æŒ Single (å•å“), Dual (åŒå“), Complex (å¤šå“/ç‰¹æ®Šç¬¦) æ ¼å¼ã€‚
3. æ•°æ®æ ¡éªŒ: éªŒè¯æå–å‡ºçš„ SKU æ˜¯å¦åœ¨ç³»ç»Ÿèµ„æ–™åº“ (Data_COGS) ä¸­å­˜åœ¨ã€‚
4. [Fix] é€»è¾‘ä¿®å¤: å¼ºåˆ¶é‡æ–°æ ¡éªŒ P_Flag=99 çš„è¡Œï¼Œè§£å†³â€œåƒµå°¸é”™è¯¯â€é—®é¢˜ã€‚
"""

import pandas as pd
import numpy as np
import re
import tqdm
from typing import Dict, Any
from sqlalchemy.types import Text

from core.components.db.client import DBClient
from core.services.correction import CorrectionService
from core.sys.logger import get_logger

# æŠ‘åˆ¶ Pandas çš„ FutureWarning
pd.set_option('future.no_silent_downcasting', True)


class TransactionParser:

    def __init__(self):
        self.db = DBClient()
        self.logger = get_logger("TransactionParser")
        self.corrector = CorrectionService()

        # ç¼“å­˜ä¿®å¤å­—å…¸ (BadSKU -> {sku, qty})ï¼Œå‡å°‘å¾ªç¯å†…çš„æŸ¥æ‰¾å¼€é”€
        self.fix_map = self._build_fast_fix_map()

        # å®šä¹‰è¾“å‡ºåˆ—ç»“æ„
        self.parse_cols = ['P_Flag', 'P_Key', 'P_Type', 'P_Check', 'Skufix_Check']
        for i in range(1, 11):
            self.parse_cols.extend([f'P_SKU{i}', f'P_Quantity{i}'])

    def _build_fast_fix_map(self) -> Dict[str, dict]:
        """[ä¼˜åŒ–] æ„å»ºå†…å­˜çº§å¿«é€Ÿä¿®å¤æŸ¥æ‰¾è¡¨"""
        memory_dict = {}
        if not self.corrector.memory_df.empty:
            for _, row in self.corrector.memory_df.iterrows():
                bad = str(row.get('BadSKU', '')).strip().upper()
                if bad:
                    memory_dict[bad] = {
                        'sku': str(row.get('CorrectSKU', '')).strip().upper(),
                        'qty': str(row.get('CorrectQty', '')).strip()
                    }
        return memory_dict

    def run(self) -> Dict[str, Any]:
        """[ä¸»å…¥å£] æ‰§è¡Œè§£ææµç¨‹"""
        self.logger.info("ğŸš€ [Parser] å¼€å§‹è§£æäº¤æ˜“æ•°æ®...")

        # 1. è¯»å– Raw Data
        df = self.db.read_df("SELECT * FROM Data_Transaction")
        if df.empty:
            self.logger.warning("Data_Transaction è¡¨ä¸ºç©ºï¼Œè·³è¿‡è§£æã€‚")
            return {"status": "empty", "auto_fixed": [], "pending_count": 0}

        # 2. åˆå§‹åŒ–åˆ—ç»“æ„
        df = self._init_columns(df)

        # 3. æ­£åˆ™è§£æ (é«˜æ€§èƒ½å‘é‡åŒ–)
        df = self._apply_regex_patterns(df)

        # 4. å¤æ‚è§£æ (è¿­ä»£å¤„ç†å…œåº•)
        df = self._process_complex_rows(df)

        # 5. æ ¡éªŒä¸è‡ªåŠ¨ä¿®å¤
        validation_result = self._validate_and_autofix(df)
        df_final = validation_result["df"]

        # 6. å›å†™æ•°æ®åº“
        self.logger.info("ğŸ’¾ æ­£åœ¨ä¿å­˜è§£æç»“æœ...")
        try:
            # å…¨é‡è¦†ç›–ï¼Œç¡®ä¿åˆ—ç»“æ„æ›´æ–°
            dtype_map = {c: Text() for c in df_final.columns}
            df_final.to_sql(
                "Data_Transaction",
                self.db.get_engine(),
                if_exists='replace',
                index=False,
                chunksize=2000,
                dtype=dtype_map
            )
            self.logger.info(" æ•°æ®åº“æ›´æ–°æˆåŠŸã€‚")
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“å›å†™å¤±è´¥: {e}")
            return {"status": "error", "message": str(e)}

        return {
            "status": "success",
            "auto_fixed": validation_result["fixed_logs"],
            "pending_count": validation_result["failed_count"]
        }

    def _init_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆå§‹åŒ–å¿…è¦çš„åˆ—"""
        # æ ‡å‡†åŒ–åˆ—å
        col_map = {c.lower().replace(" ", ""): c for c in df.columns}
        if 'customlabel' in col_map:
            df.rename(columns={col_map['customlabel']: 'Custom label'}, inplace=True)
        elif 'Custom label' not in df.columns:
            df['Custom label'] = ''

        # åˆå§‹åŒ– P_* åˆ—
        for col in self.parse_cols:
            if col not in df.columns:
                df[col] = None

        if 'Item title' not in df.columns:
            df['Item title'] = ''

        # ç¡®ä¿æ ‡è®°åˆ—ä¸ºæ•´æ•°ç±»å‹ (fillna 0)
        df['P_Flag'] = df['P_Flag'].fillna(0).astype(int)
        return df

    def _apply_regex_patterns(self, df: pd.DataFrame) -> pd.DataFrame:
        """[Stage 1] å‘é‡åŒ–æ­£åˆ™åŒ¹é…"""
        self.logger.info(" æ‰§è¡Œæ­£åˆ™è§£æ (Vectorized)...")

        s_label = df['Custom label'].astype(str).str.strip()
        mask_todo = (df['P_Flag'] == 0)

        # Pattern 1: Single (e.g., "ABCD-1234.1")
        # å…è®¸å­—ç¬¦: A-Z, 0-9, -, /, _
        pat1 = r'^(?:[A-Za-z]{1}[A-Za-z0-9]{0,2}\.)?(?P<SKU>[A-Za-z0-9\-_/]{7,})\.(?P<Quantity>\d{1,3})(?P<QuantityKey>\+2K)?(?:\.[A-Za-z0-9_]*){0,2}$'
        ext1 = s_label[mask_todo].str.extract(pat1)
        idx1 = ext1[ext1['SKU'].notna()].index

        if not idx1.empty:
            df.loc[idx1, 'P_Flag'] = 1
            df.loc[idx1, 'P_Type'] = 'single'
            df.loc[idx1, 'P_SKU1'] = ext1.loc[idx1, 'SKU']
            df.loc[idx1, 'P_Quantity1'] = ext1.loc[idx1, 'Quantity']
            df.loc[idx1, 'P_Key'] = ext1.loc[idx1, 'QuantityKey'].apply(lambda x: 2 if pd.notna(x) else 0)
            df.loc[idx1, 'Skufix_Check'] = 1

        # Pattern 2: Dual (e.g., "PART1.1+PART2.1")
        # [Fix] ç¨å¾®æ”¾å®½å‰ç¼€åŒ¹é…ï¼Œé˜²æ­¢ ZD. è¿™ç§è¢«æ¼æ‰ ([A-Za-z0-2] -> [A-Za-z0-9])
        mask_todo = (df['P_Flag'] == 0)
        pat2 = r'^(?:[A-Za-z]{1}[A-Za-z0-9]{0,2}\.)?(?P<S1>[A-Za-z0-9/\-_]{7,})\.(?P<Q1>\d{1,3})(?P<K1>\+2K)?[\+\.](?P<S2>[A-Za-z0-9/\-_]{7,})\.(?P<Q2>\d{1,3})(?P<K2>\+2K)?(?:\.[A-Za-z0-9_]*){0,2}$'
        ext2 = s_label[mask_todo].str.extract(pat2)
        idx2 = ext2[ext2['S1'].notna() & ext2['S2'].notna()].index

        if not idx2.empty:
            df.loc[idx2, 'P_Flag'] = 2
            df.loc[idx2, 'P_Type'] = 'dual'
            df.loc[idx2, 'P_SKU1'] = ext2.loc[idx2, 'S1']
            df.loc[idx2, 'P_Quantity1'] = ext2.loc[idx2, 'Q1']
            df.loc[idx2, 'P_SKU2'] = ext2.loc[idx2, 'S2']
            df.loc[idx2, 'P_Quantity2'] = ext2.loc[idx2, 'Q2']
            k1 = ext2.loc[idx2, 'K1'].notna().astype(int) * 2
            k2 = ext2.loc[idx2, 'K2'].notna().astype(int) * 2
            df.loc[idx2, 'P_Key'] = k1 + k2
            df.loc[idx2, 'Skufix_Check'] = 1

        return df

    def _process_complex_rows(self, df: pd.DataFrame) -> pd.DataFrame:
        """[Stage 2] è¿­ä»£è§£æå¤æ‚è¡Œ"""
        mask_complex = (df['P_Flag'] == 0)
        count = mask_complex.sum()
        if count == 0: return df

        self.logger.info(f"ğŸ¢ å¤„ç† {count} æ¡å¤æ‚è®°å½•...")
        junk_chars = {'--', '-', 'N/A', 'NULL', 'NONE', '', 'NAN'}
        # [Fix] æ”¾å®½å‰ç¼€åŒ¹é…
        prefix_pattern = re.compile(r'^(?:[A-Za-z]{1}[A-Za-z0-9]{0,2}\.)?(?P<main>.+?)(?:\.[A-Za-z0-9_]*)?$')

        for idx in df[mask_complex].index:
            raw_label = str(df.at[idx, 'Custom label']).strip()
            match = prefix_pattern.match(raw_label)
            main_part = match.group('main') if match else raw_label

            parts = main_part.split('+')
            p_key = 0
            p_skus = []
            p_qtys = []
            valid_parse = False

            for seg in parts:
                seg = seg.strip()
                if not seg or seg.upper() in junk_chars: continue
                if seg.upper() == '2K':
                    p_key += 2
                    continue
                if '+2K' in seg:
                    p_key += 2
                    seg = seg.replace('+2K', '')

                arr = seg.split('.')
                code = arr[0].upper().strip()
                qty = arr[1] if len(arr) > 1 else '1'

                if code in junk_chars: continue

                # å¿«é€ŸæŸ¥è¡¨ä¿®æ­£ (Fast Fix)
                if code not in self.corrector.valid_skus and code in self.fix_map:
                    code = self.fix_map[code]['sku']

                p_skus.append(code)
                p_qtys.append(qty)
                valid_parse = True

            if valid_parse:
                limit = min(len(p_skus), 10)
                for i in range(limit):
                    df.at[idx, f'P_SKU{i + 1}'] = p_skus[i]
                    df.at[idx, f'P_Quantity{i + 1}'] = p_qtys[i]

                df.at[idx, 'P_Flag'] = 5
                df.at[idx, 'P_Key'] = p_key
                df.at[idx, 'P_Check'] = 1
                df.at[idx, 'Skufix_Check'] = 1

        return df

    def _validate_and_autofix(self, df: pd.DataFrame) -> Dict[str, Any]:
        """[Stage 3] æ ¡éªŒä¸è‡ªåŠ¨ä¿®å¤"""
        self.logger.info("ğŸ” æ‰§è¡Œ SKU æ ¡éªŒä¸è‡ªåŠ¨ä¿®å¤...")

        # [Critical Fix]
        # ä¹‹å‰é€»è¾‘: mask_check = (df['P_Flag'] > 0) & (df['P_Flag'] != 99)
        # é—®é¢˜: 99 è¡¨ç¤ºä¸Šæ¬¡æ ¡éªŒå¤±è´¥ã€‚å¦‚æœç”¨æˆ·å» DB åŠ äº† SKUï¼Œä¸‹æ¬¡è·‘å¿…é¡»é‡æ£€ 99 çš„è¡Œï¼Œå¦åˆ™æ°¸è¿œæ˜¯ 99ã€‚
        # ä¿®æ­£: åªè¦æœ‰è§£æç»“æœ (P_Flag > 0)ï¼Œå°±å¿…é¡»é‡æ–°æ ¡éªŒã€‚
        mask_check = (df['P_Flag'] > 0)

        fix_logs = []
        count_failed = 0

        for idx in df[mask_check].index:
            is_row_valid = True
            custom_label = str(df.at[idx, 'Custom label'])
            order_num = str(df.at[idx, 'Order number'])

            for i in range(1, 11):
                sku_col = f'P_SKU{i}'
                qty_col = f'P_Quantity{i}'
                sku = df.at[idx, sku_col]

                if pd.isna(sku) or str(sku).strip() == "": continue
                sku = str(sku).strip().upper()

                # 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                if self.corrector.is_valid_sku(sku):
                    continue

                # 2. å°è¯•è‡ªåŠ¨ä¿®å¤ (Memory Recall)
                fixed_sku, fixed_qty = self.corrector.find_auto_fix(custom_label, sku)

                if fixed_sku:
                    df.at[idx, sku_col] = fixed_sku
                    msg = f"SKU: {sku} -> {fixed_sku}"
                    if fixed_qty and str(fixed_qty).strip():
                        df.at[idx, qty_col] = fixed_qty
                        msg += f", Qty -> {fixed_qty}"
                    fix_logs.append({"order": order_num, "msg": msg})
                else:
                    is_row_valid = False

            if not is_row_valid:
                df.at[idx, 'P_Flag'] = 99  # æ ‡è®°ä¸ºå¼‚å¸¸
                count_failed += 1
            else:
                # [Fix] å¦‚æœæ ¡éªŒé€šè¿‡äº†ï¼Œä¸”ä¹‹å‰æ˜¯ 99ï¼Œè¦æ¢å¤æˆæ­£å¸¸çŠ¶æ€ (å¦‚ 5)
                current_flag = df.at[idx, 'P_Flag']
                if current_flag == 99:
                    df.at[idx, 'P_Flag'] = 5

        self.logger.info(f"æ ¡éªŒç»“æœ: è‡ªåŠ¨ä¿®å¤ {len(fix_logs)} é¡¹ï¼Œå‰©ä½™ {count_failed} è¡Œå¼‚å¸¸ã€‚")
        return {"fixed_logs": fix_logs, "failed_count": count_failed, "df": df}
==================== END FILE: core/services/etl/parser.py ====================


==================== START FILE: core/services/etl/transformer.py ====================
# core/services/etl/transformer.py
"""
æ–‡ä»¶è¯´æ˜: äº¤æ˜“æ•°æ®è½¬æ¢å¼•æ“ (Transformer) - Date Normalized
ä¸»è¦åŠŸèƒ½:
1. å°† Raw Data è½¬æ¢ä¸º Clean Dataã€‚
2. ä¸šåŠ¡é€»è¾‘è®¡ç®— (Action/Seller/Fee Proration)ã€‚
3. [Fix] å¼ºåˆ¶æ—¥æœŸæ ¼å¼åŒ–: æ— è®ºåŸå§‹æ ¼å¼å¦‚ä½•ï¼Œå…¥åº“å‰ç»Ÿä¸€è½¬ä¸º 'YYYY-MM-DD'ã€‚
4. å››ç»´å»é‡å¹¶å¢é‡å†™å…¥ã€‚
"""

import pandas as pd
import numpy as np
from sqlalchemy import text
from sqlalchemy.types import Text
from typing import Callable, Optional
from dateutil import parser  # [New] å¼•å…¥å¼ºåŠ›è§£æå™¨

from core.components.db.client import DBClient
from core.sys.logger import get_logger


class TransactionTransformer:

    def __init__(self):
        self.db = DBClient()
        self.logger = get_logger("TransactionTransformer")

        self.output_cols = [
            'order date', 'seller', 'order number', 'item id', 'item title', 'full sku', 'quantity',
            'revenue', 'Shipping and handling', 'Seller collected tax', 'eBay collected tax',
            'Final Value Fee - fixed', 'Final Value Fee - variable', 'Regulatory operating fee',
            'International fee', 'Promoted Listings fee', 'Payments dispute fee',
            'action', 'Refund',
            'Shipping label-Earning data', 'Shipping label-Regular',
            'Shipping label-underpay', 'Shipping label-overpay', 'Shipping label-Return',
            'buyer username', 'ship to city', 'ship to country'
        ]
        for i in range(1, 11):
            self.output_cols.extend([f'sku{i}', f'qty{i}', f'qtyp{i}'])

    def _safe_float(self, series: pd.Series) -> pd.Series:
        if series.empty: return series
        clean = series.astype(str).str.replace(r'[$,\s]', '', regex=True)
        return pd.to_numeric(clean, errors='coerce').fillna(0.0)

    def _normalize_date(self, date_val) -> str:
        """
        [New] å¼ºåŠ›æ—¥æœŸè§£æ
        Input: 'Jun 30, 2025', '2025-06-30', '30-Jun-25'
        Output: '2025-06-30'
        """
        if pd.isna(date_val) or str(date_val).strip() == "":
            return None

        s = str(date_val).strip()
        try:
            # ä¼˜å…ˆå°è¯• pandas è‡ªåŠ¨æ¨æ–­
            dt = pd.to_datetime(s, errors='raise')
            return dt.strftime('%Y-%m-%d')
        except:
            try:
                # å°è¯• dateutil (æ›´æ™ºèƒ½)
                dt = parser.parse(s)
                return dt.strftime('%Y-%m-%d')
            except:
                # è§£æå¤±è´¥ï¼Œè¿”å›åŸå€¼ä»¥ä¾¿æ’æŸ¥
                return s

    def run(self, progress_callback: Optional[Callable[[float, str], None]] = None) -> str:

        def report(p, msg):
            if progress_callback: progress_callback(p, msg)
            self.logger.info(msg)

        try:
            report(0.05, "ğŸš€ [Transformer] å¯åŠ¨è½¬æ¢å¼•æ“...")

            # æ³¨æ„ï¼šæ­¤å¤„è¡¨åéœ€ä¸ Ingest é˜¶æ®µä¿æŒä¸€è‡´ï¼Œé»˜è®¤ä½¿ç”¨ Data_Transaction
            df_trans = self.db.read_df("SELECT * FROM Data_Transaction")
            df_earn = self.db.read_df("SELECT * FROM Data_Order_Earning")

            if df_trans.empty:
                return "âš ï¸ æºæ•°æ®ä¸ºç©ºï¼Œæµç¨‹ç»ˆæ­¢ã€‚"

            df_trans.columns = df_trans.columns.str.strip().str.lower()
            df_earn.columns = df_earn.columns.str.strip().str.lower()

            # --- æ•°å€¼æ¸…æ´— ---
            report(0.15, "ğŸ§¹ æ‰§è¡Œæ•°å€¼æ¸…æ´—...")
            num_cols = [
                'item subtotal', 'quantity', 'gross transaction amount',
                'shipping and handling', 'seller collected tax', 'ebay collected tax',
                'final value fee - fixed', 'final value fee - variable', 'regulatory operating fee',
                'international fee', 'promoted listings fee', 'payments dispute fee', 'refund'
            ]
            for c in num_cols:
                if c in df_trans.columns:
                    df_trans[c] = self._safe_float(df_trans[c])

            # Earning è¡¨å¤„ç†
            if not df_earn.empty and 'shipping labels' in df_earn.columns:
                df_earn['shipping labels'] = self._safe_float(df_earn['shipping labels'])
                earn_map = df_earn.groupby('order number')['shipping labels'].sum().reset_index()
                earn_map.rename(columns={'shipping labels': 'Shipping label-Earning data'}, inplace=True)
            else:
                earn_map = pd.DataFrame(columns=['order number', 'Shipping label-Earning data'])

            # --- ä¸šåŠ¡é€»è¾‘ ---
            report(0.30, "ğŸ§  è®¡ç®—ä¸šåŠ¡è§„åˆ™...")
            for col in ['type', 'reference id', 'seller', 'item id']:
                if col not in df_trans.columns: df_trans[col] = ''

            # Action Logic
            df_trans['type_lower'] = df_trans['type'].astype(str).str.lower()
            df_trans['ref_lower'] = df_trans['reference id'].astype(str).str.lower()
            df_trans['action_weight'] = 0
            df_trans['action_code'] = 'NN'

            mask_pd = df_trans['type_lower'] == 'payment dispute'
            df_trans.loc[mask_pd, 'action_weight'] = 50
            df_trans.loc[mask_pd, 'action_code'] = 'PD'

            mask_claim = df_trans['type_lower'] == 'claim'
            mask_case = mask_claim & df_trans['ref_lower'].str.contains('case', case=False)
            df_trans.loc[mask_case, 'action_code'] = 'CC';
            df_trans.loc[mask_case, 'action_weight'] = 40

            mask_req = mask_claim & df_trans['ref_lower'].str.contains('request', case=False)
            df_trans.loc[mask_req, 'action_code'] = 'CR';
            df_trans.loc[mask_req, 'action_weight'] = 30

            mask_refund = df_trans['type_lower'] == 'refund'
            mask_ret = mask_refund & df_trans['ref_lower'].str.contains('return', case=False)
            df_trans.loc[mask_ret, 'action_code'] = 'RE';
            df_trans.loc[mask_ret, 'action_weight'] = 20

            mask_cancel = mask_refund & df_trans['ref_lower'].str.contains('cancel', case=False)
            df_trans.loc[mask_cancel, 'action_code'] = 'CA';
            df_trans.loc[mask_cancel, 'action_weight'] = 10

            action_map = df_trans.sort_values('action_weight', ascending=False).drop_duplicates('order number')[
                ['order number', 'action_code']]
            action_map.rename(columns={'action_code': 'action'}, inplace=True)

            # Seller Logic
            df_trans['seller_clean'] = df_trans['seller'].astype(str).str.strip().str.replace(r'[\'"]', '', regex=True)
            df_trans['is_prio'] = df_trans['seller_clean'].str.lower().str.contains('esparts').astype(int)
            seller_map = \
            df_trans.sort_values(['is_prio', 'seller_clean'], ascending=[False, True]).drop_duplicates('order number')[
                ['order number', 'seller_clean']]
            seller_map.rename(columns={'seller_clean': 'seller'}, inplace=True)

            # --- ç‰©æµè´¹ç”¨æå– ---
            report(0.50, "ğŸšš æå–éšæ€§ç‰©æµæˆæœ¬...")
            mask_ship = df_trans['type_lower'] == 'shipping label'
            df_ship = df_trans[mask_ship].copy()
            if 'description' not in df_ship.columns: df_ship['description'] = ''

            df_ship['desc_lower'] = df_ship['description'].astype(str).str.lower()
            df_ship['amt'] = df_ship['gross transaction amount']

            df_ship['underpay'] = np.where(df_ship['desc_lower'].str.contains('underpaid'), df_ship['amt'], 0.0)
            df_ship['overpay'] = np.where(df_ship['desc_lower'].str.contains('overpaid'), df_ship['amt'], 0.0)
            df_ship['return'] = np.where(df_ship['desc_lower'].str.contains('return shipping'), df_ship['amt'], 0.0)
            df_ship['regular'] = np.where(~df_ship['desc_lower'].str.contains('underpaid|overpaid|return|voided|bulk'),
                                          df_ship['amt'], 0.0)

            ship_agg = df_ship.groupby('order number')[['underpay', 'overpay', 'return', 'regular']].sum().reset_index()
            ship_agg.columns = ['order number', 'Shipping label-underpay', 'Shipping label-overpay',
                                'Shipping label-Return', 'Shipping label-Regular']

            # --- ä¸»è¡¨æ„å»º ---
            report(0.70, "ğŸ§® è®¢å•çº§è´¹ç”¨åˆ†æ‘Š...")
            mask_order = (df_trans['type_lower'] == 'order') & (df_trans['item id'].notna())
            df_main = df_trans[mask_order].copy()

            for c in ['seller', 'action']:
                if c in df_main.columns: df_main.drop(columns=[c], inplace=True)

            df_main = df_main.merge(action_map, on='order number', how='left')
            df_main = df_main.merge(seller_map, on='order number', how='left')
            df_main = df_main.merge(earn_map, on='order number', how='left')
            df_main = df_main.merge(ship_agg, on='order number', how='left')
            df_main.fillna(0, inplace=True)

            # åˆ†æ‘Š
            order_totals = df_main.groupby('order number')['item subtotal'].transform('sum')
            df_main['ratio'] = np.where(order_totals != 0, df_main['item subtotal'] / order_totals, 0.0)

            for col in ['Shipping label-Earning data', 'Shipping label-underpay', 'Shipping label-overpay',
                        'Shipping label-Return', 'Shipping label-Regular']:
                if col in df_main.columns: df_main[col] = df_main[col] * df_main['ratio']

            col_mapping = {
                'transaction creation date': 'order date',
                'item subtotal': 'revenue',
                'shipping and handling': 'Shipping and handling',
                'seller collected tax': 'Seller collected tax',
                'ebay collected tax': 'eBay collected tax',
                'final value fee - fixed': 'Final Value Fee - fixed',
                'final value fee - variable': 'Final Value Fee - variable',
                'regulatory operating fee': 'Regulatory operating fee',
                'international fee': 'International fee',
                'promoted listings fee': 'Promoted Listings fee'
            }
            df_main.rename(columns=col_mapping, inplace=True)

            # SKU å±•å¹³
            sku_parts = []
            for i in range(1, 11):
                s_col, q_col = f'p_sku{i}', f'p_quantity{i}'
                target_s, target_q, target_qp = f'sku{i}', f'qty{i}', f'qtyp{i}'

                if s_col not in df_main.columns:
                    df_main[target_s] = '';
                    df_main[target_q] = 0;
                    df_main[target_qp] = 0;
                    continue

                df_main[target_s] = df_main[s_col]
                df_main[target_q] = self._safe_float(df_main[q_col])
                df_main[target_qp] = df_main[target_q] * df_main['quantity']

                mask = df_main[target_s].notna() & (df_main[target_s] != '')
                part = df_main.loc[mask, target_s].astype(str) + "." + df_main.loc[mask, target_q].astype(int).astype(
                    str)
                sku_parts.append(part)

            if sku_parts:
                df_parts = pd.concat(sku_parts, axis=1)
                df_main['full sku'] = df_parts.apply(lambda x: "+".join(x.dropna()), axis=1)
            else:
                df_main['full sku'] = ''

            # [Mod] å¼ºåˆ¶æ—¥æœŸæ ‡å‡†åŒ– (YYYY-MM-DD)
            df_main['order date'] = df_main['order date'].apply(self._normalize_date)

            # --- å…¥åº“ ---
            report(0.90, f"ğŸ’¾ å››ç»´å»é‡å¹¶åŒæ­¥ ({len(df_main)} è¡Œ)...")

            df_final = pd.DataFrame()
            for c in self.output_cols:
                if c in df_main.columns:
                    df_final[c] = df_main[c]
                else:
                    df_final[c] = 0 if 'fee' in c.lower() or 'label' in c.lower() else ''

            staging = "Data_Clean_Log_Staging"
            target = "Data_Clean_Log"

            with self.db.atomic_transaction() as conn:
                df_final.to_sql(staging, conn, if_exists='replace', index=False,
                                dtype={c: Text() for c in df_final.columns})

                conn.execute(text(f"CREATE INDEX idx_order ON `{staging}` (`order number`(30))"))
                conn.execute(text(f"CREATE INDEX idx_item ON `{staging}` (`item id`(30))"))
                conn.execute(text(f"CREATE INDEX idx_date ON `{staging}` (`order date`(10))"))

                exists = conn.execute(text(f"SHOW TABLES LIKE '{target}'")).first()
                if not exists:
                    conn.execute(text(f"RENAME TABLE `{staging}` TO `{target}`"))
                else:
                    del_sql = f"""
                        DELETE T1 FROM `{target}` T1 
                        INNER JOIN `{staging}` T2 
                        ON T1.`order number` = T2.`order number`
                        AND T1.`seller` = T2.`seller`
                        AND COALESCE(T1.`item id`, '') = COALESCE(T2.`item id`, '')
                        AND COALESCE(T1.`action`, '') = COALESCE(T2.`action`, '')
                    """
                    conn.execute(text(del_sql))

                    cols = ", ".join([f"`{c}`" for c in self.output_cols])
                    ins_sql = f"INSERT INTO `{target}` ({cols}) SELECT {cols} FROM `{staging}`"
                    conn.execute(text(ins_sql))
                    conn.execute(text(f"DROP TABLE `{staging}`"))

                conn.execute(text("TRUNCATE TABLE `Data_Transaction`"))
                conn.execute(text("TRUNCATE TABLE `Data_Order_Earning`"))

            report(1.0, " ETL è½¬æ¢æµç¨‹å®Œæˆ (æ—¥æœŸå·²è§„èŒƒåŒ–)ï¼")
            return "Success"

        except Exception as e:
            self.logger.error(f"Transformer Error: {e}")
            raise e
==================== END FILE: core/services/etl/transformer.py ====================


==================== START FILE: core/services/etl/repository.py ====================
# core/services/etl/repository.py
"""
æ–‡ä»¶è¯´æ˜: ETL æ•°æ®ä»“åº“ (ETL Repository)
ä¸»è¦åŠŸèƒ½:
1. æä¾›å¯¹åŸå§‹è¡¨ (Data_Transaction, Data_Order_Earning) çš„è¯»å–ä¸æ¸…ç©ºæ¥å£ã€‚
2. æä¾›å¯¹ç»“æœè¡¨ (Data_Clean_Log) çš„æŒ‰æ—¥æœŸèŒƒå›´æŸ¥è¯¢æ¥å£ã€‚
3. **å…³é”®**: åœ¨è¯»å– Clean Log æ—¶æ‰§è¡Œä¸¥æ ¼çš„ç±»å‹è½¬æ¢ (Numeric/Datetime)ï¼Œé˜²æ­¢ä¸‹æ¸¸åˆ†ææŠ¥é”™ã€‚
"""

import pandas as pd
from typing import Optional, List
from datetime import date
from sqlalchemy import text

from core.components.db.client import DBClient


class ETLRepository:
    # å®šä¹‰éœ€è¦å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼çš„åˆ— (é˜²æ­¢ Pandas å°†å…¶è¯†åˆ«ä¸º Object/String)
    NUMERIC_COLS = [
        'quantity', 'revenue', 'profit', 'Shipping and handling',
        'Seller collected tax', 'eBay collected tax', 'Refund',
        'Shipping label-Earning data', 'Shipping label-underpay',
        'Shipping label-overpay', 'Shipping label-Return',
        'Final Value Fee - fixed', 'Final Value Fee - variable',
        'Promoted Listings fee'
    ]

    @staticmethod
    def get_transactions_by_date(start_date: date, end_date: date) -> pd.DataFrame:
        """
        [æ ¸å¿ƒæŸ¥è¯¢] æŸ¥è¯¢æ¸…æ´—å (Clean Log) çš„æ•°æ®
        ç”¨äºï¼šè´¢åŠ¡åˆ†æã€é”€é‡ç»Ÿè®¡ã€å›¾è¡¨å¯è§†åŒ–ã€‚
        """
        # ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢é˜²æ­¢æ³¨å…¥ï¼Œå¹¶åˆ©ç”¨ç´¢å¼•åŠ é€Ÿ
        sql = """
              SELECT * \
              FROM Data_Clean_Log
              WHERE `order date` >= :start_date
                AND `order date` <= :end_date \
              """
        params = {
            "start_date": start_date.strftime("%Y-%m-%d"),
            "end_date": end_date.strftime("%Y-%m-%d")
        }

        df = DBClient.read_df(sql, params)
        if df.empty:
            return df

        # --- [å…³é”®é˜²å¾¡] å¼ºåˆ¶ç±»å‹è½¬æ¢ ---
        # 1. æ•°å€¼åˆ—æ¸…æ´— (ç©ºå€¼å¡«0)
        for c in ETLRepository.NUMERIC_COLS:
            # å…¼å®¹åˆ—åå¤§å°å†™å·®å¼‚ (è™½ç„¶ Clean Log åº”è¯¥æ˜¯æ ‡å‡†çš„ï¼Œä½†é˜²ä¸€æ‰‹)
            # è¿™é‡Œæˆ‘ä»¬å‡è®¾æ•°æ®åº“åˆ—åæ˜¯æ ‡å‡†çš„ï¼Œæˆ–è€…åœ¨è¯»å–åç»Ÿä¸€è½¬å°å†™å¤„ç†
            # ä¸ºäº†ç¨³å¦¥ï¼Œæˆ‘ä»¬éå† df.columns åŒ¹é…
            match_col = next((col for col in df.columns if col.lower() == c.lower()), None)
            if match_col:
                df[match_col] = pd.to_numeric(df[match_col], errors='coerce').fillna(0.0)

        # 2. æ—¥æœŸåˆ—æ¸…æ´—
        if "order date" in df.columns:
            df["order date"] = pd.to_datetime(df["order date"], errors='coerce')

        return df

    @staticmethod
    def truncate_raw_tables() -> None:
        """
        [æ¸…ç†] æ¸…ç©ºåŸå§‹æ•°æ®è¡¨
        é€šå¸¸åœ¨ Ingest æµç¨‹å¼€å§‹å‰è°ƒç”¨ï¼Œç¡®ä¿æœ¬æ¬¡å¤„ç†ç¯å¢ƒå¹²å‡€ã€‚
        """
        with DBClient.atomic_transaction() as conn:
            conn.execute(text("TRUNCATE TABLE `Data_Transaction`"))
            conn.execute(text("TRUNCATE TABLE `Data_Order_Earning`"))

    @staticmethod
    def get_raw_transaction_data() -> pd.DataFrame:
        """è·å–åŸå§‹äº¤æ˜“æ•°æ® (ç”¨äº Parser/Transformer)"""
        return DBClient.read_df("SELECT * FROM Data_Transaction")

    @staticmethod
    def get_raw_earning_data() -> pd.DataFrame:
        """è·å–åŸå§‹èµ„é‡‘æ•°æ®"""
        return DBClient.read_df("SELECT * FROM Data_Order_Earning")
==================== END FILE: core/services/etl/repository.py ====================


==================== START FILE: core/services/inventory/service.py ====================
# core/services/inventory/service.py
"""
æ–‡ä»¶è¯´æ˜: åº“å­˜ç®¡ç†ä¸šåŠ¡æœåŠ¡ (Inventory Service) - User Date Logic
ä¸»è¦åŠŸèƒ½:
1. æ ¡éªŒä¸Šä¼ çš„åº“å­˜ CSV æ–‡ä»¶ã€‚
2. [Mod] åŠ¨æ€ä¿®æ”¹æ•°æ®åº“è¡¨ç»“æ„: ç›´æ¥ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„æ—¥æœŸ (YYYY-MM-DD) ä½œä¸ºåˆ—åã€‚
3. é«˜æ€§èƒ½æ‰¹é‡åŒæ­¥åº“å­˜æ•°æ®ã€‚
4. æ³¨å†Œæ–° SKUã€‚
"""

import pandas as pd
from typing import Tuple, List, Dict
from sqlalchemy import text
from sqlalchemy.types import Integer, String
# [Removed] ä¸éœ€è¦ pandas.tseries.offsets äº†

from config.settings import settings
from core.components.db.client import DBClient
from core.sys.logger import get_logger
from core.sys.context import get_current_user
from core.services.inventory.repository import InventoryRepository


class InventoryService:

    def __init__(self):
        self.logger = get_logger("InventoryService")
        self.repo = InventoryRepository()
        self.table_inv = "Data_Inventory"

    def _normalize_date_str(self, date_val: str) -> str:
        """
        [Mod] æ—¥æœŸè§„èŒƒåŒ– (å¿ å®è®°å½•)
        ç›´æ¥å°†ç”¨æˆ·è¾“å…¥çš„æ—¥æœŸè½¬æ¢ä¸ºæ ‡å‡† ISO æ ¼å¼ YYYY-MM-DDã€‚
        ä¸è¿›è¡Œæœˆæœ«æ¨ç®—ï¼Œä»¥ç”¨æˆ·å®é™…å½•å…¥æ—¶é—´ä¸ºå‡†ã€‚
        """
        try:
            # æ— è®ºæ˜¯ datetime å¯¹è±¡è¿˜æ˜¯å­—ç¬¦ä¸²ï¼Œç»Ÿä¸€è½¬ä¸º YYYY-MM-DD
            dt = pd.to_datetime(str(date_val).strip())
            return dt.strftime("%Y-%m-%d")
        except Exception as e:
            self.logger.warning(f"æ—¥æœŸæ ¼å¼åŒ–å¤±è´¥ [{date_val}]: {e}ï¼Œå›é€€ä½¿ç”¨åŸå§‹å€¼")
            return str(date_val)

    def validate_csv(self, file_path: str) -> Tuple[bool, List[str], pd.DataFrame]:
        """[æ ¡éªŒ] æ£€æŸ¥ä¸Šä¼ çš„åº“å­˜ CSV æ–‡ä»¶"""
        self.logger.info(f"æ­£åœ¨æ ¡éªŒåº“å­˜æ–‡ä»¶...")
        try:
            df = pd.read_csv(file_path, dtype=str, encoding='utf-8-sig', on_bad_lines='skip')
            df.columns = [str(c).strip().lower() for c in df.columns]

            sku_col = next((c for c in df.columns if 'sku' in c), None)
            qty_col = next((c for c in df.columns if any(k in c for k in ['qty', 'quantity', 'amount'])), None)

            if not sku_col or not qty_col:
                return False, ["CSV æ ¼å¼é”™è¯¯: æœªæ‰¾åˆ° 'SKU' æˆ– 'Quantity' åˆ—"], pd.DataFrame()

            df = df[[sku_col, qty_col]].rename(columns={sku_col: 'SKU', qty_col: 'Quantity'})
            df['SKU'] = df['SKU'].astype(str).str.strip().str.upper()
            df = df[~df['SKU'].isin(['NAN', 'NONE', '', 'NULL'])]
            df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce').fillna(0).astype(int)

            valid_skus = set(self.repo.get_valid_skus())
            upload_skus = set(df['SKU'].unique())

            unknown = list(upload_skus - valid_skus)

            if not unknown:
                return True, [], df
            else:
                return False, unknown, df

        except Exception as e:
            self.logger.error(f"åº“å­˜æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            return False, [f"è¯»å–é”™è¯¯: {str(e)}"], pd.DataFrame()

    def sync_inventory_to_db(self, df: pd.DataFrame, target_date_input: str) -> str:
        """
        [æ‰§è¡Œ] åŒæ­¥åº“å­˜
        :param target_date_input: ç”¨æˆ·åœ¨ UI é€‰æ‹©çš„æ—¥æœŸ (å¦‚ '2025-06-15')
        """
        if df.empty: return "æ•°æ®ä¸ºç©ºï¼Œæœªæ‰§è¡Œã€‚"

        user = get_current_user()

        # 1. [Mod] ä»…åšæ ¼å¼åŒ–ï¼Œä¸åšåç§»
        norm_col_name = self._normalize_date_str(target_date_input)

        self.logger.info(f"å¼€å§‹åŒæ­¥åº“å­˜: {target_date_input} -> åˆ—å: [{norm_col_name}]",
                         extra={"action": "INVENTORY_SYNC", "user": user})

        # 2. åŠ¨æ€ DDL: ç¡®ä¿è¯¥æ—¥æœŸåˆ—å­˜åœ¨
        self._ensure_column_exists(norm_col_name)

        # 3. å†™å…¥æ•°æ®
        temp_table = "Temp_Inventory_Upload"
        try:
            with DBClient.get_engine().begin() as conn:
                df.to_sql(temp_table, conn, if_exists='replace', index=False,
                          dtype={'SKU': String(100), 'Quantity': Integer()})

                # æ›´æ–°ç›®æ ‡åˆ—
                sql = text(f"""
                    UPDATE `{self.table_inv}` T1
                    INNER JOIN `{temp_table}` T2 ON T1.SKU = T2.SKU
                    SET T1.`{norm_col_name}` = T2.Quantity
                """)
                res = conn.execute(sql)
                updated_rows = res.rowcount

                conn.execute(text(f"DROP TABLE IF EXISTS `{temp_table}`"))

            msg = f"åº“å­˜åŒæ­¥å®Œæˆã€‚æ•°æ®å·²å†™å…¥åˆ— [{norm_col_name}]ï¼Œæ›´æ–°äº† {updated_rows} æ¡è®°å½•ã€‚"
            self.logger.info(msg)
            return msg

        except Exception as e:
            self.logger.error(f"åº“å­˜åŒæ­¥å¤±è´¥: {e}")
            raise e

    def _ensure_column_exists(self, col_name: str):
        """æ£€æŸ¥æ•°æ®åº“åˆ—æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™æ‰§è¡Œ ALTER TABLE"""
        sql = """
              SELECT COUNT(*) \
              FROM information_schema.COLUMNS
              WHERE TABLE_SCHEMA = :db \
                AND TABLE_NAME = :table \
                AND COLUMN_NAME = :col \
              """
        params = {"db": settings.DB_NAME, "table": self.table_inv, "col": col_name}

        exists = DBClient.read_df(sql, params).iloc[0, 0] > 0

        if not exists:
            self.logger.info(f"Schema è‡ªåŠ¨æ‰©å±•: æ·»åŠ åˆ— [{col_name}]")
            # ç¡®ä¿åˆ—åè¢«åå¼•å·åŒ…è£¹ï¼Œé˜²æ­¢ç‰¹æ®Šå­—ç¬¦æŠ¥é”™
            safe_col = f"`{col_name.replace('`', '')}`"
            DBClient.execute_stmt(f"ALTER TABLE `{self.table_inv}` ADD COLUMN {safe_col} INT DEFAULT 0")

    def register_sku(self, sku_data: Dict) -> bool:
        try:
            cost = float(sku_data.get('Cost', 0))
            freight = float(sku_data.get('Freight', 0))
            sku_data['Cog'] = round(cost + freight, 2)
            sku_data['SKU'] = str(sku_data['SKU']).strip().upper()

            self.repo.create_sku_transactional(sku_data)
            self.logger.info(f"æ–° SKU æ³¨å†ŒæˆåŠŸ: {sku_data['SKU']}")
            return True
        except Exception as e:
            self.logger.error(f"SKU æ³¨å†Œå¤±è´¥: {e}")
            raise e
==================== END FILE: core/services/inventory/service.py ====================


==================== START FILE: core/services/inventory/__init__.py ====================

==================== END FILE: core/services/inventory/__init__.py ====================


==================== START FILE: core/services/inventory/repository.py ====================
# core/services/inventory/repository.py
"""
æ–‡ä»¶è¯´æ˜: åº“å­˜ä¸æ¡£æ¡ˆæ•°æ®ä»“åº“ (Inventory Repository)
ä¸»è¦åŠŸèƒ½:
1. è´Ÿè´£ Data_COGS (æ¡£æ¡ˆ) å’Œ Data_Inventory (åº“å­˜) çš„ CRUD æ“ä½œã€‚
2. æä¾› "è·å–æœ€æ–°åº“å­˜æœˆä»½" çš„æ™ºèƒ½é€»è¾‘ã€‚
3. æä¾›åŸå­æ€§çš„ SKU åˆ›å»ºæ¥å£ã€‚
"""

from typing import List, Dict, Any, Optional
import pandas as pd
from sqlalchemy import text

from core.components.db.client import DBClient


class InventoryRepository:

    def get_all_cogs(self) -> pd.DataFrame:
        """è·å–æ‰€æœ‰ SKU çš„æˆæœ¬ä¿¡æ¯ (ç”¨äºåˆ©æ¶¦è®¡ç®—)"""
        return DBClient.read_df("SELECT * FROM Data_COGS")

    def get_valid_skus(self) -> List[str]:
        """è·å–ç³»ç»Ÿå†…æ‰€æœ‰æœ‰æ•ˆçš„ SKU åˆ—è¡¨ (ç”¨äºæ ¡éªŒå’Œçº é”™)"""
        df = DBClient.read_df("SELECT DISTINCT SKU FROM Data_COGS")
        if df.empty:
            return []
        # å½’ä¸€åŒ–ï¼šå»ç©ºã€å¤§å†™
        return df["SKU"].dropna().astype(str).str.strip().str.upper().tolist()

    def get_inventory_latest(self) -> pd.DataFrame:
        """
        [æ™ºèƒ½é€»è¾‘] è·å–å½“å‰æœ€æ–°çš„åº“å­˜æ•°æ®
        é€»è¾‘ï¼šæ‰«æ Data_Inventory è¡¨ç»“æ„ï¼Œæ‰¾åˆ°æ‰€æœ‰ç±»ä¼¼ 'YYYY-MM' çš„åˆ—ï¼Œå–æœ€å¤§çš„ä¸€ä¸ªã€‚
        """
        # 1. è¯»è¡¨ç»“æ„ (åªè¯»0è¡Œï¼Œæå¿«)
        schema_df = DBClient.read_df("SELECT * FROM Data_Inventory LIMIT 0")

        # 2. ç­›é€‰åŒ…å« '-' çš„åˆ— (å‡è®¾æœˆä»½åˆ—æ ¼å¼ä¸º 2025-10)
        date_cols = [c for c in schema_df.columns if '-' in str(c)]

        if not date_cols:
            # å¦‚æœæ²¡æœ‰æœˆä»½åˆ—ï¼Œè¿”å›ä»…åŒ…å« SKU çš„ç©ºè¡¨
            return pd.DataFrame(columns=["SKU", "Quantity"])

        # 3. æ’åºå–æœ€æ–°
        latest_col = sorted(date_cols)[-1]

        # 4. åŠ¨æ€æ„é€ æŸ¥è¯¢
        # æ³¨æ„ï¼šåˆ—åå¿…é¡»åŠ åå¼•å·ï¼Œé˜²æ­¢ '2025-10' è¢«è¯†åˆ«ä¸ºå‡æ³•
        sql = f"SELECT SKU, `{latest_col}` as Quantity FROM Data_Inventory"
        return DBClient.read_df(sql)

    def get_distinct_values(self, column: str) -> List[str]:
        """è·å–åˆ†ç±»ä¸‹æ‹‰é€‰é¡¹ (Category, SubCategory, Type)"""
        allowed = ['Category', 'SubCategory', 'Type']
        if column not in allowed:
            return []

        sql = f"SELECT DISTINCT `{column}` FROM Data_COGS WHERE `{column}` IS NOT NULL AND `{column}` != '' ORDER BY `{column}`"
        df = DBClient.read_df(sql)
        return df[column].tolist()

    def create_sku_transactional(self, sku_data: dict) -> bool:
        """
        [åŸå­äº‹åŠ¡] åˆ›å»ºæ–° SKU
        å¿…é¡»åŒæ—¶åœ¨ Data_COGS æ’å…¥æ¡£æ¡ˆï¼Œå¹¶åœ¨ Data_Inventory æ’å…¥åº“å­˜è¡Œ (å†å²æœˆä»½å¡«0)ã€‚
        """
        try:
            with DBClient.atomic_transaction() as conn:
                # 1. æ’å…¥ Data_COGS
                cols = ", ".join(sku_data.keys())
                params_str = ", ".join([f":{k}" for k in sku_data.keys()])
                sql_cogs = text(f"INSERT INTO Data_COGS ({cols}) VALUES ({params_str})")
                conn.execute(sql_cogs, sku_data)

                # 2. æ’å…¥ Data_Inventory (è‡ªåŠ¨è¡¥å…¨å†å²æœˆä»½ä¸º 0)
                # å…ˆè·å–å½“å‰ Inventory è¡¨çš„æ‰€æœ‰åˆ—
                schema_df = pd.read_sql("SELECT * FROM Data_Inventory LIMIT 0", conn)
                inv_cols = schema_df.columns.tolist()

                # æ„é€ æ’å…¥å€¼
                inv_vals = []
                for col in inv_cols:
                    if col.upper() == 'SKU':
                        inv_vals.append(f"'{sku_data['SKU']}'")
                    else:
                        inv_vals.append("0")  # å†å²æœˆä»½é»˜è®¤å¡« 0

                col_str = ", ".join([f"`{c}`" for c in inv_cols])
                val_str = ", ".join(inv_vals)

                sql_inv = text(f"INSERT INTO Data_Inventory ({col_str}) VALUES ({val_str})")
                conn.execute(sql_inv)

            return True
        except Exception as e:
            # å¼‚å¸¸ä¼šå‘ä¸Šä¼ é€’ï¼Œç”± Service å±‚æ•è·è®°å½•æ—¥å¿—
            raise e
==================== END FILE: core/services/inventory/repository.py ====================


==================== START FILE: core/services/diagnostics/sku.py ====================
# core/services/diagnostics/sku.py
"""
æ–‡ä»¶è¯´æ˜: SKU ä¾›åº”é“¾å¥åº·åº¦è¯Šæ–­ (SKU Diagnostics)
ä¸»è¦åŠŸèƒ½:
1. åŸºäº BCG çŸ©é˜µæ¨¡å‹å¯¹ SKU è¿›è¡Œåˆ†å±‚ (æ˜æ˜Ÿ/ç°é‡‘ç‰›/ç˜¦ç‹—/æ½œåŠ›)ã€‚
2. è®¡ç®—åº“å­˜å‘¨è½¬å¤©æ•° (DOS)ï¼Œè¯†åˆ«æ–­è´§é£é™©å’Œæ»é”€ç§¯å‹ã€‚
3. ç›‘æ§å¼‚å¸¸æŒ‡æ ‡ (é«˜é€€è´§ç‡ã€å¹¿å‘Šé»‘æ´)ã€‚
"""

import pandas as pd
import numpy as np
from core.services.diagnostics.base import BaseDiagnostician

class SkuDiagnostician(BaseDiagnostician):

    def __init__(self, metrics_cur: dict, metrics_prev: dict, inventory_map: dict):
        super().__init__(metrics_cur, metrics_prev)
        self.inventory_map = inventory_map  # SKU ç‹¬æœ‰çš„åº“å­˜æ•°æ® {SKU: Qty}

    def _prepare_features(self) -> pd.DataFrame:
        """ç‰¹å¾å·¥ç¨‹: å°†å­—å…¸è½¬ä¸º DataFrame å¹¶è®¡ç®—è¡ç”ŸæŒ‡æ ‡"""
        data = []
        for sku, cur in self.m_cur.items():
            # 1. åŸºç¡€è´¢åŠ¡
            sales = cur.get("net_qty", 0)
            rev = cur.get("total_rev", 0)
            profit = cur.get("profit", 0)
            margin = profit / rev if rev > 0 else 0

            # 2. é£é™©æŒ‡æ ‡
            # é€€è´§ç‡ = (Return + Request + Claim) / Total
            bad_qty = (cur.get("return_qty", 0) + cur.get("request_qty", 0) + cur.get("claim_qty", 0))
            total_qty = cur.get("total_qty", 0)
            ret_rate = bad_qty / total_qty if total_qty > 0 else 0

            # å¹¿å‘Šè´¹æ¯”
            ad_cost = cur.get("net_ad_fee", 0)
            acos = ad_cost / rev if rev > 0 else 0

            # 3. è¶‹åŠ¿æŒ‡æ ‡ (ç¯æ¯”)
            prev_sales = self.m_prev.get(sku, {}).get("net_qty", 0)
            if prev_sales > 0:
                growth = (sales - prev_sales) / prev_sales
            else:
                growth = 1.0 if sales > 0 else 0.0

            # 4. ä¾›åº”é“¾æŒ‡æ ‡ (DOS)
            curr_inv = self.inventory_map.get(sku, 0)
            # å‡è®¾æœˆé”€å‡æ‘Šåˆ°30å¤©
            daily_sales = sales / 30 if sales > 0 else 0
            # è‹¥æ— é”€é‡ä½†æœ‰åº“å­˜ï¼ŒDOS è®¾ä¸ºæå€¼ 999
            dos = curr_inv / daily_sales if daily_sales > 0 else 999

            data.append({
                "SKU": sku, "Sales": sales, "Margin": margin,
                "ACOS": acos, "ReturnRate": ret_rate,
                "Growth": growth, "DOS": dos, "Inventory": curr_inv
            })

        return pd.DataFrame(data)

    def diagnose(self) -> pd.DataFrame:
        df = self._prepare_features()
        if df.empty: return df

        # åŠ¨æ€åˆ†ä½æ•°è®¡ç®— (BCGçŸ©é˜µæ ¸å¿ƒé€»è¾‘)
        # é¿å…å…¨ä¸º0å¯¼è‡´çš„æŠ¥é”™
        q_sales_high = df["Sales"].quantile(0.8) if not df["Sales"].empty else 0
        q_sales_low = df["Sales"].quantile(0.2) if not df["Sales"].empty else 0
        q_margin_high = df["Margin"].quantile(0.8) if not df["Margin"].empty else 0
        q_margin_low = df["Margin"].quantile(0.2) if not df["Margin"].empty else 0

        results = []
        for _, row in df.iterrows():
            tags, sugs = [], []

            # --- 1. BCG äº§å“çŸ©é˜µ ---
            if row["Sales"] > q_sales_high:
                if row["Margin"] > q_margin_high:
                    tags.append("ğŸŒŸæ˜æ˜Ÿ(Star)")
                    sugs.append("æ ¸å¿ƒèµ„äº§: ç¡®ä¿åº“å­˜ä¼˜å…ˆä¾›åº”ï¼Œå»ºç«‹é˜²å¾¡å£å’ã€‚")
                else:
                    tags.append("ğŸ®ç°é‡‘ç‰›(Cow)")
                    sugs.append("èµ„é‡‘æ”¯æŸ±: ä¸¥æ§æˆæœ¬ï¼Œç»´æŒç°æœ‰è§„æ¨¡ï¼Œä½œä¸ºå¼•æµå…¥å£ã€‚")
            elif row["Sales"] < q_sales_low:
                if row["Margin"] < q_margin_low:
                    tags.append("ğŸ•ç˜¦ç‹—(Dog)")
                    sugs.append("è´Ÿèµ„äº§: å»ºè®®ç«‹å³åœæ­¢è¡¥è´§ï¼Œè¿›è¡Œæ¸…ä»“æˆ–æ·˜æ±°ã€‚")
                else:
                    tags.append("ğŸ’æ½œåŠ›(Question)")
                    sugs.append("è§‚å¯ŸæœŸ: ä¼˜åŒ–Listingè´¨é‡ï¼Œå°è¯•ç²¾å‡†å¹¿å‘ŠæŠ•æ”¾æµ‹è¯•ã€‚")
            else:
                tags.append("ğŸ”¹å¹³åº¸(Average)")
                sugs.append("ç»´æŒç°çŠ¶: å®šæœŸå¤ç›˜ï¼Œå¯»æ‰¾çªç ´æœºä¼šã€‚")

            # --- 2. è¶‹åŠ¿ç›‘æ§ ---
            if row["Growth"] > 0.2:
                tags.append("ğŸš€é£™å‡")
            elif row["Growth"] < -0.2:
                tags.append("ğŸ“‰è¡°é€€")

            # --- 3. é£é™©é¢„è­¦ ---
            if row["ReturnRate"] > 0.1:
                tags.append("âš ï¸é«˜é€€è´§")
                sugs.append("å“è´¨çº¢çº¿: ç«‹å³æ£€æŸ¥äº§å“è´¨é‡æˆ–Listingæè¿°å‡†ç¡®æ€§ã€‚")

            if row["ACOS"] > 0.4:
                tags.append("ğŸ”¥å¹¿å‘Šé»‘æ´")
                sugs.append("ROIè¿‡ä½: ç¼©å‡å¹¿å‘Šé¢„ç®—ï¼Œä¼˜åŒ–å…³é”®è¯åŒ¹é…ã€‚")

            # --- 4. åº“å­˜å¥åº·åº¦ (DOS) ---
            if row["DOS"] < 7:
                tags.append("ğŸš¨æ–­è´§é¢„è­¦")
                sugs.append("ç´§æ€¥è¡¥è´§: åº“å­˜ä¸è¶³æ”¯æ’‘ä¸€å‘¨é”€é‡ï¼Œéœ€ç©ºè¿è¡¥è´§ã€‚")
            elif row["DOS"] > 180:
                tags.append("ğŸ“¦æ»é”€ç§¯å‹")
                sugs.append("å»åº“å­˜: åº“é¾„è¿‡é•¿ï¼Œå»ºè®®ç«™å¤–Dealæˆ–é™ä»·æ¸…ä»“ã€‚")

            results.append({
                "SKU": row["SKU"],
                "è¯Šæ–­æ ‡ç­¾": " | ".join(tags),
                "AIè¿è¥å»ºè®®": " ".join(sugs),
                "é”€é‡": int(row["Sales"]),
                "åˆ©æ¶¦ç‡": f"{row['Margin']:.2%}",
                "ACOS": f"{row['ACOS']:.2%}",
                "å‘¨è½¬å¤©æ•°": round(row["DOS"], 1),
                "é€€è´§ç‡": f"{row['ReturnRate']:.2%}",
                "å½“å‰åº“å­˜": int(row["Inventory"])
            })

        return pd.DataFrame(results)

    @staticmethod
    def get_tag_definitions() -> list:
        return [
            "ğŸ“˜ SKU ä¾›åº”é“¾è¯Šæ–­ä½“ç³»è¯´æ˜ (Supply Chain Diagnostics):",
            "=================================================================================",
            "1. ã€BCG çŸ©é˜µå®šä½ã€‘ åŸºäºæœ¬æœŸæ•°æ®åˆ†ä½æ•°(Quantile)åŠ¨æ€åˆ’åˆ†:",
            "   - ğŸŒŸ æ˜æ˜Ÿ (Star)     [é”€é‡Top20% + åˆ©æ¶¦Top20%] : ä¼ä¸šå¢é•¿å¼•æ“ï¼Œéœ€èµ„æºå€¾æ–œã€‚",
            "   - ğŸ® ç°é‡‘ç‰› (Cow)    [é”€é‡Top20% + åˆ©æ¶¦éTop]  : å¸‚åœºå æœ‰ç‡é«˜ï¼Œæä¾›ç¨³å®šç°é‡‘æµã€‚",
            "   - ğŸ’ æ½œåŠ› (Question) [é”€é‡Bottom20% + åˆ©æ¶¦Top] : é«˜æ¯›åˆ©ä½†åœ¨å­µåŒ–æœŸï¼Œéœ€æµé‡æ‰¶æŒã€‚",
            "   - ğŸ• ç˜¦ç‹— (Dog)      [é”€é‡Bottom20% + åˆ©æ¶¦Bottom] : æ»é”€ä¸”äºæŸï¼Œåº”å‰¥ç¦»èµ„äº§ã€‚",
            "",
            "2. ã€åº“å­˜å¥åº·åº¦ (DOS)ã€‘ Days of Supply:",
            "   - ğŸš¨ æ–­è´§é¢„è­¦ (DOS < 7å¤©): æé«˜ç¼ºè´§é£é™©ï¼Œå¯èƒ½å¯¼è‡´Listingé™æƒã€‚",
            "   - ğŸ“¦ æ»é”€ç§¯å‹ (DOS > 180å¤©): èµ„é‡‘å‘¨è½¬æ•ˆç‡æä½ï¼Œéœ€è®¡æè·Œä»·å‡†å¤‡ã€‚",
            "",
            "3. ã€é£é™©æ§åˆ¶ã€‘:",
            "   - ğŸ”¥ å¹¿å‘Šé»‘æ´ (ACOS > 40%): å¹¿å‘ŠèŠ±è´¹è¶…è¿‡è­¦æˆ’çº¿ï¼Œå³ä½¿å‡ºå•ä¹Ÿå¯èƒ½äºæŸã€‚",
            "   - âš ï¸ é«˜é€€è´§ (Rate > 10%): äº§å“å­˜åœ¨è´¨é‡æˆ–æè¿°ç¼ºé™·ï¼Œç”šè‡³å¯¼è‡´å°å·é£é™©ã€‚",
            "================================================================================="
        ]
==================== END FILE: core/services/diagnostics/sku.py ====================


==================== START FILE: core/services/diagnostics/crm.py ====================
# core/services/diagnostics/crm.py
"""
æ–‡ä»¶è¯´æ˜: å®¢æˆ·ä»·å€¼ä¸é£é™©è¯Šæ–­ (CRM Diagnostics)
ä¸»è¦åŠŸèƒ½:
1. åŸºäº RFM æ¨¡å‹å¯¹å®¢æˆ·è¿›è¡Œä»·å€¼åˆ†å±‚ (é²¸é±¼/é“ç²‰/æ²‰ç¡)ã€‚
2. è¯†åˆ«é«˜é£é™©å®¢æˆ· (æƒ¯æ€§é€€è´§/çº çº·å‘èµ·è€…)ã€‚
3. è¾“å‡ºè¿è¥å»ºè®® (æ‹‰é»‘/ç»´æŠ¤/å¬å›)ã€‚
"""

import pandas as pd
from .base import BaseDiagnostician


class CustomerDiagnostician(BaseDiagnostician):

    def __init__(self, metrics_cur: pd.DataFrame, metrics_prev: pd.DataFrame = None, **kwargs):
        """
        åˆå§‹åŒ–è¯Šæ–­å™¨
        :param metrics_cur: æœ¬æœŸ RFM æ•°æ® (DataFrame)
        :param metrics_prev: ä¸ŠæœŸæ•°æ® (CRM æš‚æœªä½¿ç”¨ï¼Œè®¾ä¸º None)
        """
        # æ˜¾å¼èµ‹å€¼ï¼Œå› ä¸ºåŸºç±»æœŸæœ›çš„æ˜¯ dictï¼Œä½† CRM ä¼ å…¥çš„æ˜¯ DataFrame
        self.m_cur = metrics_cur
        self.m_prev = metrics_prev
        self.kwargs = kwargs

    def diagnose(self) -> pd.DataFrame:
        df = self.m_cur.copy()
        if df.empty: return df

        # è®¡ç®—åŠ¨æ€é˜ˆå€¼ (Top 20%)
        # é¿å…ç©ºæ•°æ®æŠ¥é”™
        q_net_m_high = df["Net_Monetary"].quantile(0.8) if not df["Net_Monetary"].empty else 0

        results = []

        for _, row in df.iterrows():
            tags, sugs = [], []

            # æå–æ ¸å¿ƒæŒ‡æ ‡
            user = row["buyer username"]
            freq = row["Frequency"]
            net_monetary = row["Net_Monetary"]  # æ ¸å¿ƒæŒ‡æ ‡ï¼šå‡€å€¼
            gross_monetary = row["Gross_Monetary"]
            recency = row["Recency"]
            ret_rate = row["ReturnRate"]
            dispute = row["DisputeCount"]
            aov = row["AOV"]

            is_target_user = False

            # åŸºç¡€å®‰å…¨åˆ¤å®š: é€€è´§ç‡ < 20% ä¸” æ— çº çº·
            is_safe_user = (ret_rate < 0.2) and (dispute == 0)

            # =========================================================
            # 1. åŠ£è´¨/é£é™©ç»„ (Risk Group)
            # =========================================================

            # è§„åˆ™A: çº çº·é»‘åå•
            if dispute > 0:
                tags.append("çº çº·å‘èµ·è€…")
                sugs.append("é«˜å±é¢„è­¦: æ›¾å‘èµ·Payment Disputeï¼Œå»ºè®®æ‹‰é»‘ã€‚")
                is_target_user = True

            # è§„åˆ™B: æƒ¯æ€§é€€è´§ (ä¹°å¾—å¤šé€€å¾—å¤š)
            if freq >= 2 and ret_rate > 0.3:
                tags.append("âš ï¸æƒ¯æ€§é€€è´§")
                sugs.append(f"åˆ©æ¶¦æ€æ‰‹: é€€è´§ç‡{ret_rate:.0%}ï¼Œå®é™…å‡€å€¼ä»…{net_monetary:.2f}ã€‚")
                is_target_user = True

            # è§„åˆ™C: è™šå‡å¤§æˆ· (Grosså¾ˆé«˜ï¼ŒNetå¾ˆä½)
            if gross_monetary > 1000 and (net_monetary / gross_monetary) < 0.2:
                tags.append("ğŸ¤¡è™šå‡å¤§æˆ·")
                sugs.append("æ— æ•ˆäº¤æ˜“: äº§ç”Ÿå¤§é‡æµæ°´ä½†æ— å®é™…åˆ©æ¶¦ï¼Œæµªè´¹è¿è´¹ã€‚")
                is_target_user = True

            # =========================================================
            # 2. ä¼˜è´¨/ä»·å€¼ç»„ (Value Group)
            # =========================================================

            if is_safe_user:
                # è§„åˆ™D: è¶…çº§é²¸é±¼ (Real Whale)
                if net_monetary > df["Net_Monetary"].quantile(0.9):
                    tags.append("ğŸ³è¶…çº§é²¸é±¼")
                    sugs.append("VVIP: å‡€è´¡çŒ®æé«˜ï¼Œéœ€é˜²æ­¢æµå¤±ã€‚")
                    is_target_user = True

                # è§„åˆ™E: å¿ è¯šé“ç²‰ (Loyalist)
                elif freq >= 4 and recency < 60:
                    tags.append("â¤ï¸å¿ è¯šé“ç²‰")
                    sugs.append("é«˜ç²˜æ€§: å¤è´­ä¹ æƒ¯è‰¯å¥½ï¼Œæ–°å“æ¨å¹¿é¦–é€‰ã€‚")
                    is_target_user = True

                # è§„åˆ™F: æ½œåŠ›æ‰¹å‘å•† (Wholesaler)
                elif aov > 500 and freq <= 5:
                    tags.append("ğŸ“¦æ½œåŠ›æ‰¹å‘å•†")
                    sugs.append("Bç«¯å¼€å‘: å•ç¬”å‡€å€¼é«˜ï¼Œå¯èƒ½æ˜¯çº¿ä¸‹åº—æˆ–åŒè¡Œã€‚")
                    is_target_user = True

                # è§„åˆ™G: æ²‰ç¡å¤§å®¢æˆ·
                elif net_monetary > q_net_m_high and recency > 90:
                    tags.append("ğŸ’¤æ²‰ç¡å¤§å®¢æˆ·")
                    sugs.append("å¬å›: ä¼˜è´¨å®¢æˆ·æµå¤±é¢„è­¦ï¼Œå»ºè®®é‚®ä»¶è§¦è¾¾ã€‚")
                    is_target_user = True

            if not is_target_user:
                continue

            results.append({
                "ä¹°å®¶ç”¨æˆ·å": user,
                "å®¢æˆ·æ ‡ç­¾": " | ".join(tags),
                "è¿è¥å»ºè®®": " ".join(sugs),
                "å‡€æ¶ˆè´¹é¢(Net LTV)": round(net_monetary, 2),
                "æ€»æµæ°´(Gross)": round(gross_monetary, 2),
                "è®¢å•æ•°(1Y)": freq,
                "é€€è´§ç‡(1Y)": f"{ret_rate:.1%}",
                "æœ€è¿‘è´­ä¹°(å¤©å‰)": int(recency),
                "å®¢å•ä»·(Net AOV)": round(aov, 2)
            })

        return pd.DataFrame(results)

    @staticmethod
    def get_tag_definitions() -> list:
        return [
            "ğŸ“˜ å®¢æˆ·ä»·å€¼ä¸é£é™©è¯Šæ–­è¯´æ˜ (CRM V2.0):",
            "=================================================================================",
            "ã€æ ¸å¿ƒé€»è¾‘ã€‘ æ•°æ®åŸºäºã€è¿‡å»365å¤©ã€‘ã€‚æ‰€æœ‰ä»·å€¼åˆ¤æ–­åŸºäºã€å‡€æ¶ˆè´¹é¢ (Net LTV)ã€‘ï¼Œå³æ‰£é™¤é€€æ¬¾åçš„çœŸå®æ”¶å…¥ã€‚",
            "",
            "1. ğŸš« é£é™©æ§åˆ¶:",
            "   -  çº çº·å‘èµ·è€…: æœ‰ Payment Dispute è®°å½•ã€‚",
            "   - âš ï¸ æƒ¯æ€§é€€è´§: è´­ä¹°>=2æ¬¡ ä¸” é€€è´§ç‡>30%ã€‚",
            "   - ğŸ¤¡ è™šå‡å¤§æˆ·: æ€»æµæ°´é«˜ï¼Œä½†å‡€å€¼æä½ (é€€æ¬¾å æ¯”>80%)ï¼Œå±äºæ— æ•ˆç¹è£ã€‚",
            "",
            "2. ğŸ’ ä»·å€¼æŒ–æ˜ (å‰æ: é€€è´§ç‡<20%):",
            "   - ğŸ³ è¶…çº§é²¸é±¼: å‡€æ¶ˆè´¹é¢å¤„äºå…¨åº— Top 10%ã€‚",
            "   - â¤ï¸ å¿ è¯šé“ç²‰: å¹´è´­ä¹°>=4æ¬¡ ä¸” æœ€è¿‘60å¤©æœ‰äº¤æ˜“ã€‚",
            "   - ğŸ“¦ æ½œåŠ›æ‰¹å‘å•†: è´­ä¹°é¢‘æ¬¡ä½ï¼Œä½†å•ç¬”å‡€å€¼ > $500ã€‚",
            "   - ğŸ’¤ æ²‰ç¡å¤§å®¢æˆ·: å†å²å‡€å€¼é«˜(Top 20%) ä½† >90å¤©æœªå›è´­ã€‚",
            "================================================================================="
        ]
==================== END FILE: core/services/diagnostics/crm.py ====================


==================== START FILE: core/services/diagnostics/logistics.py ====================
# core/services/diagnostics/logistics.py
"""
æ–‡ä»¶è¯´æ˜: ç‰©æµæ•ˆç›Šè¯Šæ–­ä¸“å®¶ (Logistics Diagnostics)
ä¸»è¦åŠŸèƒ½:
1. åˆ†æç‰©æµæˆæœ¬ç»“æ„ï¼Œè¯†åˆ«å¼‚å¸¸è´¹ç”¨ (ç½šæ¬¾ã€è¶…æ”¯)ã€‚
2. è¯„ä¼°ç‰©æµè´¹æ•ˆæ¯” (Shipping Ratio)ã€‚
3. è¾“å‡ºä¼˜åŒ–å»ºè®® (å¦‚æ ¡å‡†é‡é‡ã€ä¼˜åŒ–åŒ…è£…)ã€‚
"""

import pandas as pd
from .base import BaseDiagnostician


class LogisticsDiagnostician(BaseDiagnostician):

    def diagnose(self) -> pd.DataFrame:
        # è¿™é‡Œçš„ m_cur æ˜¯ Logistics Analyzer ç”Ÿæˆçš„ DF3 (Combo è¯¦æƒ…è¡¨)
        df = self.m_cur.copy()
        if df.empty: return df

        results = []
        for _, row in df.iterrows():
            tags, sugs = [], []

            # æå–æ ¸å¿ƒæŒ‡æ ‡
            combo = row.get("Combo", "Unknown")
            # æ³¨æ„: è¿™é‡Œçš„"åŸå§‹é‚®è´¹"å®é™…ä¸Šæ˜¯æ€»æ”¯å‡º (Earning + Under + Over)
            total_shipping_cost = row.get("åŸå§‹é‚®è´¹", 0)

            # æˆ‘ä»¬éœ€è¦ä¼°ç®—é”€å”®é¢æ¥è®¡ç®—è´¹ç‡ï¼Œä½† Logistics æŠ¥è¡¨ä¸­é€šå¸¸åªæœ‰è¿è´¹æ•°æ®ã€‚
            # å¦‚æœä¸Šæ¸¸ä¼ é€’äº† Revenue æ•°æ®æœ€å¥½ï¼Œå¦‚æœæ²¡æœ‰ï¼Œæˆ‘ä»¬ä»…åšæˆæœ¬ä¾§è¯Šæ–­ã€‚
            # V1.5.3 é€»è¾‘ä¸­ä¼¼ä¹æ²¡æœ‰ä¼ é€’ Revenue åˆ° DF3ï¼Œå› æ­¤æˆ‘ä»¬ä¸»è¦å…³æ³¨å¼‚å¸¸è´¹ç”¨å æ¯”ã€‚

            # ç»†åˆ†è´¹ç”¨
            penalty = row.get("é‚®è´¹ç½šæ¬¾", 0)
            overpay = row.get("è¶…æ”¯é‚®è´¹", 0)
            return_ship = row.get("åŒ…é‚®é€€è´§é‚®è´¹", 0)

            # åŸºç¡€è¿è´¹ä¼°ç®—
            base_cost_pure = total_shipping_cost - penalty - overpay - return_ship

            # =========================================================
            # 1. å¼‚å¸¸ç®¡æ§ (Cost Control)
            # =========================================================

            # è§„åˆ™A: ç½šæ¬¾é»‘æ´
            # ç½šæ¬¾é‡‘é¢ > åŸºç¡€è¿è´¹çš„ 20%
            if base_cost_pure > 0 and (penalty / base_cost_pure) > 0.2:
                tags.append("ğŸ’¸ç½šæ¬¾é»‘æ´")
                sugs.append("å°ºå¯¸/é‡é‡å¼‚å¸¸: å®é™…å‘è´§è§„æ ¼ä¸ç”³æŠ¥ä¸¥é‡ä¸ç¬¦ï¼Œè¯·å¤æ ¸ä»“åº“SOPã€‚")

            # è§„åˆ™B: è¶…æ”¯é¢„è­¦ (Underpaid)
            if overpay > 5:  # ç»å¯¹å€¼å¤§äº5åˆ€
                tags.append("âš–ï¸é‡é‡è¶…æ”¯")
                sugs.append("æ¸ é“é”™é…: å¯èƒ½ä½¿ç”¨äº†ä¸é€‚åˆè¯¥é‡é‡æ®µçš„ç‰©æµæœåŠ¡ï¼Œå­˜åœ¨è¡¥ç¼´è®°å½•ã€‚")

            # è§„åˆ™C: é€€è´§è¿è´¹æ€æ‰‹
            if return_ship > (total_shipping_cost * 0.3):
                tags.append("â†©ï¸é€€è´§è¿è´¹æ€æ‰‹")
                sugs.append("é€€è´§æŸè€—é«˜: å¤§é‡ç‰©æµè´¹æµªè´¹åœ¨é€€è´§é¢å•ä¸Šã€‚")

            # =========================================================
            # 2. æ•ˆç›Šåˆ†æ (Efficiency)
            # =========================================================

            # å¹³å‡å•å‡è¿è´¹
            total_orders = row.get("åŸå§‹å•æ•°", 0)
            avg_cost = total_shipping_cost / total_orders if total_orders > 0 else 0

            if avg_cost > 0:
                if avg_cost < 8:
                    tags.append("å°ä»¶ä¼˜åŠ¿")
                elif avg_cost > 50:
                    tags.append("ğŸš›å¤§ä»¶ç‰©æµ")

            if not tags:
                tags.append("ğŸ”¹æ­£å¸¸")

            results.append({
                "Comboç»„åˆ": combo,
                "ç‰©æµè¯Šæ–­": " | ".join(tags),
                "ä¼˜åŒ–å»ºè®®": " ".join(sugs),
                "æ€»å•é‡": int(total_orders),
                "æ€»è¿è´¹": round(total_shipping_cost, 2),
                "ç½šæ¬¾å æ¯”": f"{(penalty / total_shipping_cost):.1%}" if total_shipping_cost > 0 else "0.0%"
            })

        # æŒ‰ æ€»è¿è´¹ é™åºï¼Œä¼˜å…ˆçœ‹èŠ±é’±æœ€å¤šçš„
        return pd.DataFrame(results).sort_values("æ€»è¿è´¹", ascending=False)

    @staticmethod
    def get_tag_definitions() -> list:
        return [
            "ğŸ“˜ ç‰©æµæ•ˆç›Šè¯Šæ–­è¯´æ˜ (Logistics Diagnostics):",
            "=================================================================================",
            "1. ã€å¼‚å¸¸ç®¡æ§ (Cost Control)ã€‘:",
            "   - ğŸ’¸ ç½šæ¬¾é»‘æ´: ç½šæ¬¾é‡‘é¢è¶…è¿‡åŸºç¡€è¿è´¹çš„20%ã€‚é€šå¸¸å› å°ºå¯¸æµ‹é‡é”™è¯¯æˆ–é‡é‡å°‘æŠ¥å¯¼è‡´ã€‚",
            "   - âš–ï¸ é‡é‡è¶…æ”¯: å­˜åœ¨ Underpaid è¡¥ç¼´è®°å½•ï¼Œéœ€æ ¡å‡†ç”µå­ç§¤æˆ–æ›´æ–°äº§å“å‚æ•°ã€‚",
            "   - â†©ï¸ é€€è´§è¿è´¹æ€æ‰‹: é€€è´§äº§ç”Ÿçš„é¢å•è´¹ç”¨å æ€»è¿è´¹30%ä»¥ä¸Šã€‚",
            "",
            "2. ã€ç±»å‹æ ‡ç­¾ã€‘:",
            "   -  å°ä»¶ä¼˜åŠ¿: å•å‡è¿è´¹ < $8ï¼Œé€‚åˆè½»å°ä»¶ä½æˆæœ¬è¿ä½œã€‚",
            "   - ğŸš› å¤§ä»¶ç‰©æµ: å•å‡è¿è´¹ > $50ï¼Œéœ€é‡ç‚¹å…³æ³¨ç‰©æµæ¸ é“æŠ˜æ‰£ã€‚",
            "================================================================================="
        ]
==================== END FILE: core/services/diagnostics/logistics.py ====================


==================== START FILE: core/services/diagnostics/__init__.py ====================

==================== END FILE: core/services/diagnostics/__init__.py ====================


==================== START FILE: core/services/diagnostics/listing.py ====================
# core/services/diagnostics/listing.py
"""
æ–‡ä»¶è¯´æ˜: Listing (Item ID) é”€å”®è¡¨ç°è¯Šæ–­ (Listing Diagnostics)
ä¸»è¦åŠŸèƒ½:
1. å¯¹ eBay Listing è¿›è¡Œåˆ†å±‚ (ç‹ç‰Œ/å¼•æµ/é•¿å°¾/åƒåœ¾)ã€‚
2. ç›‘æ§é“¾æ¥æƒé‡è¶‹åŠ¿ (ä¸Šå‡/ä¸‹æ»‘)ã€‚
3. è¯†åˆ«å¹¿å‘ŠäºæŸ (ACOS) å’Œé«˜é€€è´§é£é™©ã€‚
"""

import pandas as pd
from core.services.diagnostics.base import BaseDiagnostician


class ListingDiagnostician(BaseDiagnostician):
    """
    [ç­–ç•¥æœåŠ¡] Listing (Item ID) é”€å”®è¡¨ç°è¯Šæ–­ä¸“å®¶
    æ ¸å¿ƒå…³æ³¨ï¼šé“¾æ¥æƒé‡(Weight)ã€æµé‡ä»·å€¼(Traffic Value)ã€ç”Ÿå‘½å‘¨æœŸ(Lifecycle)ã€‚
    """

    def diagnose(self) -> pd.DataFrame:
        data = []
        for iid, cur in self.m_cur.items():
            sales = cur.get("net_qty", 0)
            rev = cur.get("total_rev", 0)
            profit = cur.get("profit", 0)
            margin = profit / rev if rev > 0 else 0

            ad_cost = cur.get("net_ad_fee", 0)
            acos = ad_cost / rev if rev > 0 else 0

            # ç¯æ¯”å¢é•¿
            prev_sales = self.m_prev.get(iid, {}).get("net_qty", 0)
            if prev_sales > 0:
                growth = (sales - prev_sales) / prev_sales
            else:
                growth = 1.0 if sales > 0 else 0.0

            # é€€è´§ç‡ (Listing ç»´åº¦çš„é€€è´§æ›´åæ˜ æè¿°ä¸ç¬¦é—®é¢˜)
            total_qty = cur.get("total_qty", 0)
            bad_qty = cur.get("return_qty", 0) + cur.get("claim_qty", 0)
            ret_rate = bad_qty / total_qty if total_qty > 0 else 0

            data.append({
                "item id": iid,
                "item title": cur.get("title", ""),
                "Sales": sales,
                "Margin": margin,
                "ACOS": acos,
                "Growth": growth,
                "ReturnRate": ret_rate
            })

        df = pd.DataFrame(data)
        if df.empty: return df

        # åŠ¨æ€åˆ†ä½æ•°è®¡ç®— (Top 20% / Bottom 20%)
        # é¿å…å…¨ä¸º0å¯¼è‡´çš„æŠ¥é”™
        q_sales_high = df["Sales"].quantile(0.8) if not df["Sales"].empty else 0
        q_sales_low = df["Sales"].quantile(0.2) if not df["Sales"].empty else 0
        q_margin_high = df["Margin"].quantile(0.8) if not df["Margin"].empty else 0
        q_margin_low = df["Margin"].quantile(0.2) if not df["Margin"].empty else 0

        results = []
        for _, row in df.iterrows():
            tags, sugs = [], []

            # --- 1. é“¾æ¥åˆ†å±‚ç­–ç•¥ ---
            if row["Sales"] > q_sales_high:
                if row["Margin"] > q_margin_high:
                    tags.append("ğŸŒŸç‹ç‰ŒListing")
                    sugs.append("æ ¸å¿ƒèµ„äº§: é“¾æ¥æƒé‡æé«˜ï¼Œé˜²å®ˆç«å“è·Ÿå–ã€‚")
                else:
                    tags.append("ğŸ®å¼•æµListing")
                    sugs.append("æµé‡å…¥å£: åˆ©ç”¨å…³è”é”€å”®(Variation/Bundle)å¸¦åŠ¨é«˜åˆ©æ¬¾ã€‚")
            elif row["Sales"] < q_sales_low:
                if row["Margin"] < q_margin_low:
                    tags.append("ğŸ•åƒåœ¾Listing")
                    sugs.append("æ²‰æ²¡æˆæœ¬: å»ºè®®ä¸‹æ¶é‡åšï¼Œé‡Šæ”¾åˆŠç™»é¢åº¦ã€‚")
                else:
                    tags.append("ğŸ’é•¿å°¾Listing")
                    sugs.append("ç²¾å‡†æµé‡: ä¼˜åŒ–é•¿å°¾å…³é”®è¯ï¼Œç»´æŒé«˜ROIã€‚")
            else:
                tags.append("ğŸ”¹è…°éƒ¨Listing")
                sugs.append("æ½œåŠ›æŒ–æ˜: åˆ†æè½¬åŒ–ç‡ï¼Œå°è¯•å‚åŠ å¹³å°æ´»åŠ¨ã€‚")

            # --- 2. è¶‹åŠ¿ä¸é£é™© ---
            if row["Growth"] > 0.2:
                tags.append("ğŸš€æƒé‡ä¸Šå‡")
            elif row["Growth"] < -0.2:
                tags.append("ğŸ“‰æƒé‡ä¸‹æ»‘")

            if row["ReturnRate"] > 0.1:
                tags.append("âš ï¸é«˜é€€è´§é£é™©")

            if row["ACOS"] > 0.4:
                tags.append("ğŸ”¥å¹¿å‘ŠäºæŸ")

            results.append({
                "Item ID": row["item id"],
                "Item Title": row["item title"],
                "è¯Šæ–­æ ‡ç­¾": " | ".join(tags),
                "AIè¿è¥å»ºè®®": " ".join(sugs),
                "é”€é‡": int(row["Sales"]),
                "åˆ©æ¶¦ç‡": f"{row['Margin']:.2%}",
                "å¢é•¿ç‡": f"{row['Growth']:.2%}",
                "ACOS": f"{row['ACOS']:.2%}"
            })

        return pd.DataFrame(results)

    @staticmethod
    def get_tag_definitions() -> list:
        return [
            "ğŸ“˜ Listing é”€å”®è¡¨ç°è¯Šæ–­è¯´æ˜ (Sales Performance):",
            "=================================================================================",
            "1. ã€é“¾æ¥åˆ†å±‚ç­–ç•¥ã€‘:",
            "   - ğŸŒŸ ç‹ç‰Œ (Ace):      é«˜æµé‡è½¬åŒ– + é«˜æº¢ä»·èƒ½åŠ› -> å“ç‰ŒæŠ¤åŸæ²³ã€‚",
            "   - ğŸ® å¼•æµ (Traffic):  é«˜æµé‡è½¬åŒ– + ä½æº¢ä»· -> ç”¨äºæŠ¢å ç±»ç›®æ’åã€‚",
            "   - ğŸ’ é•¿å°¾ (Long Tail): ä½æµé‡ + é«˜æº¢ä»· -> æ»¡è¶³ç‰¹å®šå°ä¼—éœ€æ±‚ã€‚",
            "   - ğŸ• åƒåœ¾ (Trash):    æ— æµé‡ + æ— åˆ©æ¶¦ -> å ç”¨è¿è¥ç²¾åŠ›çš„è´Ÿå€ºã€‚",
            "",
            "2. ã€è¿è¥å…³é”®æŒ‡æ ‡ã€‘:",
            "   - ğŸš€ æƒé‡ä¸Šå‡: é”€é‡ç¯æ¯”æ¶¨å¹… > 20%ï¼Œå»ºè®®åŠ å¤§æ¨å¹¿åŠ›åº¦ä¹˜èƒœè¿½å‡»ã€‚",
            "   - ğŸ”¥ å¹¿å‘ŠäºæŸ: ACOS > 40%ï¼Œéœ€æ£€æŸ¥æ˜¯å¦å…³é”®è¯å®½æ³›åŒ¹é…å¯¼è‡´æ— æ•ˆç‚¹å‡»ã€‚",
            "================================================================================="
        ]
==================== END FILE: core/services/diagnostics/listing.py ====================


==================== START FILE: core/services/diagnostics/combo.py ====================
# core/services/diagnostics/combo.py
"""
æ–‡ä»¶è¯´æ˜: Combo ç»„åˆç­–ç•¥è¯Šæ–­ (Bundling Diagnostics)
ä¸»è¦åŠŸèƒ½:
1. è¯„ä¼°æ‰“åŒ…ç­–ç•¥çš„æœ‰æ•ˆæ€§ (é»„é‡‘ç»„åˆ vs æ— æ•ˆæ†ç»‘)ã€‚
2. è¯†åˆ« "è¿åé£é™©" (å› ä½è´¨å­ä»¶å¯¼è‡´æ•´ä¸ªComboé€€è´§)ã€‚
3. ç›‘æ§ç»„åˆçƒ­åº¦è¶‹åŠ¿ã€‚
"""

import pandas as pd
from core.services.diagnostics.base import BaseDiagnostician


class ComboDiagnostician(BaseDiagnostician):
    """
    [ç­–ç•¥æœåŠ¡] Combo (Full SKU) æ†ç»‘ç­–ç•¥è¯Šæ–­ä¸“å®¶
    æ ¸å¿ƒå…³æ³¨ï¼šæ†ç»‘æœ‰æ•ˆæ€§(Bundling Efficiency)ã€è¿åé£é™©(Risk Association)ã€‚
    """

    def diagnose(self) -> pd.DataFrame:
        data = []
        for sku, cur in self.m_cur.items():
            sales = cur.get("net_qty", 0)
            rev = cur.get("total_rev", 0)
            profit = cur.get("profit", 0)
            margin = profit / rev if rev > 0 else 0

            # ç¯æ¯”
            prev_sales = self.m_prev.get(sku, {}).get("net_qty", 0)
            if prev_sales > 0:
                growth = (sales - prev_sales) / prev_sales
            else:
                growth = 1.0 if sales > 0 else 0.0

            # è¿åé£é™©æ£€æŸ¥
            # å¦‚æœ Combo ä¸­æœ‰ä¸€ä¸ªå­ä»¶è´¨é‡å·®ï¼Œä¼šå¯¼è‡´æ•´ä¸ªå¤§é‡‘é¢è®¢å•é€€è´§
            bad_qty = cur.get("return_qty", 0) + cur.get("claim_qty", 0)
            total_qty = cur.get("total_qty", 0)
            ret_rate = bad_qty / total_qty if total_qty > 0 else 0

            data.append({
                "full sku": sku,
                "Sales": sales,
                "Margin": margin,
                "Growth": growth,
                "ReturnRate": ret_rate
            })

        df = pd.DataFrame(data)
        if df.empty: return df

        # åˆ†ä½æ•°
        q_sales_high = df["Sales"].quantile(0.8) if not df["Sales"].empty else 0
        q_sales_low = df["Sales"].quantile(0.2) if not df["Sales"].empty else 0
        q_margin_high = df["Margin"].quantile(0.8) if not df["Margin"].empty else 0

        results = []
        for _, row in df.iterrows():
            tags, sugs = [], []

            # --- 1. ç»„åˆæ•ˆç›Šåˆ†æ ---
            if row["Sales"] > q_sales_high:
                if row["Margin"] > q_margin_high:
                    tags.append("ğŸ‘‘é»„é‡‘ç»„åˆ")
                    sugs.append("ç­–ç•¥æˆåŠŸ: æ†ç»‘é”€å”®æˆåŠŸæå‡äº†å®¢å•ä»·(AOV)ä¸åˆ©æ¶¦ã€‚")
                else:
                    tags.append("ğŸ“¦å¼•æµåŒ…")
                    sugs.append("èµ°é‡å·¥å…·: é€šè¿‡ä½ä»·æ‰“åŒ…æŠ¢å å¸‚åœºä»½é¢ã€‚")
            elif row["Sales"] < q_sales_low:
                tags.append("æ— æ•ˆæ†ç»‘")
                sugs.append("ç­–ç•¥å¤±è´¥: å®¢æˆ·ä¸ä¹°è´¦ï¼Œå»ºè®®è§£ç»‘æˆ–æ›´æ¢ç»„åˆæ–¹å¼ã€‚")
            else:
                tags.append("ğŸ”¹æ™®é€šç»„åˆ")

            if row["Growth"] > 0.2:
                tags.append("ğŸ“ˆçƒ­åº¦ä¸Šå‡")

            # --- 2. è¿åé£é™© ---
            if row["ReturnRate"] > 0.1:
                tags.append("âš ï¸è¿åé£é™©")
                sugs.append("ä¸¥é‡è­¦å‘Š: é€€è´§ç‡è¿‡é«˜ï¼Œè¯·æ’æŸ¥Comboä¸­æ˜¯å¦å­˜åœ¨ä½è´¨é‡å­SKUã€‚")

            results.append({
                "Full SKU": row["full sku"],
                "è¯Šæ–­æ ‡ç­¾": " | ".join(tags),
                "AIè¿è¥å»ºè®®": " ".join(sugs),
                "é”€é‡": int(row["Sales"]),
                "åˆ©æ¶¦ç‡": f"{row['Margin']:.2%}",
                "é€€è´§ç‡": f"{row['ReturnRate']:.2%}"
            })

        return pd.DataFrame(results)

    @staticmethod
    def get_tag_definitions() -> list:
        return [
            "ğŸ“˜ Combo æ†ç»‘ç­–ç•¥è¯Šæ–­è¯´æ˜ (Bundling Strategy):",
            "=================================================================================",
            "1. ã€ç»„åˆæ•ˆç›Šåˆ†æã€‘:",
            "   - ğŸ‘‘ é»„é‡‘ç»„åˆ: 1+1>2ï¼ŒæˆåŠŸè®©å®¢æˆ·ä¸ºé«˜å®¢å•ä»·ä¹°å•ï¼Œä¸”åˆ©æ¶¦ä¸°åšã€‚",
            "   -  æ— æ•ˆæ†ç»‘: 1+1<2ï¼Œå®¢æˆ·æ›´å€¾å‘äºå•ç‹¬è´­ä¹°å­äº§å“ï¼Œæ‰“åŒ…åè€Œé™ä½äº†è½¬åŒ–ã€‚",
            "",
            "2. ã€è¿åé£é™© (Cannibalization)ã€‘:",
            "   - âš ï¸ è¿åé£é™©: å› ç»„åˆä¸­æŸä¸€ä¸ªå»‰ä»·é…ä»¶è´¨é‡å·®ï¼Œå¯¼è‡´å®¢æˆ·é€€æ‰æ•´ä¸ªé«˜ä»·åŒ…è£¹ã€‚",
            "                 (ä¾‹: èµ å“èºä¸ç”Ÿé”ˆï¼Œå¯¼è‡´å‡ ç™¾åˆ€çš„è½®æ¯‚è¢«é€€è´§)",
            "================================================================================="
        ]
==================== END FILE: core/services/diagnostics/combo.py ====================


==================== START FILE: core/services/diagnostics/base.py ====================
# core/services/diagnostics/base.py
"""
æ–‡ä»¶è¯´æ˜: è¯Šæ–­ä¸“å®¶åŸºç±» (Diagnostic Interface)
ä¸»è¦åŠŸèƒ½:
1. å®šä¹‰æ‰€æœ‰è¯Šæ–­æœåŠ¡å¿…é¡»éµå®ˆçš„æ¥å£è§„èŒƒ (Strategy Pattern)ã€‚
2. å¼ºåˆ¶è¦æ±‚å®ç° diagnose() è®¡ç®—é€»è¾‘å’Œ get_tag_definitions() è§£é‡Šæ–‡æ¡ˆã€‚
"""

from abc import ABC, abstractmethod
import pandas as pd
from typing import Dict, Any

class BaseDiagnostician(ABC):
    """
    [æ¥å£å®šä¹‰] è¯Šæ–­ä¸“å®¶åŸºç±»
    """

    def __init__(self, metrics_cur: Dict[str, Any], metrics_prev: Dict[str, Any], **kwargs):
        """
        Args:
            metrics_cur: æœ¬æœŸè´¢åŠ¡æŒ‡æ ‡å­—å…¸ (Key -> Metrics Dict)
            metrics_prev: ä¸ŠæœŸè´¢åŠ¡æŒ‡æ ‡å­—å…¸ (ç”¨äºç¯æ¯”åˆ†æ)
            kwargs: æ‰©å±•å‚æ•° (å¦‚åº“å­˜æ˜ å°„è¡¨)
        """
        self.m_cur = metrics_cur
        self.m_prev = metrics_prev
        self.kwargs = kwargs

    @abstractmethod
    def diagnose(self) -> pd.DataFrame:
        """
        [æ ¸å¿ƒ] æ‰§è¡Œè¯Šæ–­è®¡ç®—
        Returns: åŒ…å« [Key, è¯Šæ–­æ ‡ç­¾, AIè¿è¥å»ºè®®, ...] çš„ DataFrame
        """
        pass

    @staticmethod
    @abstractmethod
    def get_tag_definitions() -> list:
        """
        [æ–‡æ¡£] è¿”å›æ ‡ç­¾çš„è¯¦ç»†ä¸šåŠ¡å®šä¹‰è¯´æ˜
        ç”¨äºåœ¨æŠ¥è¡¨åº•éƒ¨ç”Ÿæˆ "Explainable AI" è¯´æ˜åŒºã€‚
        """
        pass
==================== END FILE: core/services/diagnostics/base.py ====================


==================== START FILE: core/services/finance/profit_sku.py ====================
# core/services/finance/profit_sku.py
"""
æ–‡ä»¶è¯´æ˜: SKU çº§åˆ©æ¶¦åˆ†æå™¨ (SKU Profit Analyzer)
ä¸»è¦åŠŸèƒ½:
1. éå†æ¸…æ´—åçš„è®¢å•æ•°æ®ï¼Œå°†è®¢å•çº§è´¢åŠ¡æŒ‡æ ‡æ‹†è§£åˆ†æ‘Šåˆ°å•ä¸ª SKUã€‚
2. åˆ†æ‘Šé€»è¾‘: åŸºäº SKU çš„è´§å€¼å æ¯” (Weighted Proration)ã€‚
3. è°ƒç”¨ SkuDiagnostician ç”Ÿæˆä¾›åº”é“¾å¥åº·åº¦è¯Šæ–­ã€‚
4. è¾“å‡ºæ ‡å‡† A1-B3 æŠ¥è¡¨åŠ C1 è¯Šæ–­è¡¨ã€‚
"""

import os
import pandas as pd
from collections import defaultdict
import tqdm

from core.services.finance.base import ProfitAnalyzerBase
from core.services.diagnostics.sku import SkuDiagnostician


class SkuProfitAnalyzer(ProfitAnalyzerBase):

    def _aggregate(self, df: pd.DataFrame) -> dict:
        """[æ ¸å¿ƒé€»è¾‘] æ•°æ®èšåˆä¸åˆ†æ‘Š"""
        metrics = defaultdict(lambda: defaultdict(float))
        if df.empty: return metrics

        records = df.to_dict('records')
        for row in tqdm.tqdm(records, desc="æ­£åœ¨èšåˆ SKU æ•°æ®"):
            qty_sets = int(float(row.get("quantity", 0)))
            action = str(row.get("action", "")).strip().upper()
            revenue = float(row.get("revenue", 0))
            refund = float(row.get("Refund", 0))

            # 1. è§£æå½“å‰è¡ŒåŒ…å«çš„æ‰€æœ‰ SKU åŠå…¶ä»·å€¼
            current_sku_units = {}
            current_sku_value = {}
            order_total_cost_val = 0.0

            for i in range(1, 11):
                s_key = f"sku{i}"
                q_key = f"qty{i}"
                if s_key not in row: break

                # å½’ä¸€åŒ–å»é‡
                raw_sku = str(row.get(s_key, ""))
                if not raw_sku or raw_sku.lower() in ['nan', 'none', '0', '']: continue
                sku = raw_sku.strip().upper()

                try:
                    per_qty = float(row.get(q_key, 0))
                except:
                    per_qty = 0

                units = per_qty * qty_sets
                unit_cost = self.sku_cost_map.get(sku, 0.0)
                val = units * unit_cost  # è´§å€¼ = æ•°é‡ * æˆæœ¬

                current_sku_units[sku] = units
                current_sku_value[sku] = val
                order_total_cost_val += val

            if not current_sku_units: continue

            # é˜²å¾¡ï¼šå¦‚æœæ€»æˆæœ¬ä¸º0ï¼ˆä¾‹å¦‚å…¨æ˜¯èµ å“ï¼‰ï¼ŒæŒ‰æ•°é‡å‡æ‘Š
            if order_total_cost_val == 0:
                total_units = sum(current_sku_units.values())
                for s, u in current_sku_units.items():
                    # ä»·å€¼æƒé‡é€€åŒ–ä¸ºæ•°é‡æƒé‡
                    current_sku_value[s] = u
                order_total_cost_val = total_units

            # 2. åˆ†æ‘Šè®¡ç®—
            for sku, units in current_sku_units.items():
                # è®¡ç®—åˆ†æ‘Šæƒé‡ (Weight)
                w = 0.0
                if order_total_cost_val > 0:
                    w = current_sku_value[sku] / order_total_cost_val

                # ç´¯åŠ æ•°é‡
                metrics[sku]["total_qty"] += units
                if action == "CA":
                    metrics[sku]["cancel_qty"] += units
                elif action == "RE":
                    metrics[sku]["return_qty"] += units
                elif action == "CR":
                    metrics[sku]["request_qty"] += units
                elif action == "CC":
                    metrics[sku]["claim_qty"] += units
                elif action == "PD":
                    metrics[sku]["dispute_qty"] += units

                # ç´¯åŠ é‡‘é¢ (æŒ‰æƒé‡)
                metrics[sku]["total_rev"] += revenue * w
                if action == "CA":
                    metrics[sku]["cancel_rev"] += refund * w
                elif action == "RE":
                    metrics[sku]["return_rev"] += refund * w
                elif action == "CR":
                    metrics[sku]["request_rev"] += refund * w
                elif action == "CC":
                    metrics[sku]["claim_rev"] += refund * w
                elif action == "PD":
                    metrics[sku]["dispute_rev"] += refund * w

                # ç´¯åŠ æˆæœ¬ (ç›´æ¥è®¡ç®—ï¼Œä¸åˆ†æ‘Š)
                unit_cost = self.sku_cost_map.get(sku, 0.0)
                metrics[sku]["cog_value"] += -(unit_cost * units)  # æˆæœ¬æ˜¯è´Ÿæ”¯å‡º

                # ç´¯åŠ å„é¡¹è´¹ç”¨ (è°ƒç”¨åŸºç±» Helper)
                self._accumulate_fees(row, metrics, sku, weight=w)

        return metrics

    def run(self):
        """ä¸»æ‰§è¡Œæµç¨‹"""
        # 1. åŠ è½½æ•°æ®
        self._load_basics()

        if self.df_cur is None or self.df_cur.empty:
            self.log("âš ï¸ æœ¬æœŸæ— æ•°æ®ï¼Œæ— æ³•åˆ†æ")
            return

        self.log(f"ğŸ“Š å·²åŠ è½½åŸå§‹è®°å½•: {len(self.df_cur)} æ¡")

        # 2. èšåˆ
        self.log("æ­£åœ¨èšåˆæœ¬æœŸæ•°æ®...")
        m_cur = self._calculate_net_profit(self._aggregate(self.df_cur))

        self.log("æ­£åœ¨èšåˆä¸ŠæœŸæ•°æ®(ç”¨äºç¯æ¯”)...")
        m_prev = self._calculate_net_profit(self._aggregate(self.df_prev))

        # 3. ç”ŸæˆåŸºç¡€æŠ¥è¡¨
        tables = self.generate_full_report_suite(m_cur, m_prev, key_name="SKU")

        # 4. æ‰§è¡Œè¯Šæ–­
        self.log("æ­£åœ¨æ‰§è¡Œ AI æ™ºèƒ½è¯Šæ–­...")

        # è·å–åº“å­˜æ•°æ®ç”¨äº DOS è®¡ç®—
        df_inv = self.inv_repo.get_inventory_latest()
        if df_inv.empty:
            inv_map = {}
        else:
            inv_map = dict(zip(
                df_inv["SKU"].astype(str).str.strip().str.upper(),
                pd.to_numeric(df_inv["Quantity"], errors='coerce').fillna(0)
            ))

        diagnostician = SkuDiagnostician(m_cur, m_prev, inv_map)
        df_diag = diagnostician.diagnose()

        tables.append(("C1_æ™ºèƒ½è¯Šæ–­è¡¨ (AI Diagnostics)", df_diag))
        explanation_lines = diagnostician.get_tag_definitions()

        # 5. ä¿å­˜
        filename = f"Profit_Analysis_SKU_{self.file_suffix}.csv"

        # è‡ªå®šä¹‰ä¿å­˜é€»è¾‘ä»¥å†™å…¥å¤šä¸ª Table
        save_path = self.save_csv(pd.DataFrame(), filename)  # å…ˆè·å–è·¯å¾„

        if save_path:
            try:
                with open(save_path, "w", encoding="utf-8-sig") as f:
                    for name, df in tables:
                        f.write(f"=== {name} ===\n")
                        df.to_csv(f, index=False)
                        f.write("\n\n")

                    f.write("\n")
                    for line in explanation_lines:
                        f.write(f"{line}\n")

                self.log(f" SKU åˆ©æ¶¦ä¸è¯Šæ–­æŠ¥è¡¨å·²ç”Ÿæˆ: {filename}")
            except Exception as e:
                self.log(f" ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
==================== END FILE: core/services/finance/profit_sku.py ====================


==================== START FILE: core/services/finance/sales.py ====================
# core/services/finance/sales.py
"""
æ–‡ä»¶è¯´æ˜: SKU é”€é‡ç»Ÿè®¡åˆ†æ (Sales Quantity Analyzer)
ä¸»è¦åŠŸèƒ½:
1. ç»Ÿè®¡æ¯ä¸ª SKU åœ¨ä¸åŒåº—é“º (88/Plus) å’Œä¸åŒåŠ¨ä½œ (Sale/Cancel/Return) ä¸‹çš„æ•°é‡ã€‚
2. å¤„ç†ç‰¹æ®Š SKU (NU1C8) çš„æ˜ å°„è§„åˆ™ã€‚
"""

import pandas as pd
import numpy as np
import tqdm
from collections import defaultdict

from config.settings import settings
from core.services.finance.base import ProfitAnalyzerBase


class SalesQtyAnalyzer(ProfitAnalyzerBase):

    def run(self):
        self.log(f"ğŸš€ å¼€å§‹åˆ†æé”€é‡: {self.start_date} -> {self.end_date}")

        # 1. åŠ è½½æ•°æ® (åˆ©ç”¨åŸºç±»)
        # æ³¨æ„: SalesAnalyzer ä¸ä¾èµ–æˆæœ¬æ•°æ®ï¼Œæ‰€ä»¥å¯ä»¥æ‰‹åŠ¨è°ƒç”¨ repo æŸ¥è¯¢ï¼Œ
        # ä½†ä¸ºäº†å¤ç”¨åŸºç±»çš„æ—¥æœŸå¤„ç†ï¼Œæˆ‘ä»¬è°ƒç”¨ _load_basics (è™½ç„¶å®ƒä¼šåŠ è½½æˆæœ¬ï¼Œä½†è¿™æ— å®³ä¸”ä¿è¯ä¸€è‡´æ€§)
        self._load_basics()

        if self.df_cur.empty:
            self.log("âš ï¸ æœŸé—´æ— æ•°æ®")
            return

        self.log(f"ğŸ“Š å·²åŠ è½½åŸå§‹è®°å½•: {len(self.df_cur)} æ¡")

        stats = defaultdict(lambda: defaultdict(int))
        records = self.df_cur.to_dict('records')

        for row in tqdm.tqdm(records, desc="è®¡ç®—é”€é‡"):
            self._process_row(row, stats)

        if not stats:
            self.log("âš ï¸ ç»Ÿè®¡ç»“æœä¸ºç©º")
            return

        # è½¬æ¢ä¸º DataFrame
        df_res = pd.DataFrame.from_dict(stats, orient='index').reset_index()
        df_res.rename(columns={'index': 'SKU'}, inplace=True)
        df_res.fillna(0, inplace=True)

        # è®¡ç®—å‡€å€¼å’Œç™¾åˆ†æ¯”
        R = settings.LOSS_RATES
        prefixes = ["88", "plus", "total"]
        metrics = ["Canceled", "Returned", "Cased", "Request", "Dispute"]

        for prefix in prefixes:
            if f"{prefix}_Sold" not in df_res.columns:
                df_res[f"{prefix}_Sold"] = 0
            sold = df_res[f"{prefix}_Sold"]

            for metric in metrics:
                col_name = f"{prefix}_{metric}"
                if col_name not in df_res.columns: df_res[col_name] = 0

                # è®¡ç®—ç™¾åˆ†æ¯”
                df_res[f"{prefix}_{metric}_%"] = (
                    (df_res[col_name] / sold)
                    .replace([np.inf, -np.inf], 0)
                    .fillna(0)
                    .apply(lambda x: f"{x:.2%}")
                )

            # è®¡ç®— Net
            df_res[f"{prefix}_Net"] = (
                    df_res[f"{prefix}_Sold"]
                    - df_res[f"{prefix}_Canceled"]
                    - df_res[f"{prefix}_Returned"] * R.get('RETURN', 0.3)
                    - df_res[f"{prefix}_Cased"] * R.get('CASE', 0.6)
                    - df_res[f"{prefix}_Request"] * R.get('REQUEST', 0.5)
                    - df_res[f"{prefix}_Dispute"] * R.get('DISPUTE', 1.0)
            ).astype(int)

        # æ•´ç†åˆ—é¡ºåº
        cols = ["SKU"]
        for p in prefixes:
            cols.extend([
                f"{p}_Sold", f"{p}_Canceled", f"{p}_Canceled_%",
                f"{p}_Returned", f"{p}_Returned_%", f"{p}_Cased", f"{p}_Cased_%",
                f"{p}_Request", f"{p}_Request_%", f"{p}_Dispute", f"{p}_Dispute_%",
                f"{p}_Net"
            ])

        # è¡¥å…¨ç¼ºå¤±åˆ—
        for c in cols:
            if c not in df_res.columns: df_res[c] = 0
        df_res = df_res[cols]

        filename = f"SKU_Sold_{self.file_suffix}.csv"

        footer = [
            " ", "å¤‡æ³¨è¯´æ˜ï¼š",
            "1. å–æ¶ˆçš„è®¢å•ä¸ç®—åº“å­˜æ¶ˆè€—",
            f"2. Caseä¸ºå®¢æˆ·æŠ•è¯‰é€€è´§,å¹³å°ä»‹å…¥å¼ºåˆ¶é€€æ¬¾,è€—æŸç‡{int(R.get('CASE', 0.6) * 100)}%",
            f"3. Requestä¸ºå®¢æˆ·ç”³è¯·é€€è´§,å¹³å°ä»‹å…¥,å–å®¶é€€æ¬¾,è€—æŸç‡{int(R.get('REQUEST', 0.5) * 100)}%",
            f"4. Returnä¸ºå®¢æˆ·ç”³è¯·é€€è´§,æ— å¹³å°ä»‹å…¥,å–å®¶é€€æ¬¾, è€—æŸç‡{int(R.get('RETURN', 0.3) * 100)}%",
            f"5. Disputeä¸ºå®¢æˆ·é€šè¿‡é“¶è¡ŒæŠ•è¯‰, å¹³å°å¼ºåˆ¶é€€æ¬¾, è€—æŸç‡{int(R.get('DISPUTE', 1.0) * 100)}%",
        ]

        self.save_csv(df_res, filename, footer)

    def _process_row(self, row, stats_dict):
        """å¤„ç†å•è¡Œæ•°æ®"""
        raw_seller = str(row.get("seller", "")).strip().lower()
        action = str(row.get("action", "")).strip().upper()

        try:
            quantity_val = int(float(row.get("quantity", 0)))
        except:
            quantity_val = 0

        # è§£æ SKU åˆ—è¡¨ (ä» Clean Log çš„ sku1..qty1 åˆ—)
        sku_list = []
        for i in range(1, 11):
            s_key = f"sku{i}"
            q_key = f"qty{i}"

            if s_key not in row: break

            sku_val = row.get(s_key)
            if sku_val and str(sku_val).strip() not in ["", "None", "nan", "NaN", "0"]:
                try:
                    q = int(float(row.get(q_key, 0)))
                except:
                    q = 0
                sku_clean = str(sku_val).strip().upper()
                sku_list.append((sku_clean, q))
            else:
                break

        # ç‰¹æ®Š SKU è§„åˆ™
        if any(sku in ["NU1C8E51C", "NU1C8E51K"] for sku, _ in sku_list):
            sku_list.append(("NU1C8SKT7", 2))

        # å½’å±åˆ¤å®š
        action_map = {
            "88": ["esparts88"],
            "plus": ["espartsplus"],
            "total": None
        }
        code_map = {
            "Canceled": "CA", "Returned": "RE", "Cased": "CC",
            "Request": "CR", "Dispute": "PD"
        }

        for prefix, target_sellers in action_map.items():
            if target_sellers is not None and raw_seller not in target_sellers:
                continue

            for sku, qtyp in sku_list:
                total_qty = quantity_val * qtyp
                stats_dict[sku][f"{prefix}_Sold"] += total_qty

                for label, code in code_map.items():
                    if action == code:
                        stats_dict[sku][f"{prefix}_{label}"] += total_qty
==================== END FILE: core/services/finance/sales.py ====================


==================== START FILE: core/services/finance/__init__.py ====================

==================== END FILE: core/services/finance/__init__.py ====================


==================== START FILE: core/services/finance/profit_combo.py ====================
# core/services/finance/profit_combo.py
"""
æ–‡ä»¶è¯´æ˜: Combo (Full SKU) ç»„åˆåˆ©æ¶¦åˆ†æå™¨
ä¸»è¦åŠŸèƒ½:
1. æŒ‰ Full SKU ç»„åˆå½’é›†è´¢åŠ¡æ•°æ®ã€‚
2. è¯„ä¼°æ‰“åŒ…ç­–ç•¥çš„åˆ©æ¶¦è¡¨ç°ã€‚
3. è°ƒç”¨ ComboDiagnostician ç”Ÿæˆè¯Šæ–­ã€‚
"""

import os
import pandas as pd
from collections import defaultdict
import tqdm

from core.services.finance.base import ProfitAnalyzerBase
from core.services.diagnostics.combo import ComboDiagnostician


class ComboProfitAnalyzer(ProfitAnalyzerBase):

    def _aggregate(self, df: pd.DataFrame) -> dict:
        metrics = defaultdict(lambda: defaultdict(float))
        if df.empty: return metrics

        records = df.to_dict('records')
        for row in tqdm.tqdm(records, desc="èšåˆ Combo æ•°æ®"):
            # å¼ºåˆ¶å¤§å†™
            raw_full = str(row.get("full sku", ""))
            if not raw_full or raw_full == '0': continue
            full_sku = raw_full.strip().upper()

            qty_sets = int(float(row.get("quantity", 0)))
            action = str(row.get("action", "")).strip().upper()
            revenue = float(row.get("revenue", 0))
            refund = float(row.get("Refund", 0))

            metrics[full_sku]["total_qty"] += qty_sets
            metrics[full_sku]["total_rev"] += revenue

            if action == "CA":
                metrics[full_sku]["cancel_qty"] += qty_sets; metrics[full_sku]["cancel_rev"] += refund
            elif action == "RE":
                metrics[full_sku]["return_qty"] += qty_sets; metrics[full_sku]["return_rev"] += refund
            elif action == "CR":
                metrics[full_sku]["request_qty"] += qty_sets; metrics[full_sku]["request_rev"] += refund
            elif action == "CC":
                metrics[full_sku]["claim_qty"] += qty_sets; metrics[full_sku]["claim_rev"] += refund
            elif action == "PD":
                metrics[full_sku]["dispute_qty"] += qty_sets; metrics[full_sku]["dispute_rev"] += refund

            # è®¡ç®—æ€»æˆæœ¬
            row_cost = 0.0
            for i in range(1, 11):
                s_key, q_key = f"sku{i}", f"qty{i}"
                if s_key not in row: break

                raw_sku = str(row[s_key])
                if not raw_sku or raw_sku == '0': continue
                sku = raw_sku.strip().upper()

                try:
                    q_per = float(row[q_key])
                except:
                    q_per = 0

                row_cost += (self.sku_cost_map.get(sku, 0.0) * q_per * qty_sets)

                if sku in ["NU1C8E51C", "NU1C8E51K"]:
                    row_cost += (self.sku_cost_map.get("NU1C8SKT7", 0.0) * 2 * qty_sets)

            metrics[full_sku]["cog_value"] += -row_cost
            self._accumulate_fees(row, metrics, full_sku, weight=1.0)

        return metrics

    def run(self):
        self._load_basics()

        if self.df_cur is None or self.df_cur.empty:
            self.log("âš ï¸ æœ¬æœŸæ— æ•°æ®ï¼Œæ— æ³•åˆ†æ")
            return

        self.log(f"ğŸ“Š å·²åŠ è½½åŸå§‹è®°å½•: {len(self.df_cur)} æ¡")

        self.log("æ­£åœ¨èšåˆæœ¬æœŸæ•°æ®...")
        m_cur = self._calculate_net_profit(self._aggregate(self.df_cur))

        self.log("æ­£åœ¨èšåˆä¸ŠæœŸæ•°æ®...")
        m_prev = self._calculate_net_profit(self._aggregate(self.df_prev))

        tables = self.generate_full_report_suite(m_cur, m_prev, key_name="Full SKU")

        self.log("æ­£åœ¨æ‰§è¡Œ AI æ™ºèƒ½è¯Šæ–­...")
        diag = ComboDiagnostician(m_cur, m_prev)
        df_diag = diag.diagnose()
        tables.append(("C1_æ™ºèƒ½è¯Šæ–­è¡¨ (AI Diagnostics)", df_diag))
        explanation_lines = diag.get_tag_definitions()

        filename = f"Profit_Analysis_Combo_{self.file_suffix}.csv"

        save_path = self.save_csv(pd.DataFrame(), filename)
        if save_path:
            try:
                with open(save_path, "w", encoding="utf-8-sig") as f:
                    for name, df in tables:
                        f.write(f"=== {name} ===\n")
                        df.to_csv(f, index=False)
                        f.write("\n\n")
                    f.write("\n")
                    for line in explanation_lines:
                        f.write(f"{line}\n")
                self.log(f" Combo åˆ©æ¶¦æŠ¥è¡¨å·²ç”Ÿæˆ: {filename}")
            except Exception as e:
                self.log(f" ä¿å­˜å¤±è´¥: {e}")
==================== END FILE: core/services/finance/profit_combo.py ====================


==================== START FILE: core/services/finance/profit_listing.py ====================
# core/services/finance/profit_listing.py
"""
æ–‡ä»¶è¯´æ˜: Listing çº§åˆ©æ¶¦åˆ†æå™¨ (Listing Profit Analyzer)
ä¸»è¦åŠŸèƒ½:
1. æŒ‰ Item ID å½’é›†è´¢åŠ¡æ•°æ®ã€‚
2. æ— éœ€åˆ†æ‘Šï¼Œç›´æ¥æ±‡æ€»è®¢å•çº§æ”¶å…¥ä¸è´¹ç”¨ã€‚
3. è°ƒç”¨ ListingDiagnostician ç”Ÿæˆé”€å”®è¡¨ç°è¯Šæ–­ã€‚
"""

import os
import pandas as pd
from collections import defaultdict
import tqdm

from core.services.finance.base import ProfitAnalyzerBase
from core.services.diagnostics.listing import ListingDiagnostician


class ListingProfitAnalyzer(ProfitAnalyzerBase):

    def _aggregate(self, df: pd.DataFrame) -> dict:
        metrics = defaultdict(lambda: defaultdict(float))
        if df.empty: return metrics

        records = df.to_dict('records')
        for row in tqdm.tqdm(records, desc="èšåˆ Listing æ•°æ®"):
            raw_id = str(row.get("item id", ""))
            # ç§»é™¤ .0 åç¼€å¹¶å»ç©ºæ ¼
            item_id = raw_id.strip().replace(".0", "")
            if not item_id or item_id == '0': continue

            # è®°å½• Title
            if "title" not in metrics[item_id]:
                metrics[item_id]["title"] = str(row.get("item title", "")).strip()

            qty_sets = int(float(row.get("quantity", 0)))
            action = str(row.get("action", "")).strip().upper()
            revenue = float(row.get("revenue", 0))
            refund = float(row.get("Refund", 0))

            # ç´¯åŠ æ•°é‡ä¸é‡‘é¢ (Listing ç»´åº¦æ— éœ€åˆ†æ‘Šæƒé‡)
            metrics[item_id]["total_qty"] += qty_sets
            metrics[item_id]["total_rev"] += revenue

            if action == "CA":
                metrics[item_id]["cancel_qty"] += qty_sets; metrics[item_id]["cancel_rev"] += refund
            elif action == "RE":
                metrics[item_id]["return_qty"] += qty_sets; metrics[item_id]["return_rev"] += refund
            elif action == "CR":
                metrics[item_id]["request_qty"] += qty_sets; metrics[item_id]["request_rev"] += refund
            elif action == "CC":
                metrics[item_id]["claim_qty"] += qty_sets; metrics[item_id]["claim_rev"] += refund
            elif action == "PD":
                metrics[item_id]["dispute_qty"] += qty_sets; metrics[item_id]["dispute_rev"] += refund

            # è®¡ç®—æ€»æˆæœ¬ (ç´¯åŠ æ‰€æœ‰å­ SKU çš„æˆæœ¬)
            row_cost = 0.0
            for i in range(1, 11):
                s_key, q_key = f"sku{i}", f"qty{i}"
                if s_key not in row: break

                raw_sku = str(row[s_key])
                if not raw_sku or raw_sku == '0': continue
                sku = raw_sku.strip().upper()

                try:
                    q_per = float(row[q_key])
                except:
                    q_per = 0

                unit_cost = self.sku_cost_map.get(sku, 0.0)
                row_cost += (unit_cost * q_per * qty_sets)

                # ç‰¹æ®Š SKU æˆæœ¬é€»è¾‘
                if sku in ["NU1C8E51C", "NU1C8E51K"]:
                    extra_cost = self.sku_cost_map.get("NU1C8SKT7", 0.0)
                    row_cost += (extra_cost * 2 * qty_sets)

            metrics[item_id]["cog_value"] += -row_cost
            self._accumulate_fees(row, metrics, item_id, weight=1.0)

        return metrics

    def run(self):
        self._load_basics()

        if self.df_cur is None or self.df_cur.empty:
            self.log("âš ï¸ æœ¬æœŸæ— æ•°æ®ï¼Œæ— æ³•åˆ†æ")
            return

        self.log(f"ğŸ“Š å·²åŠ è½½åŸå§‹è®°å½•: {len(self.df_cur)} æ¡")

        self.log("æ­£åœ¨èšåˆæœ¬æœŸæ•°æ®...")
        m_cur = self._calculate_net_profit(self._aggregate(self.df_cur))

        self.log("æ­£åœ¨èšåˆä¸ŠæœŸæ•°æ®...")
        m_prev = self._calculate_net_profit(self._aggregate(self.df_prev))

        tables = self.generate_full_report_suite(m_cur, m_prev, key_name="Item ID")

        self.log("æ­£åœ¨æ‰§è¡Œ AI æ™ºèƒ½è¯Šæ–­...")
        diag = ListingDiagnostician(m_cur, m_prev)
        df_diag = diag.diagnose()
        tables.append(("C1_æ™ºèƒ½è¯Šæ–­è¡¨ (AI Diagnostics)", df_diag))
        explanation_lines = diag.get_tag_definitions()

        filename = f"Profit_Analysis_Listing_{self.file_suffix}.csv"

        # ä¿å­˜é€»è¾‘
        save_path = self.save_csv(pd.DataFrame(), filename)
        if save_path:
            try:
                with open(save_path, "w", encoding="utf-8-sig") as f:
                    for name, df in tables:
                        f.write(f"=== {name} ===\n")
                        df.to_csv(f, index=False)
                        f.write("\n\n")
                    f.write("\n")
                    for line in explanation_lines:
                        f.write(f"{line}\n")
                self.log(f" Listing åˆ©æ¶¦æŠ¥è¡¨å·²ç”Ÿæˆ: {filename}")
            except Exception as e:
                self.log(f" ä¿å­˜å¤±è´¥: {e}")
==================== END FILE: core/services/finance/profit_listing.py ====================


==================== START FILE: core/services/finance/base.py ====================
# core/services/finance/base.py
"""
æ–‡ä»¶è¯´æ˜: åˆ©æ¶¦åˆ†æé€šç”¨åŸºç±» (Profit Analyzer Base) - V2.1 Fix Output Dir
ä¸»è¦åŠŸèƒ½:
1. æä¾›æ ‡å‡†åŒ–çš„æ•°æ®åŠ è½½ (_load_basics) å’Œæ¸…æ´— (_clean_data) æµç¨‹ã€‚
2. [Fix] __init__: åˆå§‹åŒ– self.output_dir (ç”¨æˆ·éš”ç¦»ç›®å½•)ï¼Œä¾›å­ç±»ä½¿ç”¨ã€‚
3. [Fix] save_csv: å¤ç”¨ self.output_dirã€‚
"""

import os
import pandas as pd
from abc import ABC
from typing import List, Tuple, Dict, Any, Optional
from datetime import date

from config.settings import settings
from core.services.etl.repository import ETLRepository
from core.services.inventory.repository import InventoryRepository
from core.sys.logger import get_logger
from core.sys.context import get_current_user


class ProfitAnalyzerBase(ABC):
    # è´¹ç”¨åˆ—å®šä¹‰
    FEE_COLUMNS = [
        "Shipping and handling", "Refund Shipping and handling",
        "Seller collected tax", "eBay collected tax",
        "Refund Seller collected tax", "Refund eBay collected tax",
        "Final Value Fee - fixed", "Final Value Fee - variable",
        "Regulatory operating fee", "International fee",
        "Charity donation", "Deposit processing fee",
        "Refund Final Value Fee - fixed", "Refund Final Value Fee - variable",
        "Refund Regulatory operating fee", "Refund International fee",
        "Refund Charity donation", "Refund Deposit processing fee",
        'Very high "item not as described" fee', 'Refund Very high "item not as described" fee',
        "Below standard performance fee", "Refund Below standard performance fee",
        "Payments dispute fee", "Promoted Listings fee", "Refund Promoted Listings fee",
        "Shipping label-Earning data", "Shipping label-Return"
    ]

    PLATFORM_FEE_GROUP = {
        "Final Value Fee - fixed", "Final Value Fee - variable",
        "Regulatory operating fee", "International fee",
        "Charity donation", "Deposit processing fee",
        "Refund Final Value Fee - fixed", "Refund Final Value Fee - variable",
        "Refund Regulatory operating fee", "Refund International fee",
        "Refund Charity donation", "Refund Deposit processing fee"
    }

    def __init__(self, start_date: date, end_date: date, file_suffix: str = ""):
        self.start_date = start_date
        self.end_date = end_date
        self.file_suffix = file_suffix
        self.logger = get_logger(self.__class__.__name__)

        self.etl_repo = ETLRepository()
        self.inv_repo = InventoryRepository()

        self.sku_cost_map = {}
        self.df_cur = pd.DataFrame()
        self.df_prev = pd.DataFrame()

        # [Fix] åˆå§‹åŒ–ç”¨æˆ·éš”ç¦»çš„è¾“å‡ºç›®å½•
        user = get_current_user()
        safe_user = "".join([c for c in user if c.isalnum() or c in ('_', '-')])
        sub_dir = safe_user if safe_user else "default"

        self.output_dir = settings.OUTPUT_DIR / sub_dir
        if not self.output_dir.exists():
            self.output_dir.mkdir(parents=True, exist_ok=True)

    def log(self, msg: str):
        self.logger.info(msg)

    def save_csv(self, df: pd.DataFrame, filename: str, footer: List[str] = None) -> str:
        """
        [é€šç”¨ä¿å­˜] ä¿å­˜ DataFrame ä¸º CSV
        """
        # [Fix] ç›´æ¥ä½¿ç”¨ __init__ ä¸­åˆå§‹åŒ–çš„ç›®å½•
        save_path = self.output_dir / filename

        is_empty = df.empty
        status_tag = "[EMPTY]" if is_empty else "[DATA]"

        try:
            with open(save_path, "w", encoding="utf-8-sig") as f:
                df.to_csv(f, index=False)
                if footer:
                    f.write("\n")
                    for line in footer:
                        f.write(f"{line}\n")

            self.log(f" {status_tag} æŠ¥è¡¨ç”Ÿæˆ: {filename}")
            return str(save_path)
        except Exception as e:
            self.logger.error(f" ä¿å­˜å¤±è´¥ [{filename}]: {e}")
            return ""

    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        if df.empty: return df
        exclude = {"action", "order date", "seller", "order number", "item id", "item title", "buyer username",
                   "full sku"}
        df = df.copy()
        for col in df.columns:
            if str(col).lower().startswith("sku"): continue
            if col in exclude: continue
            try:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
            except:
                pass
        return df

    def _load_basics(self):
        self.log("æ­£åœ¨åŠ è½½ SKU æˆæœ¬è¡¨ (Data_COGS)...")
        df_cogs = self.inv_repo.get_all_cogs()
        self.sku_cost_map = {
            str(k).strip().upper(): pd.to_numeric(v, errors='coerce')
            for k, v in zip(df_cogs["SKU"], df_cogs["Cog"])
        }

        self.log(f"æ­£åœ¨åŠ è½½æœ¬æœŸæ•°æ®: {self.start_date} -> {self.end_date}")
        self.df_cur = self._clean_data(self.etl_repo.get_transactions_by_date(self.start_date, self.end_date))

        end_dt = pd.to_datetime(self.end_date)
        start_dt = pd.to_datetime(self.start_date)
        delta = end_dt - start_dt
        prev_end = start_dt - pd.Timedelta(days=1)
        prev_start = prev_end - delta

        self.log(f"æ­£åœ¨åŠ è½½ä¸ŠæœŸæ•°æ®: {prev_start.date()} -> {prev_end.date()}")
        self.df_prev = self._clean_data(self.etl_repo.get_transactions_by_date(prev_start.date(), prev_end.date()))
        if self.df_prev.empty:
            self.log("ï¸ ä¸ŠæœŸæ•°æ®ä¸ºç©ºï¼Œç¯æ¯”å°†æ— æ³•è®¡ç®—ã€‚")

    def _accumulate_fees(self, row: dict, metrics_dict: dict, key: str, weight: float = 1.0):
        for col in self.FEE_COLUMNS:
            val = float(row.get(col, 0))
            if val != 0:
                metrics_dict[key][col] += val * weight

    def _calculate_net_profit(self, metrics: dict) -> dict:
        R = settings.LOSS_RATES
        for k, m in metrics.items():
            m["net_qty"] = (
                    m["total_qty"]
                    - m.get("cancel_qty", 0)
                    - m.get("return_qty", 0) * R.get('RETURN', 0.3)
                    - m.get("request_qty", 0) * R.get('REQUEST', 0.5)
                    - m.get("claim_qty", 0) * R.get('CASE', 0.6)
                    - m.get("dispute_qty", 0) * R.get('DISPUTE', 1.0)
            )
            m["net_rev"] = (
                    m["total_rev"] + m.get("cancel_rev", 0) + m.get("return_rev", 0) +
                    m.get("request_rev", 0) + m.get("claim_rev", 0) + m.get("dispute_rev", 0)
            )
            m["net_shipping"] = m.get("Shipping and handling", 0) + m.get("Refund Shipping and handling", 0)
            m["net_tax"] = (
                    m.get("Seller collected tax", 0) + m.get("eBay collected tax", 0) +
                    m.get("Refund Seller collected tax", 0) + m.get("Refund eBay collected tax", 0)
            )
            m["net_platform_fee"] = sum(v for field, v in m.items() if field in self.PLATFORM_FEE_GROUP)
            m["net_high_return_fee"] = m.get('Very high "item not as described" fee', 0) + m.get(
                'Refund Very high "item not as described" fee', 0)
            m["net_low_rating_fee"] = m.get("Below standard performance fee", 0) + m.get(
                "Refund Below standard performance fee", 0)
            m["net_third_party_fee"] = m.get("Payments dispute fee", 0)
            m["net_ad_fee"] = m.get("Promoted Listings fee", 0) + m.get("Refund Promoted Listings fee", 0)
            m["net_postage_cost"] = m.get("Shipping label-Earning data", 0)
            m["net_return_postage"] = m.get("Shipping label-Return", 0)

            m["profit"] = (
                    m["net_rev"] + m["cog_value"] + m["net_shipping"] +
                    m["net_platform_fee"] + m["net_high_return_fee"] + m["net_low_rating_fee"] +
                    m["net_third_party_fee"] + m["net_ad_fee"] + m["net_postage_cost"] + m["net_return_postage"]
            )
        return metrics

    def generate_full_report_suite(self, m_cur: dict, m_prev: dict, key_name: str) -> List[Tuple[str, pd.DataFrame]]:
        map_a = {"æ€»é”€é‡": "total_qty", "æ€»å–æ¶ˆæ•°": "cancel_qty", "æ€»é€€è´§æ•°(æ— å¹³å°ä»‹å…¥)": "return_qty",
                 "æ€»é€€è´§æ•°(å¹³å°ä»‹å…¥)": "request_qty", "æ€»é€€è´§æ•°(å¹³å°å¼ºåˆ¶é€€æ¬¾)": "claim_qty",
                 "å¼ºåˆ¶é€€è´§(ä»…é€€æ¬¾)": "dispute_qty", "å‡€é”€å”®": "net_qty"}
        map_b = {"æ€»é”€å”®é¢": "total_rev", "æ€»å–æ¶ˆé¢": "cancel_rev", "æ€»é€€è´§é¢(æ— å¹³å°ä»‹å…¥)": "return_rev",
                 "æ€»é€€è´§é¢(å¹³å°ä»‹å…¥)": "request_rev", "æ€»é€€è´§é¢(å¹³å°å¼ºåˆ¶é€€æ¬¾)": "claim_rev",
                 "å¼ºåˆ¶é€€æ¬¾(ä»…é€€æ¬¾)": "dispute_rev", "å‡€é”€å”®": "net_rev", "å‡€é”€å”®äº§å“æˆæœ¬": "cog_value",
                 "å‡€ä¹°å®¶æ”¯ä»˜é‚®è´¹": "net_shipping", "å‡€é”€å”®ç¨": "net_tax", "å‡€å›ºå®šå¹³å°è´¹ç”¨": "net_platform_fee",
                 "å‡€é«˜é€€è´§äº§å“ç½šæ¬¾": "net_high_return_fee", "å‡€ä½è´¦æˆ·è¯„çº§ç½šæ¬¾": "net_low_rating_fee",
                 "å‡€ç¬¬ä¸‰æ–¹æŠ•è¯‰ç½šæ¬¾": "net_third_party_fee", "å‡€å¹¿å‘Šå¼€é”€": "net_ad_fee",
                 "é€€è´§åŒ…é‚®è´¹ç”¨": "net_return_postage", "å‡€é‚®è´¹æ”¯å‡º": "net_postage_cost", "ç›ˆäº": "profit"}

        def build_df(mets, mapping):
            data = []
            for k in sorted(mets.keys()):
                row = {key_name: k}
                for label, field in mapping.items(): row[label] = round(mets[k].get(field, 0), 2)
                data.append(row)
            return pd.DataFrame(data) if data else pd.DataFrame(columns=[key_name] + list(mapping.keys()))

        df_a1 = build_df(m_cur, map_a)
        df_pa1 = build_df(m_prev, map_a)
        df_a2 = self._format_pct(self._calc_pct(df_a1, "æ€»é”€é‡"))
        df_a3 = self._calc_mom(self._calc_pct(df_a1, "æ€»é”€é‡"), self._calc_pct(df_pa1, "æ€»é”€é‡"), key_name)
        df_b1 = build_df(m_cur, map_b)
        df_pb1 = build_df(m_prev, map_b)
        df_b2 = self._format_pct(self._calc_pct(df_b1, "æ€»é”€å”®é¢"))
        df_b3 = self._calc_mom(self._calc_pct(df_b1, "æ€»é”€å”®é¢"), self._calc_pct(df_pb1, "æ€»é”€å”®é¢"), key_name)

        return [("A1_æ•°é‡è¡¨", df_a1), ("A2_æ•°é‡å æ¯”è¡¨", df_a2), ("A3_æ•°é‡ç»“æ„ç¯æ¯”è¡¨", df_a3),
                ("B1_é‡‘é¢è¡¨", df_b1), ("B2_é‡‘é¢å æ¯”è¡¨", df_b2), ("B3_è´¹ç”¨ç»“æ„ç¯æ¯”è¡¨", df_b3)]

    def _calc_pct(self, df, base_col):
        res = df.copy()
        if res.empty: return res
        for c in res.columns[1:]:
            if c != base_col: res[c] = res.apply(lambda r: 0 if r[base_col] == 0 else r[c] / r[base_col], axis=1)
        if base_col in res.columns: res[base_col] = 1.0
        return res

    def _format_pct(self, df):
        res = df.copy()
        if res.empty: return res
        for c in res.columns[1:]:
            if pd.api.types.is_numeric_dtype(res[c]): res[c] = res[c].apply(lambda x: f"{x:.2%}")
        return res

    def _calc_mom(self, cur, prev, key):
        res = cur.copy()
        if prev.empty or key not in prev.columns:
            for c in res.columns:
                if c != key: res[c] = "New"
            return res
        prev_map = prev.set_index(key)
        for idx in res.index:
            k_val = res.at[idx, key]
            for c in res.columns:
                if c == key: continue
                if k_val in prev_map.index and c in prev_map.columns:
                    try:
                        v1 = float(str(res.at[idx, c]).strip('%'))
                        v0 = float(str(prev_map.at[k_val, c]).strip('%'))
                        if v0 == 0:
                            res.at[idx, c] = "N/A"
                        else:
                            res.at[idx, c] = f"{(v1 - v0) / abs(v0):.2%}"
                    except:
                        res.at[idx, c] = "-"
                else:
                    res.at[idx, c] = "New"
        return res

    def run(self):
        pass

    def _aggregate(self, df):
        return {}
==================== END FILE: core/services/finance/base.py ====================


==================== START FILE: .pytest_cache/README.md ====================
# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.


==================== END FILE: .pytest_cache/README.md ====================


==================== START FILE: config/settings.py ====================
# config/settings.py
"""
æ–‡ä»¶è¯´æ˜: å…¨å±€é…ç½®ä¸­å¿ƒ (Settings)
ä¸»è¦åŠŸèƒ½:
1. è·¯å¾„å®šä¹‰ä¸ç¯å¢ƒåŠ è½½ã€‚
2. æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²ç”Ÿæˆã€‚
3. [V5 Upgrade] åŠ è½½ä¼ä¸šçº§å®‰å…¨ç  (Security Codes)ã€‚
4. [V5 Upgrade] æŒ‡å‘ action_registry.json (åŠ¨ä½œæ³¨å†Œè¡¨)ã€‚
"""

import os
import json
import re
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ .env ç¯å¢ƒå˜é‡
load_dotenv()


class Settings:
    # =========================================================================
    # 1. è·¯å¾„å®šä¹‰ (Path Definitions)
    # =========================================================================
    BASE_DIR = Path(__file__).resolve().parent.parent

    CONFIG_DIR = BASE_DIR / "config"
    DATA_DIR = BASE_DIR / "data"
    LOG_DIR = BASE_DIR / "logs"
    BACKUP_DIR = BASE_DIR / "backup"
    OUTPUT_DIR = BASE_DIR / "output"

    UI_DIR = BASE_DIR / "ui"
    ASSETS_DIR = UI_DIR / "assets"

    # æ•°æ®å­ç›®å½•
    ARCHIVE_DIR = DATA_DIR / "archive"
    KNOWLEDGE_BASE_DIR = DATA_DIR / "knowledge_base"
    CHAT_HISTORY_DIR = DATA_DIR / "chat_history"

    for _dir in [DATA_DIR, LOG_DIR, BACKUP_DIR, OUTPUT_DIR, ARCHIVE_DIR, KNOWLEDGE_BASE_DIR, CHAT_HISTORY_DIR,
                 ASSETS_DIR]:
        _dir.mkdir(parents=True, exist_ok=True)

    # =========================================================================
    # 2. ç‰ˆæœ¬ä¸å…ƒæ•°æ®
    # =========================================================================
    APP_NAME = "Eaglestar ERP Enterprise"
    APP_VERSION = "V2.0.0 (Dev)"
    VERSION_DATE = "Unknown Date"
    AUTHOR = "Aaron"

    PATCH_NOTES_FILE = ASSETS_DIR / "patch_notes.txt"
    PATCH_NOTES_LIST = []

    # å…¨ç«™ç®¡ç†å‘˜è´¦å·
    SUPER_ADMIN_USER = os.getenv("DEFAULT_ADMIN_USERNAME", "admin")

    def __init__(self):
        self.load_version_info()

    def load_version_info(self):
        if not self.PATCH_NOTES_FILE.exists(): return
        try:
            with open(self.PATCH_NOTES_FILE, "r", encoding="utf-8") as f:
                content = f.read().strip()
            blocks = content.split('\n\n')
            self.PATCH_NOTES_LIST = []
            for block in blocks:
                block = block.strip()
                if not block: continue
                if block.startswith("VERSION="):
                    self.APP_VERSION = block.split("=")[1].strip()
                    continue
                lines = block.split('\n', 1)
                if len(lines) < 2: continue
                header = lines[0].strip()
                desc = lines[1].strip()
                match = re.match(r"\[(.*?)\]\s+(.*)", header)
                if match:
                    ver, date_str = match.groups()
                    if ver == self.APP_VERSION:
                        self.VERSION_DATE = date_str
                    self.PATCH_NOTES_LIST.append({"ver": ver, "date": date_str, "desc": desc})
        except Exception as e:
            print(f" Error loading patch notes: {e}")

    # =========================================================================
    # 3. æ•°æ®åº“é…ç½®
    # =========================================================================
    DB_HOST = os.getenv("DB_HOST", "localhost")
    DB_PORT = int(os.getenv("DB_PORT", 3306))
    DB_USER = os.getenv("DB_USER", "root")
    DB_PASS = os.getenv("DB_PASS", "")
    DB_NAME = os.getenv("DB_NAME", "MGMT")
    DB_CHARSET = os.getenv("DB_CHARSET", "utf8mb4")

    @property
    def SQLALCHEMY_URL(self):
        return f"mysql+pymysql://{self.DB_USER}:{self.DB_PASS}@{self.DB_HOST}:{self.DB_PORT}/{self.DB_NAME}?charset={self.DB_CHARSET}"

    # =========================================================================
    # 4. [New] ä¼ä¸šçº§å®‰å…¨é…ç½® (Security V5)
    # =========================================================================

    # --- A. åŸå§‹å‡­è¯ (From .env) ---
    SEC_CODE_QUERY = os.getenv("SEC_CODE_QUERY", "1111")  # L1: æŸ¥è¯¢
    SEC_CODE_MODIFY = os.getenv("SEC_CODE_MODIFY", "2222")  # L2: ä¿®æ”¹
    SEC_CODE_DB = os.getenv("SEC_CODE_DB", "1522")  # L3: è¿ç»´
    SEC_CODE_SYSTEM = os.getenv("SEC_CODE_SYSTEM", "RedButton!")  # L4: æ ¸å¿ƒ

    # --- B. å…¼å®¹æ—§ç‰ˆ (å°†åœ¨é‡æ„å®ŒæˆååºŸå¼ƒ) ---
    DB_OPERATOR_PWD = SEC_CODE_DB
    PLATFORM_SEC_PWD = SEC_CODE_SYSTEM

    # --- C. åŠ¨ä½œæ³¨å†Œè¡¨è·¯å¾„ ---
    # å®šä¹‰äº† Module -> Tab -> Action çš„å…¨ç«™å±‚çº§
    ACTION_REGISTRY_FILE = CONFIG_DIR / "action_registry.json"

    # =========================================================================
    # 5. ä¸šåŠ¡å‚æ•°
    # =========================================================================
    LEAD_MONTH = 2.0
    MIN_SAFETY_MONTH = 1.0
    LOSS_RATES = {"CASE": 0.6, "REQUEST": 0.5, "RETURN": 0.3, "DISPUTE": 1.0}

    @classmethod
    def load_modules_config(cls):
        """åŠ è½½æ—§ç‰ˆæ¨¡å—é…ç½® (UI å¯¼èˆªç”¨)"""
        config_path = cls.CONFIG_DIR / "modules.json"
        if not config_path.exists(): return []
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f" Error loading modules.json: {e}")
            return []

    @classmethod
    def load_action_registry(cls):
        """[New] åŠ è½½åŠ¨ä½œæ³¨å†Œè¡¨ (å®‰å…¨æ§åˆ¶ç”¨)"""
        if not cls.ACTION_REGISTRY_FILE.exists(): return {}
        try:
            with open(cls.ACTION_REGISTRY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f" Error loading action_registry.json: {e}")
            return {}


settings = Settings()
==================== END FILE: config/settings.py ====================


==================== START FILE: config/action_registry.json ====================
{
  "_meta": {
    "version": "2.2",
    "description": "å…¨ç«™åŠŸèƒ½åŠ¨ä½œæ³¨å†Œè¡¨ (Four-Level Security Code System)"
  },
  "modules": [
    {
      "key": "audit",
      "name": "ğŸ“œ å®‰å…¨å®¡è®¡æ—¥å¿—",
      "tabs": [
        {
          "key": "business",
          "name": "ä¸šåŠ¡æ“ä½œæ—¥å¿—",
          "actions": [
            {
              "key": "btn_purge_business",
              "name": "ç‰¹æƒæ¸…æ´— (ä¸šåŠ¡æ—¥å¿—)",
              "default_security": ["user", "modify", "db", "system"],
              "description": "ç‰©ç†åˆ é™¤ä¸šåŠ¡æ“ä½œæ—¥å¿—ï¼Œéœ€æ ¸å¼¹çº§æƒé™ã€‚"
            },
            {
              "key": "btn_unlock_view",
              "name": "æ˜¾ç¤ºå®Œæ•´ä¿¡æ¯ (ä¸Šå¸è§†è§’)",
              "default_security": ["user", "query", "modify"],
              "description": "æŸ¥çœ‹è„±æ•æ•°æ®çš„åŸå§‹å†…å®¹ã€‚"
            }
          ]
        },
        {
          "key": "infra",
          "name": "åº•å±‚æ•°æ®å®¡è®¡",
          "actions": [
            {
              "key": "btn_purge_infra",
              "name": "ç‰¹æƒæ¸…æ´— (åº•å±‚æ—¥å¿—)",
              "default_security": ["user", "modify", "db", "system"],
              "description": "ç‰©ç†åˆ é™¤åº•å±‚æ—¥å¿—ã€‚"
            }
          ]
        },
        {
          "key": "system",
          "name": "ç³»ç»Ÿæ•…éšœæ—¥å¿—",
          "actions": [
            {
              "key": "btn_purge_system",
              "name": "ç‰¹æƒæ¸…æ´— (é”™è¯¯æ—¥å¿—)",
              "default_security": ["user", "modify", "db", "system"],
              "description": "ç‰©ç†åˆ é™¤é”™è¯¯æ—¥å¿—ã€‚"
            }
          ]
        }
      ]
    },
    {
      "key": "db_admin",
      "name": "ğŸ—„ï¸ æ•°æ®åº“è¿ç»´",
      "tabs": [
        {
          "key": "backup",
          "name": "åˆ›å»ºå¤‡ä»½",
          "actions": [
            {
              "key": "btn_create_backup",
              "name": "ç«‹å³å¤‡ä»½",
              "default_security": ["user"],
              "description": "ç”Ÿæˆå…¨é‡æ•°æ®åº“å¿«ç…§ã€‚"
            }
          ]
        },
        {
          "key": "restore",
          "name": "å¤‡ä»½æ¢å¤",
          "actions": [
            {
              "key": "btn_restore_db",
              "name": "æ‰§è¡Œæ¢å¤",
              "default_security": ["user", "db", "modify"],
              "description": "è¦†ç›–å½“å‰æ•°æ®åº“ã€‚"
            }
          ]
        },
        {
          "key": "manage",
          "name": "å†å²å¤‡ä»½ç®¡ç†",
          "actions": [
            {
              "key": "btn_delete_backup",
              "name": "åˆ é™¤å¤‡ä»½",
              "default_security": ["user", "db"],
              "description": "åˆ é™¤ç‰©ç†å¤‡ä»½æ–‡ä»¶ã€‚"
            }
          ]
        },
        {
          "key": "delete",
          "name": "æ•°æ®åˆ é™¤",
          "actions": [
            {
              "key": "btn_clean_data",
              "name": "æ‰§è¡Œæ•°æ®æ¸…é™¤",
              "default_security": ["user", "modify", "db", "system"],
              "description": "æŒ‰æ—¥æœŸèŒƒå›´ç‰©ç†æ¸…æ´—ä¸šåŠ¡æ•°æ®ã€‚"
            }
          ]
        }
      ]
    },
    {
      "key": "data_ops",
      "name": "ğŸ› ï¸ æ•°æ®ä¿®æ”¹ä¸­å¿ƒ",
      "tabs": [
        {
          "key": "inv",
          "name": "ä¿®æ”¹åº“å­˜",
          "actions": [
            {
              "key": "btn_update_single_inv",
              "name": "ä¿®æ”¹å•å“åº“å­˜",
              "default_security": ["user", "modify"],
              "description": "ä¿®æ­£å•ä¸ª SKU çš„åº“å­˜æ•°é‡ã€‚"
            },
            {
              "key": "btn_drop_inv_col",
              "name": "åˆ é™¤æ•´åˆ—åº“å­˜ (è€æ•°æ®)",
              "default_security": ["user", "modify", "db", "system"],
              "description": "ç‰©ç†åˆ é™¤å½’æ¡£æ•°æ®åˆ—ã€‚"
            },
            {
              "key": "btn_drop_inv_col_recent",
              "name": "åˆ é™¤æ•´åˆ—åº“å­˜ (è¿‘æœŸ)",
              "default_security": ["user"],
              "description": "åˆ é™¤è¿‘æœŸåˆšä¸Šä¼ çš„é”™è¯¯åˆ—ã€‚"
            }
          ]
        },
        {
          "key": "cogs",
          "name": "èµ„æ–™ç»´æŠ¤",
          "actions": [
            {
              "key": "btn_batch_update_cogs",
              "name": "æ‰¹é‡æ›´æ–°æ¡£æ¡ˆ",
              "default_security": ["user", "modify"],
              "description": "æ‰¹é‡ä¿®æ”¹ SKU æˆæœ¬ä¸å±æ€§ã€‚"
            },
            {
              "key": "btn_create_skus",
              "name": "æ‰¹é‡æ–°å¢ SKU",
              "default_security": ["user", "modify"],
              "description": "åˆ›å»ºæ–° SKU å¹¶åˆå§‹åŒ–åº“å­˜ã€‚"
            }
          ]
        }
      ]
    },
    {
      "key": "user_admin",
      "name": "ğŸ‘¤ ç”¨æˆ·æƒé™ç®¡ç†",
      "tabs": [
        {
          "key": "management",
          "name": "ç”¨æˆ·ç®¡ç†",
          "actions": [
            {
              "key": "btn_create_user",
              "name": "æ³¨å†Œæ–°ç”¨æˆ·",
              "default_security": ["user"],
              "description": "åˆ›å»ºæ–°çš„ç™»å½•è´¦å·ã€‚"
            },
            {
              "key": "btn_lock_user",
              "name": "é”å®šè´¦å·",
              "default_security": ["user"],
              "description": "ç¦æ­¢ç”¨æˆ·ç™»å½•ã€‚"
            },
            {
              "key": "btn_unlock_user",
              "name": "è§£é”è´¦å·",
              "default_security": ["user"],
              "description": "æ¢å¤ç”¨æˆ·ç™»å½•æƒé™ã€‚"
            },
            {
              "key": "btn_reset_pwd",
              "name": "é‡ç½®å¯†ç ",
              "default_security": ["user", "system"],
              "description": "å¼ºåˆ¶ä¿®æ”¹ç”¨æˆ·å¯†ç ã€‚"
            },
            {
              "key": "btn_update_perms",
              "name": "ä¿å­˜æƒé™å˜æ›´",
              "default_security": ["user"],
              "description": "ä¿®æ”¹ç”¨æˆ·çš„æ¨¡å—è®¿é—®æƒé™ã€‚"
            }
          ]
        }
      ]
    },
    {
      "key": "reports",
      "name": "ğŸ“Š å•†ä¸šæ™ºèƒ½æŠ¥è¡¨",
      "tabs": [
        {
          "key": "gen",
          "name": "ç”ŸæˆæŠ¥è¡¨",
          "actions": [
            {
              "key": "btn_generate_report",
              "name": "å¯åŠ¨åˆ†æå¼•æ“",
              "default_security": ["user"],
              "description": "æ‰§è¡Œå…¨é‡æ•°æ®æ¸…æ´—ä¸è´¢åŠ¡åˆ†æã€‚"
            }
          ]
        }
      ]
    },
    {
      "key": "etl",
      "name": "ğŸ”Œ æ•°æ®é›†æˆ (ETL)",
      "tabs": [
        {
          "key": "trans",
          "name": "äº¤æ˜“æ•°æ®",
          "actions": [
            {
              "key": "btn_start_etl_pipeline",
              "name": "å¯åŠ¨ ETL æµæ°´çº¿",
              "default_security": ["user"],
              "description": "æ‰§è¡Œæ•°æ®æ‘„å…¥ã€è§£æä¸è½¬æ¢ã€‚"
            },
            {
              "key": "btn_commit_sku_fix",
              "name": "æäº¤ SKU ä¿®å¤",
              "default_security": ["user"],
              "description": "ä¿å­˜äººå·¥æ¸…æ´—ç»“æœå¹¶åº”ç”¨åˆ°æ•°æ®åº“ã€‚"
            }
          ]
        },
        {
          "key": "inv",
          "name": "åº“å­˜åŒæ­¥",
          "actions": [
            {
              "key": "btn_sync_inventory",
              "name": "æ‰§è¡Œåº“å­˜åŒæ­¥",
              "default_security": ["user", "db"],
              "description": "å°†ç›˜ç‚¹æ•°æ®æ‰¹é‡å†™å…¥æ•°æ®åº“ã€‚"
            }
          ]
        }
      ]
    }
  ]
}
==================== END FILE: config/action_registry.json ====================


==================== START FILE: config/modules.json ====================
[
  {
    "key": "home",
    "name": "ğŸ  ç³»ç»Ÿé¦–é¡µ",
    "path": "ui.pages.home",
    "permission": "public",
    "enabled": true
  },
  {
    "key": "etl",
    "name": "ğŸ”Œ æ•°æ®é›†æˆ (ETL)",
    "path": "ui.pages.etl_ingest",
    "permission": "module.etl",
    "enabled": true,
    "tabs": [
      {
        "key": "trans",
        "name": "ğŸ“Š äº¤æ˜“æ•°æ® (Transaction)",
        "path": "ui.pages.etl_ingest.trans_wizard",
        "permission": "module.etl.trans"
      },
      {
        "key": "inv",
        "name": "ğŸ“¦ åº“å­˜åŒæ­¥ (Inventory)",
        "path": "ui.pages.etl_ingest.inv_wizard",
        "permission": "module.etl.inv"
      }
    ]
  },
  {
    "key": "reports",
    "name": "ğŸ“Š å•†ä¸šæ™ºèƒ½æŠ¥è¡¨",
    "path": "ui.pages.reports",
    "permission": "module.reports",
    "enabled": true,
    "tabs": [
      {
        "key": "gen",
        "name": "ğŸš€ ç”ŸæˆæŠ¥è¡¨ (Generator)",
        "path": "ui.pages.reports.generator",
        "permission": "module.reports.gen"
      },
      {
        "key": "center",
        "name": "ğŸ“‚ æŠ¥è¡¨ä¸­å¿ƒ (Center)",
        "path": "ui.pages.reports.center",
        "permission": "module.reports.center"
      }
    ]
  },
  {
    "key": "visuals",
    "name": "ğŸ“ˆ æ•°æ®äº¤äº’å¯è§†åŒ–",
    "path": "ui.pages.data_visualization",
    "permission": "module.visuals",
    "enabled": true
  },
  {
    "key": "data_ops",
    "name": "ğŸ› ï¸ æ•°æ®ä¿®æ”¹ä¸­å¿ƒ",
    "path": "ui.pages.db_data_change",
    "permission": "module.db_modify",
    "enabled": true,
    "tabs": [
      {
        "key": "inv",
        "name": "ğŸ“¦ ä¿®æ”¹åº“å­˜",
        "path": "ui.pages.db_data_change.tab_inv",
        "permission": "module.db_modify.inv"
      },
      {
        "key": "cogs",
        "name": "ğŸ“ èµ„æ–™ç»´æŠ¤",
        "path": "ui.pages.db_data_change.tab_cogs",
        "permission": "module.db_modify.cogs"
      }
    ]
  },
  {
    "key": "db_admin",
    "name": "ğŸ—„ï¸ æ•°æ®åº“è¿ç»´",
    "path": "ui.pages.db_admin",
    "permission": "module.db_admin",
    "enabled": true,
    "tabs": [
      {
        "key": "backup",
        "name": "ğŸ“¤ åˆ›å»ºå¤‡ä»½",
        "path": "ui.pages.db_admin.tab_backup",
        "permission": "module.db_admin.backup"
      },
      {
        "key": "restore",
        "name": "ğŸ“¥ å¤‡ä»½æ¢å¤",
        "path": "ui.pages.db_admin.tab_restore",
        "permission": "module.db_admin.restore"
      },
      {
        "key": "manage",
        "name": "ğŸ—‚ï¸ å†å²å¤‡ä»½ç®¡ç†",
        "path": "ui.pages.db_admin.tab_manage",
        "permission": "module.db_admin.manage"
      },
      {
        "key": "delete",
        "name": "â˜¢ï¸ æ•°æ®åˆ é™¤ (Data Deletion)",
        "path": "ui.pages.db_admin.tab_delete",
        "permission": "module.db_admin.delete"
      }
    ]
  },
  {
    "key": "user_admin",
    "name": "ğŸ‘¤ ç”¨æˆ·æƒé™ç®¡ç†",
    "path": "ui.pages.user_admin",
    "permission": "admin_only",
    "enabled": true
  },
  {
    "key": "audit",
    "name": "ğŸ“œ å®‰å…¨å®¡è®¡æ—¥å¿—",
    "path": "ui.pages.audit_logs",
    "permission": "admin_only",
    "enabled": true,
    "tabs": [
      {
        "key": "business",
        "name": "ğŸ“¢ ä¸šåŠ¡æ“ä½œæ—¥å¿—",
        "permission": "module.audit.business"
      },
      {
        "key": "infra",
        "name": "ğŸ›¡ï¸ åº•å±‚æ•°æ®å®¡è®¡",
        "permission": "module.audit.infra"
      },
      {
        "key": "system",
        "name": "ğŸ ç³»ç»Ÿæ•…éšœæ—¥å¿—",
        "permission": "module.audit.system"
      }
    ]
  },
  {
    "key": "gemini",
    "name": "ğŸ¤– AI æ™ºèƒ½åŠ©æ‰‹",
    "path": "ui.pages.ai_gemini",
    "permission": "public",
    "enabled": true
  }
]
==================== END FILE: config/modules.json ====================


==================== START FILE: #/Eaglestar ERP-darwin-arm64/Eaglestar ERP.app/Contents/Resources/app/nativefier.json ====================
{"accessibilityPrompt":true,"alwaysOnTop":false,"arch":"arm64","asar":false,"blockExternalUrls":false,"bounce":false,"buildDate":1765004442632,"clearCache":false,"counter":false,"darwinDarkModeSupport":false,"disableContextMenu":false,"disableDevTools":false,"disableGpu":false,"disableOldBuildWarning":false,"diskCacheSize":104857600,"electronVersionUsed":"25.7.0","enableEs3Apis":false,"fastQuit":false,"fullScreen":false,"height":900,"hideWindowFrame":false,"ignoreCertificate":false,"ignoreGpuBlacklist":false,"insecure":false,"isUpgrade":false,"maximize":false,"name":"Eaglestar ERP","nativefierVersion":"52.0.0","portable":false,"quiet":false,"showMenuBar":false,"singleInstance":true,"strictInternalUrls":false,"targetUrl":"https://erp.topmorrowusa.com/","tray":"false","userAgentHonest":false,"width":1440,"widevine":false,"win32metadata":{"ProductName":"Eaglestar ERP","InternalName":"Eaglestar ERP","FileDescription":"Eaglestar ERP"},"zoom":1,"oldBuildWarningText":""}

==================== END FILE: #/Eaglestar ERP-darwin-arm64/Eaglestar ERP.app/Contents/Resources/app/nativefier.json ====================


==================== START FILE: #/Eaglestar ERP-darwin-arm64/Eaglestar ERP.app/Contents/Resources/app/npm-shrinkwrap.json ====================
{
  "name": "nativefier-placeholder",
  "version": "1.0.0",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "name": "nativefier-placeholder",
      "version": "1.0.0",
      "license": "MIT",
      "dependencies": {
        "electron-context-menu": "^3.6.1",
        "electron-dl": "^3.5.0",
        "electron-squirrel-startup": "^1.0.0",
        "electron-window-state": "^5.0.3",
        "loglevel": "^1.8.1",
        "source-map-support": "^0.5.21"
      },
      "devDependencies": {
        "electron": "^25.7.0"
      }
    },
    "node_modules/@electron/get": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/@electron/get/-/get-2.0.2.tgz",
      "integrity": "sha512-eFZVFoRXb3GFGd7Ak7W4+6jBl9wBtiZ4AaYOse97ej6mKj5tkyO0dUnUChs1IhJZtx1BENo4/p4WUTXpi6vT+g==",
      "dev": true,
      "dependencies": {
        "debug": "^4.1.1",
        "env-paths": "^2.2.0",
        "fs-extra": "^8.1.0",
        "got": "^11.8.5",
        "progress": "^2.0.3",
        "semver": "^6.2.0",
        "sumchecker": "^3.0.1"
      },
      "engines": {
        "node": ">=12"
      },
      "optionalDependencies": {
        "global-agent": "^3.0.0"
      }
    },
    "node_modules/@sindresorhus/is": {
      "version": "4.6.0",
      "resolved": "https://registry.npmjs.org/@sindresorhus/is/-/is-4.6.0.tgz",
      "integrity": "sha512-t09vSN3MdfsyCHoFcTRCH/iUtG7OJ0CsjzB8cjAmKc/va/kIgeDI/TxsigdncE/4be734m0cvIYwNaV4i2XqAw==",
      "dev": true,
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sindresorhus/is?sponsor=1"
      }
    },
    "node_modules/@szmarczak/http-timer": {
      "version": "4.0.6",
      "resolved": "https://registry.npmjs.org/@szmarczak/http-timer/-/http-timer-4.0.6.tgz",
      "integrity": "sha512-4BAffykYOgO+5nzBWYwE3W90sBgLJoUPRWWcL8wlyiM8IB8ipJz3UMJ9KXQd1RKQXpKp8Tutn80HZtWsu2u76w==",
      "dev": true,
      "dependencies": {
        "defer-to-connect": "^2.0.0"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/@types/cacheable-request": {
      "version": "6.0.3",
      "resolved": "https://registry.npmjs.org/@types/cacheable-request/-/cacheable-request-6.0.3.tgz",
      "integrity": "sha512-IQ3EbTzGxIigb1I3qPZc1rWJnH0BmSKv5QYTalEwweFvyBDLSAe24zP0le/hyi7ecGfZVlIVAg4BZqb8WBwKqw==",
      "dev": true,
      "dependencies": {
        "@types/http-cache-semantics": "*",
        "@types/keyv": "^3.1.4",
        "@types/node": "*",
        "@types/responselike": "^1.0.0"
      }
    },
    "node_modules/@types/http-cache-semantics": {
      "version": "4.0.1",
      "resolved": "https://registry.npmjs.org/@types/http-cache-semantics/-/http-cache-semantics-4.0.1.tgz",
      "integrity": "sha512-SZs7ekbP8CN0txVG2xVRH6EgKmEm31BOxA07vkFaETzZz1xh+cbt8BcI0slpymvwhx5dlFnQG2rTlPVQn+iRPQ==",
      "dev": true
    },
    "node_modules/@types/keyv": {
      "version": "3.1.4",
      "resolved": "https://registry.npmjs.org/@types/keyv/-/keyv-3.1.4.tgz",
      "integrity": "sha512-BQ5aZNSCpj7D6K2ksrRCTmKRLEpnPvWDiLPfoGyhZ++8YtiK9d/3DBKPJgry359X/P1PfruyYwvnvwFjuEiEIg==",
      "dev": true,
      "dependencies": {
        "@types/node": "*"
      }
    },
    "node_modules/@types/node": {
      "version": "18.17.11",
      "resolved": "https://registry.npmjs.org/@types/node/-/node-18.17.11.tgz",
      "integrity": "sha512-r3hjHPBu+3LzbGBa8DHnr/KAeTEEOrahkcL+cZc4MaBMTM+mk8LtXR+zw+nqfjuDZZzYTYgTcpHuP+BEQk069g==",
      "dev": true
    },
    "node_modules/@types/responselike": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/@types/responselike/-/responselike-1.0.0.tgz",
      "integrity": "sha512-85Y2BjiufFzaMIlvJDvTTB8Fxl2xfLo4HgmHzVBz08w4wDePCTjYw66PdrolO0kzli3yam/YCgRufyo1DdQVTA==",
      "dev": true,
      "dependencies": {
        "@types/node": "*"
      }
    },
    "node_modules/@types/yauzl": {
      "version": "2.10.0",
      "resolved": "https://registry.npmjs.org/@types/yauzl/-/yauzl-2.10.0.tgz",
      "integrity": "sha512-Cn6WYCm0tXv8p6k+A8PvbDG763EDpBoTzHdA+Q/MF6H3sapGjCm9NzoaJncJS9tUKSuCoDs9XHxYYsQDgxR6kw==",
      "dev": true,
      "optional": true,
      "dependencies": {
        "@types/node": "*"
      }
    },
    "node_modules/ansi-regex": {
      "version": "5.0.1",
      "resolved": "https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz",
      "integrity": "sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/ansi-styles": {
      "version": "4.3.0",
      "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-4.3.0.tgz",
      "integrity": "sha512-zbB9rCJAT1rbjiVDb2hqKFHNYLxgtk8NURxZ3IZwD3F6NtxbXZQCnnSi1Lkx+IDohdPlFp222wVALIheZJQSEg==",
      "dependencies": {
        "color-convert": "^2.0.1"
      },
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/chalk/ansi-styles?sponsor=1"
      }
    },
    "node_modules/astral-regex": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/astral-regex/-/astral-regex-2.0.0.tgz",
      "integrity": "sha512-Z7tMw1ytTXt5jqMcOP+OQteU1VuNK9Y02uuJtKQ1Sv69jXQKKg5cibLwGJow8yzZP+eAc18EmLGPal0bp36rvQ==",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/boolean": {
      "version": "3.2.0",
      "resolved": "https://registry.npmjs.org/boolean/-/boolean-3.2.0.tgz",
      "integrity": "sha512-d0II/GO9uf9lfUHH2BQsjxzRJZBdsjgsBiW4BvhWk/3qoKwQFjIDVN19PfX8F2D/r9PCMTtLWjYVCFrpeYUzsw==",
      "dev": true,
      "optional": true
    },
    "node_modules/buffer-crc32": {
      "version": "0.2.13",
      "resolved": "https://registry.npmjs.org/buffer-crc32/-/buffer-crc32-0.2.13.tgz",
      "integrity": "sha512-VO9Ht/+p3SN7SKWqcrgEzjGbRSJYTx+Q1pTQC0wrWqHx0vpJraQ6GtHx8tvcg1rlK1byhU5gccxgOgj7B0TDkQ==",
      "dev": true,
      "engines": {
        "node": "*"
      }
    },
    "node_modules/buffer-from": {
      "version": "1.1.2",
      "resolved": "https://registry.npmjs.org/buffer-from/-/buffer-from-1.1.2.tgz",
      "integrity": "sha512-E+XQCRwSbaaiChtv6k6Dwgc+bx+Bs6vuKJHHl5kox/BaKbhiXzqQOwK4cO22yElGp2OCmjwVhT3HmxgyPGnJfQ=="
    },
    "node_modules/cacheable-lookup": {
      "version": "5.0.4",
      "resolved": "https://registry.npmjs.org/cacheable-lookup/-/cacheable-lookup-5.0.4.tgz",
      "integrity": "sha512-2/kNscPhpcxrOigMZzbiWF7dz8ilhb/nIHU3EyZiXWXpeq/au8qJ8VhdftMkty3n7Gj6HIGalQG8oiBNB3AJgA==",
      "dev": true,
      "engines": {
        "node": ">=10.6.0"
      }
    },
    "node_modules/cacheable-request": {
      "version": "7.0.4",
      "resolved": "https://registry.npmjs.org/cacheable-request/-/cacheable-request-7.0.4.tgz",
      "integrity": "sha512-v+p6ongsrp0yTGbJXjgxPow2+DL93DASP4kXCDKb8/bwRtt9OEF3whggkkDkGNzgcWy2XaF4a8nZglC7uElscg==",
      "dev": true,
      "dependencies": {
        "clone-response": "^1.0.2",
        "get-stream": "^5.1.0",
        "http-cache-semantics": "^4.0.0",
        "keyv": "^4.0.0",
        "lowercase-keys": "^2.0.0",
        "normalize-url": "^6.0.1",
        "responselike": "^2.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/cli-truncate": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/cli-truncate/-/cli-truncate-2.1.0.tgz",
      "integrity": "sha512-n8fOixwDD6b/ObinzTrp1ZKFzbgvKZvuz/TvejnLn1aQfC6r52XEx85FmuC+3HI+JM7coBRXUvNqEU2PHVrHpg==",
      "dependencies": {
        "slice-ansi": "^3.0.0",
        "string-width": "^4.2.0"
      },
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/clone-response": {
      "version": "1.0.3",
      "resolved": "https://registry.npmjs.org/clone-response/-/clone-response-1.0.3.tgz",
      "integrity": "sha512-ROoL94jJH2dUVML2Y/5PEDNaSHgeOdSDicUyS7izcF63G6sTc/FTjLub4b8Il9S8S0beOfYt0TaA5qvFK+w0wA==",
      "dev": true,
      "dependencies": {
        "mimic-response": "^1.0.0"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/color-convert": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/color-convert/-/color-convert-2.0.1.tgz",
      "integrity": "sha512-RRECPsj7iu/xb5oKYcsFHSppFNnsj/52OVTRKb4zP5onXwVF3zVmmToNcOfGC+CRDpfK/U584fMg38ZHCaElKQ==",
      "dependencies": {
        "color-name": "~1.1.4"
      },
      "engines": {
        "node": ">=7.0.0"
      }
    },
    "node_modules/color-name": {
      "version": "1.1.4",
      "resolved": "https://registry.npmjs.org/color-name/-/color-name-1.1.4.tgz",
      "integrity": "sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA=="
    },
    "node_modules/debug": {
      "version": "4.3.4",
      "resolved": "https://registry.npmjs.org/debug/-/debug-4.3.4.tgz",
      "integrity": "sha512-PRWFHuSU3eDtQJPvnNY7Jcket1j0t5OuOsFzPPzsekD52Zl8qUfFIPEiswXqIvHWGVHOgX+7G/vCNNhehwxfkQ==",
      "dev": true,
      "dependencies": {
        "ms": "2.1.2"
      },
      "engines": {
        "node": ">=6.0"
      },
      "peerDependenciesMeta": {
        "supports-color": {
          "optional": true
        }
      }
    },
    "node_modules/decompress-response": {
      "version": "6.0.0",
      "resolved": "https://registry.npmjs.org/decompress-response/-/decompress-response-6.0.0.tgz",
      "integrity": "sha512-aW35yZM6Bb/4oJlZncMH2LCoZtJXTRxES17vE3hoRiowU2kWHaJKFkSBDnDR+cm9J+9QhXmREyIfv0pji9ejCQ==",
      "dev": true,
      "dependencies": {
        "mimic-response": "^3.1.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/decompress-response/node_modules/mimic-response": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/mimic-response/-/mimic-response-3.1.0.tgz",
      "integrity": "sha512-z0yWI+4FDrrweS8Zmt4Ej5HdJmky15+L2e6Wgn3+iK5fWzb6T3fhNFq2+MeTRb064c6Wr4N/wv0DzQTjNzHNGQ==",
      "dev": true,
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/defer-to-connect": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/defer-to-connect/-/defer-to-connect-2.0.1.tgz",
      "integrity": "sha512-4tvttepXG1VaYGrRibk5EwJd1t4udunSOVMdLSAL6mId1ix438oPwPZMALY41FCijukO1L0twNcGsdzS7dHgDg==",
      "dev": true,
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/define-properties": {
      "version": "1.2.0",
      "resolved": "https://registry.npmjs.org/define-properties/-/define-properties-1.2.0.tgz",
      "integrity": "sha512-xvqAVKGfT1+UAvPwKTVw/njhdQ8ZhXK4lI0bCIuCMrp2up9nPnaDftrLtmpTazqd1o+UY4zgzU+avtMbDP+ldA==",
      "dev": true,
      "optional": true,
      "dependencies": {
        "has-property-descriptors": "^1.0.0",
        "object-keys": "^1.1.1"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/detect-node": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/detect-node/-/detect-node-2.1.0.tgz",
      "integrity": "sha512-T0NIuQpnTvFDATNuHN5roPwSBG83rFsuO+MXXH9/3N1eFbn4wcPjttvjMLEPWJ0RGUYgQE7cGgS3tNxbqCGM7g==",
      "dev": true,
      "optional": true
    },
    "node_modules/electron": {
      "version": "25.7.0",
      "resolved": "https://registry.npmjs.org/electron/-/electron-25.7.0.tgz",
      "integrity": "sha512-P82EzYZ8k9J21x5syhXV7EkezDmEXwycReXnagfzS0kwepnrlWzq1aDIUWdNvzTdHobky4m/nYcL98qd73mEVA==",
      "dev": true,
      "hasInstallScript": true,
      "dependencies": {
        "@electron/get": "^2.0.0",
        "@types/node": "^18.11.18",
        "extract-zip": "^2.0.1"
      },
      "bin": {
        "electron": "cli.js"
      },
      "engines": {
        "node": ">= 12.20.55"
      }
    },
    "node_modules/electron-context-menu": {
      "version": "3.6.1",
      "resolved": "https://registry.npmjs.org/electron-context-menu/-/electron-context-menu-3.6.1.tgz",
      "integrity": "sha512-lcpO6tzzKUROeirhzBjdBWNqayEThmdW+2I2s6H6QMrwqTVyT3EK47jW3Nxm60KTxl5/bWfEoIruoUNn57/QkQ==",
      "dependencies": {
        "cli-truncate": "^2.1.0",
        "electron-dl": "^3.2.1",
        "electron-is-dev": "^2.0.0"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/electron-dl": {
      "version": "3.5.0",
      "resolved": "https://registry.npmjs.org/electron-dl/-/electron-dl-3.5.0.tgz",
      "integrity": "sha512-Oj+VSuScVx8hEKM2HEvTQswTX6G3MLh7UoAz/oZuvKyNDfudNi1zY6PK/UnFoK1nCl9DF6k+3PFwElKbtZlDig==",
      "dependencies": {
        "ext-name": "^5.0.0",
        "pupa": "^2.0.1",
        "unused-filename": "^2.1.0"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/electron-is-dev": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/electron-is-dev/-/electron-is-dev-2.0.0.tgz",
      "integrity": "sha512-3X99K852Yoqu9AcW50qz3ibYBWY79/pBhlMCab8ToEWS48R0T9tyxRiQhwylE7zQdXrMnx2JKqUJyMPmt5FBqA==",
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/electron-squirrel-startup": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/electron-squirrel-startup/-/electron-squirrel-startup-1.0.0.tgz",
      "integrity": "sha512-Oce8mvgGdFmwr+DsAcXBmFK8jFfN6yaFAP9IvyhTfupM3nFkBku/7VS/mdtJteWumImkC6P+BKGsxScoDDkv9Q==",
      "dependencies": {
        "debug": "^2.2.0"
      }
    },
    "node_modules/electron-squirrel-startup/node_modules/debug": {
      "version": "2.6.9",
      "resolved": "https://registry.npmjs.org/debug/-/debug-2.6.9.tgz",
      "integrity": "sha512-bC7ElrdJaJnPbAP+1EotYvqZsb3ecl5wi6Bfi6BJTUcNowp6cvspg0jXznRTKDjm/E7AdgFBVeAPVMNcKGsHMA==",
      "dependencies": {
        "ms": "2.0.0"
      }
    },
    "node_modules/electron-squirrel-startup/node_modules/ms": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/ms/-/ms-2.0.0.tgz",
      "integrity": "sha512-Tpp60P6IUJDTuOq/5Z8cdskzJujfwqfOTkrwIwj7IRISpnkJnT6SyJ4PCPnGMoFjC9ddhal5KVIYtAt97ix05A=="
    },
    "node_modules/electron-window-state": {
      "version": "5.0.3",
      "resolved": "https://registry.npmjs.org/electron-window-state/-/electron-window-state-5.0.3.tgz",
      "integrity": "sha512-1mNTwCfkolXl3kMf50yW3vE2lZj0y92P/HYWFBrb+v2S/pCka5mdwN3cagKm458A7NjndSwijynXgcLWRodsVg==",
      "dependencies": {
        "jsonfile": "^4.0.0",
        "mkdirp": "^0.5.1"
      },
      "engines": {
        "node": ">=8.0.0"
      }
    },
    "node_modules/emoji-regex": {
      "version": "8.0.0",
      "resolved": "https://registry.npmjs.org/emoji-regex/-/emoji-regex-8.0.0.tgz",
      "integrity": "sha512-MSjYzcWNOA0ewAHpz0MxpYFvwg6yjy1NG3xteoqz644VCo/RPgnr1/GGt+ic3iJTzQ8Eu3TdM14SawnVUmGE6A=="
    },
    "node_modules/end-of-stream": {
      "version": "1.4.4",
      "resolved": "https://registry.npmjs.org/end-of-stream/-/end-of-stream-1.4.4.tgz",
      "integrity": "sha512-+uw1inIHVPQoaVuHzRyXd21icM+cnt4CzD5rW+NC1wjOUSTOs+Te7FOv7AhN7vS9x/oIyhLP5PR1H+phQAHu5Q==",
      "dev": true,
      "dependencies": {
        "once": "^1.4.0"
      }
    },
    "node_modules/env-paths": {
      "version": "2.2.1",
      "resolved": "https://registry.npmjs.org/env-paths/-/env-paths-2.2.1.tgz",
      "integrity": "sha512-+h1lkLKhZMTYjog1VEpJNG7NZJWcuc2DDk/qsqSTRRCOXiLjeQ1d1/udrUGhqMxUgAlwKNZ0cf2uqan5GLuS2A==",
      "dev": true,
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/es6-error": {
      "version": "4.1.1",
      "resolved": "https://registry.npmjs.org/es6-error/-/es6-error-4.1.1.tgz",
      "integrity": "sha512-Um/+FxMr9CISWh0bi5Zv0iOD+4cFh5qLeks1qhAopKVAJw3drgKbKySikp7wGhDL0HPeaja0P5ULZrxLkniUVg==",
      "dev": true,
      "optional": true
    },
    "node_modules/escape-goat": {
      "version": "2.1.1",
      "resolved": "https://registry.npmjs.org/escape-goat/-/escape-goat-2.1.1.tgz",
      "integrity": "sha512-8/uIhbG12Csjy2JEW7D9pHbreaVaS/OpN3ycnyvElTdwM5n6GY6W6e2IPemfvGZeUMqZ9A/3GqIZMgKnBhAw/Q==",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/escape-string-regexp": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/escape-string-regexp/-/escape-string-regexp-4.0.0.tgz",
      "integrity": "sha512-TtpcNJ3XAzx3Gq8sWRzJaVajRs0uVxA2YAkdb1jm2YkPz4G6egUFAyA3n5vtEIZefPk5Wa4UXbKuS5fKkJWdgA==",
      "dev": true,
      "optional": true,
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/ext-list": {
      "version": "2.2.2",
      "resolved": "https://registry.npmjs.org/ext-list/-/ext-list-2.2.2.tgz",
      "integrity": "sha512-u+SQgsubraE6zItfVA0tBuCBhfU9ogSRnsvygI7wht9TS510oLkBRXBsqopeUG/GBOIQyKZO9wjTqIu/sf5zFA==",
      "dependencies": {
        "mime-db": "^1.28.0"
      },
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/ext-name": {
      "version": "5.0.0",
      "resolved": "https://registry.npmjs.org/ext-name/-/ext-name-5.0.0.tgz",
      "integrity": "sha512-yblEwXAbGv1VQDmow7s38W77hzAgJAO50ztBLMcUyUBfxv1HC+LGwtiEN+Co6LtlqT/5uwVOxsD4TNIilWhwdQ==",
      "dependencies": {
        "ext-list": "^2.0.0",
        "sort-keys-length": "^1.0.0"
      },
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/extract-zip": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/extract-zip/-/extract-zip-2.0.1.tgz",
      "integrity": "sha512-GDhU9ntwuKyGXdZBUgTIe+vXnWj0fppUEtMDL0+idd5Sta8TGpHssn/eusA9mrPr9qNDym6SxAYZjNvCn/9RBg==",
      "dev": true,
      "dependencies": {
        "debug": "^4.1.1",
        "get-stream": "^5.1.0",
        "yauzl": "^2.10.0"
      },
      "bin": {
        "extract-zip": "cli.js"
      },
      "engines": {
        "node": ">= 10.17.0"
      },
      "optionalDependencies": {
        "@types/yauzl": "^2.9.1"
      }
    },
    "node_modules/fd-slicer": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/fd-slicer/-/fd-slicer-1.1.0.tgz",
      "integrity": "sha512-cE1qsB/VwyQozZ+q1dGxR8LBYNZeofhEdUNGSMbQD3Gw2lAzX9Zb3uIU6Ebc/Fmyjo9AWWfnn0AUCHqtevs/8g==",
      "dev": true,
      "dependencies": {
        "pend": "~1.2.0"
      }
    },
    "node_modules/fs-extra": {
      "version": "8.1.0",
      "resolved": "https://registry.npmjs.org/fs-extra/-/fs-extra-8.1.0.tgz",
      "integrity": "sha512-yhlQgA6mnOJUKOsRUFsgJdQCvkKhcz8tlZG5HBQfReYZy46OwLcY+Zia0mtdHsOo9y/hP+CxMN0TU9QxoOtG4g==",
      "dev": true,
      "dependencies": {
        "graceful-fs": "^4.2.0",
        "jsonfile": "^4.0.0",
        "universalify": "^0.1.0"
      },
      "engines": {
        "node": ">=6 <7 || >=8"
      }
    },
    "node_modules/function-bind": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/function-bind/-/function-bind-1.1.1.tgz",
      "integrity": "sha512-yIovAzMX49sF8Yl58fSCWJ5svSLuaibPxXQJFLmBObTuCr0Mf1KiPopGM9NiFjiYBCbfaa2Fh6breQ6ANVTI0A==",
      "dev": true,
      "optional": true
    },
    "node_modules/get-intrinsic": {
      "version": "1.2.1",
      "resolved": "https://registry.npmjs.org/get-intrinsic/-/get-intrinsic-1.2.1.tgz",
      "integrity": "sha512-2DcsyfABl+gVHEfCOaTrWgyt+tb6MSEGmKq+kI5HwLbIYgjgmMcV8KQ41uaKz1xxUcn9tJtgFbQUEVcEbd0FYw==",
      "dev": true,
      "optional": true,
      "dependencies": {
        "function-bind": "^1.1.1",
        "has": "^1.0.3",
        "has-proto": "^1.0.1",
        "has-symbols": "^1.0.3"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/get-stream": {
      "version": "5.2.0",
      "resolved": "https://registry.npmjs.org/get-stream/-/get-stream-5.2.0.tgz",
      "integrity": "sha512-nBF+F1rAZVCu/p7rjzgA+Yb4lfYXrpl7a6VmJrU8wF9I1CKvP/QwPNZHnOlwbTkY6dvtFIzFMSyQXbLoTQPRpA==",
      "dev": true,
      "dependencies": {
        "pump": "^3.0.0"
      },
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/global-agent": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/global-agent/-/global-agent-3.0.0.tgz",
      "integrity": "sha512-PT6XReJ+D07JvGoxQMkT6qji/jVNfX/h364XHZOWeRzy64sSFr+xJ5OX7LI3b4MPQzdL4H8Y8M0xzPpsVMwA8Q==",
      "dev": true,
      "optional": true,
      "dependencies": {
        "boolean": "^3.0.1",
        "es6-error": "^4.1.1",
        "matcher": "^3.0.0",
        "roarr": "^2.15.3",
        "semver": "^7.3.2",
        "serialize-error": "^7.0.1"
      },
      "engines": {
        "node": ">=10.0"
      }
    },
    "node_modules/global-agent/node_modules/semver": {
      "version": "7.5.4",
      "resolved": "https://registry.npmjs.org/semver/-/semver-7.5.4.tgz",
      "integrity": "sha512-1bCSESV6Pv+i21Hvpxp3Dx+pSD8lIPt8uVjRrxAUt/nbswYc+tK6Y2btiULjd4+fnq15PX+nqQDC7Oft7WkwcA==",
      "dev": true,
      "optional": true,
      "dependencies": {
        "lru-cache": "^6.0.0"
      },
      "bin": {
        "semver": "bin/semver.js"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/globalthis": {
      "version": "1.0.3",
      "resolved": "https://registry.npmjs.org/globalthis/-/globalthis-1.0.3.tgz",
      "integrity": "sha512-sFdI5LyBiNTHjRd7cGPWapiHWMOXKyuBNX/cWJ3NfzrZQVa8GI/8cofCl74AOVqq9W5kNmguTIzJ/1s2gyI9wA==",
      "dev": true,
      "optional": true,
      "dependencies": {
        "define-properties": "^1.1.3"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/got": {
      "version": "11.8.6",
      "resolved": "https://registry.npmjs.org/got/-/got-11.8.6.tgz",
      "integrity": "sha512-6tfZ91bOr7bOXnK7PRDCGBLa1H4U080YHNaAQ2KsMGlLEzRbk44nsZF2E1IeRc3vtJHPVbKCYgdFbaGO2ljd8g==",
      "dev": true,
      "dependencies": {
        "@sindresorhus/is": "^4.0.0",
        "@szmarczak/http-timer": "^4.0.5",
        "@types/cacheable-request": "^6.0.1",
        "@types/responselike": "^1.0.0",
        "cacheable-lookup": "^5.0.3",
        "cacheable-request": "^7.0.2",
        "decompress-response": "^6.0.0",
        "http2-wrapper": "^1.0.0-beta.5.2",
        "lowercase-keys": "^2.0.0",
        "p-cancelable": "^2.0.0",
        "responselike": "^2.0.0"
      },
      "engines": {
        "node": ">=10.19.0"
      },
      "funding": {
        "url": "https://github.com/sindresorhus/got?sponsor=1"
      }
    },
    "node_modules/graceful-fs": {
      "version": "4.2.11",
      "resolved": "https://registry.npmjs.org/graceful-fs/-/graceful-fs-4.2.11.tgz",
      "integrity": "sha512-RbJ5/jmFcNNCcDV5o9eTnBLJ/HszWV0P73bc+Ff4nS/rJj+YaS6IGyiOL0VoBYX+l1Wrl3k63h/KrH+nhJ0XvQ==",
      "devOptional": true
    },
    "node_modules/has": {
      "version": "1.0.3",
      "resolved": "https://registry.npmjs.org/has/-/has-1.0.3.tgz",
      "integrity": "sha512-f2dvO0VU6Oej7RkWJGrehjbzMAjFp5/VKPp5tTpWIV4JHHZK1/BxbFRtf/siA2SWTe09caDmVtYYzWEIbBS4zw==",
      "dev": true,
      "optional": true,
      "dependencies": {
        "function-bind": "^1.1.1"
      },
      "engines": {
        "node": ">= 0.4.0"
      }
    },
    "node_modules/has-property-descriptors": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/has-property-descriptors/-/has-property-descriptors-1.0.0.tgz",
      "integrity": "sha512-62DVLZGoiEBDHQyqG4w9xCuZ7eJEwNmJRWw2VY84Oedb7WFcA27fiEVe8oUQx9hAUJ4ekurquucTGwsyO1XGdQ==",
      "dev": true,
      "optional": true,
      "dependencies": {
        "get-intrinsic": "^1.1.1"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/has-proto": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/has-proto/-/has-proto-1.0.1.tgz",
      "integrity": "sha512-7qE+iP+O+bgF9clE5+UoBFzE65mlBiVj3tKCrlNQ0Ogwm0BjpT/gK4SlLYDMybDh5I3TCTKnPPa0oMG7JDYrhg==",
      "dev": true,
      "optional": true,
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/has-symbols": {
      "version": "1.0.3",
      "resolved": "https://registry.npmjs.org/has-symbols/-/has-symbols-1.0.3.tgz",
      "integrity": "sha512-l3LCuF6MgDNwTDKkdYGEihYjt5pRPbEg46rtlmnSPlUbgmB8LOIrKJbYYFBSbnPaJexMKtiPO8hmeRjRz2Td+A==",
      "dev": true,
      "optional": true,
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/http-cache-semantics": {
      "version": "4.1.1",
      "resolved": "https://registry.npmjs.org/http-cache-semantics/-/http-cache-semantics-4.1.1.tgz",
      "integrity": "sha512-er295DKPVsV82j5kw1Gjt+ADA/XYHsajl82cGNQG2eyoPkvgUhX+nDIyelzhIWbbsXP39EHcI6l5tYs2FYqYXQ==",
      "dev": true
    },
    "node_modules/http2-wrapper": {
      "version": "1.0.3",
      "resolved": "https://registry.npmjs.org/http2-wrapper/-/http2-wrapper-1.0.3.tgz",
      "integrity": "sha512-V+23sDMr12Wnz7iTcDeJr3O6AIxlnvT/bmaAAAP/Xda35C90p9599p0F1eHR/N1KILWSoWVAiOMFjBBXaXSMxg==",
      "dev": true,
      "dependencies": {
        "quick-lru": "^5.1.1",
        "resolve-alpn": "^1.0.0"
      },
      "engines": {
        "node": ">=10.19.0"
      }
    },
    "node_modules/is-fullwidth-code-point": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/is-fullwidth-code-point/-/is-fullwidth-code-point-3.0.0.tgz",
      "integrity": "sha512-zymm5+u+sCsSWyD9qNaejV3DFvhCKclKdizYaJUuHA83RLjb7nSuGnddCHGv0hk+KY7BMAlsWeK4Ueg6EV6XQg==",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/is-plain-obj": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/is-plain-obj/-/is-plain-obj-1.1.0.tgz",
      "integrity": "sha512-yvkRyxmFKEOQ4pNXCmJG5AEQNlXJS5LaONXo5/cLdTZdWvsZ1ioJEonLGAosKlMWE8lwUy/bJzMjcw8az73+Fg==",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/json-buffer": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/json-buffer/-/json-buffer-3.0.1.tgz",
      "integrity": "sha512-4bV5BfR2mqfQTJm+V5tPPdf+ZpuhiIvTuAB5g8kcrXOZpTT/QwwVRWBywX1ozr6lEuPdbHxwaJlm9G6mI2sfSQ==",
      "dev": true
    },
    "node_modules/json-stringify-safe": {
      "version": "5.0.1",
      "resolved": "https://registry.npmjs.org/json-stringify-safe/-/json-stringify-safe-5.0.1.tgz",
      "integrity": "sha512-ZClg6AaYvamvYEE82d3Iyd3vSSIjQ+odgjaTzRuO3s7toCdFKczob2i0zCh7JE8kWn17yvAWhUVxvqGwUalsRA==",
      "dev": true,
      "optional": true
    },
    "node_modules/jsonfile": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/jsonfile/-/jsonfile-4.0.0.tgz",
      "integrity": "sha512-m6F1R3z8jjlf2imQHS2Qez5sjKWQzbuuhuJ/FKYFRZvPE3PuHcSMVZzfsLhGVOkfd20obL5SWEBew5ShlquNxg==",
      "optionalDependencies": {
        "graceful-fs": "^4.1.6"
      }
    },
    "node_modules/keyv": {
      "version": "4.5.3",
      "resolved": "https://registry.npmjs.org/keyv/-/keyv-4.5.3.tgz",
      "integrity": "sha512-QCiSav9WaX1PgETJ+SpNnx2PRRapJ/oRSXM4VO5OGYGSjrxbKPVFVhB3l2OCbLCk329N8qyAtsJjSjvVBWzEug==",
      "dev": true,
      "dependencies": {
        "json-buffer": "3.0.1"
      }
    },
    "node_modules/loglevel": {
      "version": "1.8.1",
      "resolved": "https://registry.npmjs.org/loglevel/-/loglevel-1.8.1.tgz",
      "integrity": "sha512-tCRIJM51SHjAayKwC+QAg8hT8vg6z7GSgLJKGvzuPb1Wc+hLzqtuVLxp6/HzSPOozuK+8ErAhy7U/sVzw8Dgfg==",
      "engines": {
        "node": ">= 0.6.0"
      },
      "funding": {
        "type": "tidelift",
        "url": "https://tidelift.com/funding/github/npm/loglevel"
      }
    },
    "node_modules/lowercase-keys": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/lowercase-keys/-/lowercase-keys-2.0.0.tgz",
      "integrity": "sha512-tqNXrS78oMOE73NMxK4EMLQsQowWf8jKooH9g7xPavRT706R6bkQJ6DY2Te7QukaZsulxa30wQ7bk0pm4XiHmA==",
      "dev": true,
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/lru-cache": {
      "version": "6.0.0",
      "resolved": "https://registry.npmjs.org/lru-cache/-/lru-cache-6.0.0.tgz",
      "integrity": "sha512-Jo6dJ04CmSjuznwJSS3pUeWmd/H0ffTlkXXgwZi+eq1UCmqQwCh+eLsYOYCwY991i2Fah4h1BEMCx4qThGbsiA==",
      "dev": true,
      "optional": true,
      "dependencies": {
        "yallist": "^4.0.0"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/matcher": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/matcher/-/matcher-3.0.0.tgz",
      "integrity": "sha512-OkeDaAZ/bQCxeFAozM55PKcKU0yJMPGifLwV4Qgjitu+5MoAfSQN4lsLJeXZ1b8w0x+/Emda6MZgXS1jvsapng==",
      "dev": true,
      "optional": true,
      "dependencies": {
        "escape-string-regexp": "^4.0.0"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/mime-db": {
      "version": "1.52.0",
      "resolved": "https://registry.npmjs.org/mime-db/-/mime-db-1.52.0.tgz",
      "integrity": "sha512-sPU4uV7dYlvtWJxwwxHD0PuihVNiE7TyAbQ5SWxDCB9mUYvOgroQOwYQQOKPJ8CIbE+1ETVlOoK1UC2nU3gYvg==",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/mimic-response": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/mimic-response/-/mimic-response-1.0.1.tgz",
      "integrity": "sha512-j5EctnkH7amfV/q5Hgmoal1g2QHFJRraOtmx0JpIqkxhBhI/lJSl1nMpQ45hVarwNETOoWEimndZ4QK0RHxuxQ==",
      "dev": true,
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/minimist": {
      "version": "1.2.8",
      "resolved": "https://registry.npmjs.org/minimist/-/minimist-1.2.8.tgz",
      "integrity": "sha512-2yyAR8qBkN3YuheJanUpWC5U3bb5osDywNB8RzDVlDwDHbocAJveqqj1u8+SVD7jkWT4yvsHCpWqqWqAxb0zCA==",
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/mkdirp": {
      "version": "0.5.6",
      "resolved": "https://registry.npmjs.org/mkdirp/-/mkdirp-0.5.6.tgz",
      "integrity": "sha512-FP+p8RB8OWpF3YZBCrP5gtADmtXApB5AMLn+vdyA+PyxCjrCs00mjyUozssO33cwDeT3wNGdLxJ5M//YqtHAJw==",
      "dependencies": {
        "minimist": "^1.2.6"
      },
      "bin": {
        "mkdirp": "bin/cmd.js"
      }
    },
    "node_modules/modify-filename": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/modify-filename/-/modify-filename-1.1.0.tgz",
      "integrity": "sha512-EickqnKq3kVVaZisYuCxhtKbZjInCuwgwZWyAmRIp1NTMhri7r3380/uqwrUHfaDiPzLVTuoNy4whX66bxPVog==",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/ms": {
      "version": "2.1.2",
      "resolved": "https://registry.npmjs.org/ms/-/ms-2.1.2.tgz",
      "integrity": "sha512-sGkPx+VjMtmA6MX27oA4FBFELFCZZ4S4XqeGOXCv68tT+jb3vk/RyaKWP0PTKyWtmLSM0b+adUTEvbs1PEaH2w==",
      "dev": true
    },
    "node_modules/normalize-url": {
      "version": "6.1.0",
      "resolved": "https://registry.npmjs.org/normalize-url/-/normalize-url-6.1.0.tgz",
      "integrity": "sha512-DlL+XwOy3NxAQ8xuC0okPgK46iuVNAK01YN7RueYBqqFeGsBjV9XmCAzAdgt+667bCl5kPh9EqKKDwnaPG1I7A==",
      "dev": true,
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/object-keys": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/object-keys/-/object-keys-1.1.1.tgz",
      "integrity": "sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA==",
      "dev": true,
      "optional": true,
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/once": {
      "version": "1.4.0",
      "resolved": "https://registry.npmjs.org/once/-/once-1.4.0.tgz",
      "integrity": "sha512-lNaJgI+2Q5URQBkccEKHTQOPaXdUxnZZElQTZY0MFUAuaEqe1E+Nyvgdz/aIyNi6Z9MzO5dv1H8n58/GELp3+w==",
      "dev": true,
      "dependencies": {
        "wrappy": "1"
      }
    },
    "node_modules/p-cancelable": {
      "version": "2.1.1",
      "resolved": "https://registry.npmjs.org/p-cancelable/-/p-cancelable-2.1.1.tgz",
      "integrity": "sha512-BZOr3nRQHOntUjTrH8+Lh54smKHoHyur8We1V8DSMVrl5A2malOOwuJRnKRDjSnkoeBh4at6BwEnb5I7Jl31wg==",
      "dev": true,
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/path-exists": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/path-exists/-/path-exists-4.0.0.tgz",
      "integrity": "sha512-ak9Qy5Q7jYb2Wwcey5Fpvg2KoAc/ZIhLSLOSBmRmygPsGwkVVt0fZa0qrtMz+m6tJTAHfZQ8FnmB4MG4LWy7/w==",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/pend": {
      "version": "1.2.0",
      "resolved": "https://registry.npmjs.org/pend/-/pend-1.2.0.tgz",
      "integrity": "sha512-F3asv42UuXchdzt+xXqfW1OGlVBe+mxa2mqI0pg5yAHZPvFmY3Y6drSf/GQ1A86WgWEN9Kzh/WrgKa6iGcHXLg==",
      "dev": true
    },
    "node_modules/progress": {
      "version": "2.0.3",
      "resolved": "https://registry.npmjs.org/progress/-/progress-2.0.3.tgz",
      "integrity": "sha512-7PiHtLll5LdnKIMw100I+8xJXR5gW2QwWYkT6iJva0bXitZKa/XMrSbdmg3r2Xnaidz9Qumd0VPaMrZlF9V9sA==",
      "dev": true,
      "engines": {
        "node": ">=0.4.0"
      }
    },
    "node_modules/pump": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/pump/-/pump-3.0.0.tgz",
      "integrity": "sha512-LwZy+p3SFs1Pytd/jYct4wpv49HiYCqd9Rlc5ZVdk0V+8Yzv6jR5Blk3TRmPL1ft69TxP0IMZGJ+WPFU2BFhww==",
      "dev": true,
      "dependencies": {
        "end-of-stream": "^1.1.0",
        "once": "^1.3.1"
      }
    },
    "node_modules/pupa": {
      "version": "2.1.1",
      "resolved": "https://registry.npmjs.org/pupa/-/pupa-2.1.1.tgz",
      "integrity": "sha512-l1jNAspIBSFqbT+y+5FosojNpVpF94nlI+wDUpqP9enwOTfHx9f0gh5nB96vl+6yTpsJsypeNrwfzPrKuHB41A==",
      "dependencies": {
        "escape-goat": "^2.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/quick-lru": {
      "version": "5.1.1",
      "resolved": "https://registry.npmjs.org/quick-lru/-/quick-lru-5.1.1.tgz",
      "integrity": "sha512-WuyALRjWPDGtt/wzJiadO5AXY+8hZ80hVpe6MyivgraREW751X3SbhRvG3eLKOYN+8VEvqLcf3wdnt44Z4S4SA==",
      "dev": true,
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/resolve-alpn": {
      "version": "1.2.1",
      "resolved": "https://registry.npmjs.org/resolve-alpn/-/resolve-alpn-1.2.1.tgz",
      "integrity": "sha512-0a1F4l73/ZFZOakJnQ3FvkJ2+gSTQWz/r2KE5OdDY0TxPm5h4GkqkWWfM47T7HsbnOtcJVEF4epCVy6u7Q3K+g==",
      "dev": true
    },
    "node_modules/responselike": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/responselike/-/responselike-2.0.1.tgz",
      "integrity": "sha512-4gl03wn3hj1HP3yzgdI7d3lCkF95F21Pz4BPGvKHinyQzALR5CapwC8yIi0Rh58DEMQ/SguC03wFj2k0M/mHhw==",
      "dev": true,
      "dependencies": {
        "lowercase-keys": "^2.0.0"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/roarr": {
      "version": "2.15.4",
      "resolved": "https://registry.npmjs.org/roarr/-/roarr-2.15.4.tgz",
      "integrity": "sha512-CHhPh+UNHD2GTXNYhPWLnU8ONHdI+5DI+4EYIAOaiD63rHeYlZvyh8P+in5999TTSFgUYuKUAjzRI4mdh/p+2A==",
      "dev": true,
      "optional": true,
      "dependencies": {
        "boolean": "^3.0.1",
        "detect-node": "^2.0.4",
        "globalthis": "^1.0.1",
        "json-stringify-safe": "^5.0.1",
        "semver-compare": "^1.0.0",
        "sprintf-js": "^1.1.2"
      },
      "engines": {
        "node": ">=8.0"
      }
    },
    "node_modules/semver": {
      "version": "6.3.1",
      "resolved": "https://registry.npmjs.org/semver/-/semver-6.3.1.tgz",
      "integrity": "sha512-BR7VvDCVHO+q2xBEWskxS6DJE1qRnb7DxzUrogb71CWoSficBxYsiAGd+Kl0mmq/MprG9yArRkyrQxTO6XjMzA==",
      "dev": true,
      "bin": {
        "semver": "bin/semver.js"
      }
    },
    "node_modules/semver-compare": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/semver-compare/-/semver-compare-1.0.0.tgz",
      "integrity": "sha512-YM3/ITh2MJ5MtzaM429anh+x2jiLVjqILF4m4oyQB18W7Ggea7BfqdH/wGMK7dDiMghv/6WG7znWMwUDzJiXow==",
      "dev": true,
      "optional": true
    },
    "node_modules/serialize-error": {
      "version": "7.0.1",
      "resolved": "https://registry.npmjs.org/serialize-error/-/serialize-error-7.0.1.tgz",
      "integrity": "sha512-8I8TjW5KMOKsZQTvoxjuSIa7foAwPWGOts+6o7sgjz41/qMD9VQHEDxi6PBvK2l0MXUmqZyNpUK+T2tQaaElvw==",
      "dev": true,
      "optional": true,
      "dependencies": {
        "type-fest": "^0.13.1"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/slice-ansi": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/slice-ansi/-/slice-ansi-3.0.0.tgz",
      "integrity": "sha512-pSyv7bSTC7ig9Dcgbw9AuRNUb5k5V6oDudjZoMBSr13qpLBG7tB+zgCkARjq7xIUgdz5P1Qe8u+rSGdouOOIyQ==",
      "dependencies": {
        "ansi-styles": "^4.0.0",
        "astral-regex": "^2.0.0",
        "is-fullwidth-code-point": "^3.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/sort-keys": {
      "version": "1.1.2",
      "resolved": "https://registry.npmjs.org/sort-keys/-/sort-keys-1.1.2.tgz",
      "integrity": "sha512-vzn8aSqKgytVik0iwdBEi+zevbTYZogewTUM6dtpmGwEcdzbub/TX4bCzRhebDCRC3QzXgJsLRKB2V/Oof7HXg==",
      "dependencies": {
        "is-plain-obj": "^1.0.0"
      },
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/sort-keys-length": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/sort-keys-length/-/sort-keys-length-1.0.1.tgz",
      "integrity": "sha512-GRbEOUqCxemTAk/b32F2xa8wDTs+Z1QHOkbhJDQTvv/6G3ZkbJ+frYWsTcc7cBB3Fu4wy4XlLCuNtJuMn7Gsvw==",
      "dependencies": {
        "sort-keys": "^1.0.0"
      },
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/source-map": {
      "version": "0.6.1",
      "resolved": "https://registry.npmjs.org/source-map/-/source-map-0.6.1.tgz",
      "integrity": "sha512-UjgapumWlbMhkBgzT7Ykc5YXUT46F0iKu8SGXq0bcwP5dz/h0Plj6enJqjz1Zbq2l5WaqYnrVbwWOWMyF3F47g==",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/source-map-support": {
      "version": "0.5.21",
      "resolved": "https://registry.npmjs.org/source-map-support/-/source-map-support-0.5.21.tgz",
      "integrity": "sha512-uBHU3L3czsIyYXKX88fdrGovxdSCoTGDRZ6SYXtSRxLZUzHg5P/66Ht6uoUlHu9EZod+inXhKo3qQgwXUT/y1w==",
      "dependencies": {
        "buffer-from": "^1.0.0",
        "source-map": "^0.6.0"
      }
    },
    "node_modules/sprintf-js": {
      "version": "1.1.2",
      "resolved": "https://registry.npmjs.org/sprintf-js/-/sprintf-js-1.1.2.tgz",
      "integrity": "sha512-VE0SOVEHCk7Qc8ulkWw3ntAzXuqf7S2lvwQaDLRnUeIEaKNQJzV6BwmLKhOqT61aGhfUMrXeaBk+oDGCzvhcug==",
      "dev": true,
      "optional": true
    },
    "node_modules/string-width": {
      "version": "4.2.3",
      "resolved": "https://registry.npmjs.org/string-width/-/string-width-4.2.3.tgz",
      "integrity": "sha512-wKyQRQpjJ0sIp62ErSZdGsjMJWsap5oRNihHhu6G7JVO/9jIB6UyevL+tXuOqrng8j/cxKTWyWUwvSTriiZz/g==",
      "dependencies": {
        "emoji-regex": "^8.0.0",
        "is-fullwidth-code-point": "^3.0.0",
        "strip-ansi": "^6.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/strip-ansi": {
      "version": "6.0.1",
      "resolved": "https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz",
      "integrity": "sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==",
      "dependencies": {
        "ansi-regex": "^5.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/sumchecker": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/sumchecker/-/sumchecker-3.0.1.tgz",
      "integrity": "sha512-MvjXzkz/BOfyVDkG0oFOtBxHX2u3gKbMHIF/dXblZsgD3BWOFLmHovIpZY7BykJdAjcqRCBi1WYBNdEC9yI7vg==",
      "dev": true,
      "dependencies": {
        "debug": "^4.1.0"
      },
      "engines": {
        "node": ">= 8.0"
      }
    },
    "node_modules/type-fest": {
      "version": "0.13.1",
      "resolved": "https://registry.npmjs.org/type-fest/-/type-fest-0.13.1.tgz",
      "integrity": "sha512-34R7HTnG0XIJcBSn5XhDd7nNFPRcXYRZrBB2O2jdKqYODldSzBAqzsWoZYYvduky73toYS/ESqxPvkDf/F0XMg==",
      "dev": true,
      "optional": true,
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/universalify": {
      "version": "0.1.2",
      "resolved": "https://registry.npmjs.org/universalify/-/universalify-0.1.2.tgz",
      "integrity": "sha512-rBJeI5CXAlmy1pV+617WB9J63U6XcazHHF2f2dbJix4XzpUF0RS3Zbj0FGIOCAva5P/d/GBOYaACQ1w+0azUkg==",
      "dev": true,
      "engines": {
        "node": ">= 4.0.0"
      }
    },
    "node_modules/unused-filename": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/unused-filename/-/unused-filename-2.1.0.tgz",
      "integrity": "sha512-BMiNwJbuWmqCpAM1FqxCTD7lXF97AvfQC8Kr/DIeA6VtvhJaMDupZ82+inbjl5yVP44PcxOuCSxye1QMS0wZyg==",
      "dependencies": {
        "modify-filename": "^1.1.0",
        "path-exists": "^4.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/wrappy": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/wrappy/-/wrappy-1.0.2.tgz",
      "integrity": "sha512-l4Sp/DRseor9wL6EvV2+TuQn63dMkPjZ/sp9XkghTEbV9KlPS1xUsZ3u7/IQO4wxtcFB4bgpQPRcR3QCvezPcQ==",
      "dev": true
    },
    "node_modules/yallist": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/yallist/-/yallist-4.0.0.tgz",
      "integrity": "sha512-3wdGidZyq5PB084XLES5TpOSRA3wjXAlIWMhum2kRcv/41Sn2emQ0dycQW4uZXLejwKvg6EsvbdlVL+FYEct7A==",
      "dev": true,
      "optional": true
    },
    "node_modules/yauzl": {
      "version": "2.10.0",
      "resolved": "https://registry.npmjs.org/yauzl/-/yauzl-2.10.0.tgz",
      "integrity": "sha512-p4a9I6X6nu6IhoGmBqAcbJy1mlC4j27vEPZX9F4L4/vZT3Lyq1VkFHw/V/PUcB9Buo+DG3iHkT0x3Qya58zc3g==",
      "dev": true,
      "dependencies": {
        "buffer-crc32": "~0.2.3",
        "fd-slicer": "~1.1.0"
      }
    }
  }
}


==================== END FILE: #/Eaglestar ERP-darwin-arm64/Eaglestar ERP.app/Contents/Resources/app/npm-shrinkwrap.json ====================


==================== START FILE: #/Eaglestar ERP-darwin-arm64/Eaglestar ERP.app/Contents/Resources/app/package.json ====================
{
  "name": "eaglestar-erp-nativefier-42cdec",
  "version": "1.0.0",
  "description": "Placeholder for the nativefier cli to override with a target url",
  "main": "lib/main.js",
  "author": "Jia Hao",
  "license": "MIT",
  "keywords": [
    "desktop",
    "electron",
    "placeholder"
  ],
  "scripts": {},
  "dependencies": {
    "electron-context-menu": "^3.6.1",
    "electron-dl": "^3.5.0",
    "electron-squirrel-startup": "^1.0.0",
    "electron-window-state": "^5.0.3",
    "loglevel": "^1.8.1",
    "source-map-support": "^0.5.21"
  },
  "devDependencies": {
    "electron": "^25.7.0"
  }
}

==================== END FILE: #/Eaglestar ERP-darwin-arm64/Eaglestar ERP.app/Contents/Resources/app/package.json ====================


==================== START FILE: #/Eaglestar ERP-darwin-arm64/Eaglestar ERP.app/Contents/Frameworks/Electron Framework.framework/Versions/A/Libraries/vk_swiftshader_icd.json ====================
{"file_format_version": "1.0.0", "ICD": {"library_path": "./libvk_swiftshader.dylib", "api_version": "1.0.5"}}

==================== END FILE: #/Eaglestar ERP-darwin-arm64/Eaglestar ERP.app/Contents/Frameworks/Electron Framework.framework/Versions/A/Libraries/vk_swiftshader_icd.json ====================


==================== START FILE: data/security_overrides.json ====================
{
  "btn_create_user": [
    "db"
  ],
  "btn_lock_user": [
    "db"
  ],
  "btn_update_perms": [
    "db"
  ],
  "btn_unlock_user": [
    "db"
  ],
  "btn_purge_business": [
    "modify",
    "db",
    "system"
  ],
  "btn_unlock_view": [
    "modify",
    "db",
    "system"
  ],
  "btn_purge_infra": [
    "modify",
    "db",
    "system"
  ],
  "btn_purge_system": [
    "modify",
    "db",
    "system"
  ],
  "btn_create_backup": [
    "modify"
  ],
  "btn_restore_db": [
    "db",
    "modify",
    "system"
  ],
  "btn_clean_data": [
    "modify",
    "db",
    "system"
  ],
  "btn_delete_backup": [
    "modify",
    "db",
    "system"
  ],
  "btn_reset_pwd": [
    "db"
  ],
  "btn_update_single_inv": [
    "modify"
  ],
  "btn_drop_inv_col": [
    "modify",
    "db",
    "system"
  ],
  "btn_drop_inv_col_recent": [
    "modify"
  ],
  "btn_batch_update_cogs": [
    "modify",
    "db"
  ],
  "btn_create_skus": [
    "modify",
    "db"
  ],
  "btn_generate_report": [],
  "btn_start_etl_pipeline": [
    "modify"
  ],
  "btn_commit_sku_fix": [],
  "btn_sync_inventory": [
    "modify"
  ]
}
==================== END FILE: data/security_overrides.json ====================

