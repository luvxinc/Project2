Project Root: /Users/aaron/Desktop/app/MGMT
Version: V1.5.2
Generated at: 2025-12-07 19:25:13
==================================================


==================== START SPECIAL FILE: assets/patch_notes.txt ====================
VERSION=V1.5.2

[V1.0.0] 2025-08-10
å¥ åŸºç‰ˆæœ¬å‘å¸ƒ
- æ ¸å¿ƒå·¥å…·é›†ï¼šå‘å¸ƒäº†åŸºç¡€ Python è„šæœ¬å·¥å…·åŒ…ã€‚
- åŸºç¡€åŠŸèƒ½ï¼šæ”¯æŒ SKU é”€é‡ç»Ÿè®¡ã€åº“å­˜ä¸Šä¼ ä»¥åŠåŸºç¡€äº¤æ˜“æ•°æ®çš„æ¸…æ´—ä¸å¤„ç†ã€‚

[V1.1.0] 2025-09-28
è´¢åŠ¡æ¨¡å—é›†æˆ
- å¤šæºæ•°æ®é›†æˆï¼šé›†æˆäº† Order Earning èµ„é‡‘æµæ•°æ®æºï¼Œå®ç°äº†ä¸šè´¢ä¸€ä½“åŒ–ã€‚
- é€»è¾‘ä¼˜åŒ–ï¼šåœ¨äº¤æ˜“å¤„ç†é€»è¾‘ä¸­å¼•å…¥äº†åŸºäºå“ˆå¸Œ (Hash) çš„å»é‡ç®—æ³•ï¼Œæå‡äº†æ•°æ®å¤„ç†çš„å‡†ç¡®æ€§ã€‚
- æŠ¥è¡¨ç³»ç»Ÿï¼šå‘å¸ƒäº†å¤šç»´åº¦çš„ç›ˆäºåˆ†ææŠ¥è¡¨ä½“ç³»ã€‚

[V1.2.0] 2025-11-29
æ™ºèƒ½åŒ–è¿ç»´æ¨¡å—
- æœºå™¨å­¦ä¹ é›†æˆï¼šé›†æˆäº† K-Means èšç±»å’Œ PCA é™ç»´ç®—æ³•ï¼Œå®ç°äº† SKU çš„è‡ªåŠ¨åŒ–æ‰“æ ‡ä¸åˆ†å±‚ã€‚
- é«˜çº§åˆ†æå·¥å…·ï¼šæ–°å¢äº† Listing æµé‡è¯Šæ–­å·¥å…·ä¸ Combo ç»„åˆç­–ç•¥è¯„ä¼°æ¨¡å‹ã€‚

[V1.3.0] 2025-12-02
Web åº”ç”¨åŒ–è½¬å‹
- UI/UX å…¨é¢å‡çº§ï¼šä»å‘½ä»¤è¡Œè„šæœ¬è¿ç§»è‡³ Streamlit Web æ¡†æ¶ï¼Œå®ç°äº†äº¤äº’å¼å¯è§†åŒ–æ“ä½œç•Œé¢ã€‚
- CRM å®¢æˆ·ç®¡ç†æ¨¡å—ï¼šå¼•å…¥äº†åŸºäº RFM (æœ€è¿‘ä¸€æ¬¡æ¶ˆè´¹ã€æ¶ˆè´¹é¢‘ç‡ã€æ¶ˆè´¹é‡‘é¢) æ¨¡å‹çš„å®¢æˆ·ä»·å€¼åˆ†æåŠŸèƒ½ï¼Œæ”¯æŒç²¾å‡†ç”¨æˆ·ç”»åƒã€‚

[V1.4.0] 2025-12-04
ä¼ä¸šçº§æ¶æ„é‡æ„
- è‡ªåŠ¨åŒ– ETL æµæ°´çº¿ï¼šå‘å¸ƒäº†è¦†ç›–æ•°æ®åŠ è½½ (Ingest)ã€è§£æ (Parsing)ã€è½¬æ¢ (Transformation) å’Œå½’æ¡£ (Archiving) çš„å…¨ç”Ÿå‘½å‘¨æœŸè‡ªåŠ¨åŒ–å¤„ç†æµç¨‹ã€‚
- ç¾éš¾æ¢å¤ç³»ç»Ÿï¼šå¼•å…¥äº†å…·å¤‡æ—¶é—´ç‚¹æ¢å¤èƒ½åŠ›çš„æ•°æ®åº“å¤‡ä»½ä¸è¿˜åŸä¸­å¿ƒ (Backup & Restore Center)ã€‚
- åŸºç¡€è®¾æ–½å‡çº§ï¼šå®ç°äº†é…ç½®ä¸ä»£ç çš„å®Œå…¨åˆ†ç¦» (Configuration Decoupling)ï¼›å‰ç«¯æ¶æ„å…¨é¢è¿ç§»è‡³ç»„ä»¶åŒ– UI æ¨¡å¼ (Component-Based UI)ã€‚

[V1.4.1] 2025-12-05
æ ¸å¿ƒæ•°æ®å®Œæ•´æ€§ä¸ç®—æ³•çƒ­ä¿®å¤
- ETL æ™ºèƒ½å»é‡ (Smart Upsert)ï¼šå®æ–½äº†åŸºäºæ—¶é—´çª—å£å‰ªæå’Œå››ç»´å¤åˆä¸»é”® (Seller + Order + Item + Action) åŒ¹é…çš„åˆ†åŒºå»é‡ç­–ç•¥ï¼Œå®ç°äº†æ¯«ç§’çº§ç²¾å‡†è¦†ç›–ï¼Œå½»åº•æ¶ˆé™¤äº†è¯¯åˆ å†å²æ•°æ®çš„é£é™©ã€‚
- å…¨é“¾è·¯å½’ä¸€åŒ– (Global Normalization)ï¼šåœ¨æ‰€æœ‰åˆ†æå™¨ (Sales, Profit, Ordering, Prediction) ä¸­å¼ºåˆ¶å®æ–½ä¸¥æ ¼çš„ SKU å½’ä¸€åŒ–æ ‡å‡†ï¼ˆå¤§å†™/å»ç©ºæ ¼ï¼‰ï¼Œè§£å†³äº†æŠ¥è¡¨é¦–åˆ—é‡å¤å’Œç»Ÿè®¡åå·®é—®é¢˜ã€‚
- æ•°æ®æµåŠ å›º (Data Pipeline Hardening)ï¼šä¿®å¤äº† Repository åˆ° Analyzer ä¼ è¾“è¿‡ç¨‹ä¸­çš„é™é»˜æ•°æ®ä¸¢å¤±é—®é¢˜ï¼›å¢å¼ºäº†æ•°æ®ä»“åº“ä¸­æ—¥æœŸæ ¼å¼è§£æçš„å®¹é”™èƒ½åŠ›ã€‚
- Bug ä¿®å¤ï¼šä¿®æ­£äº†é”€é‡ç»Ÿè®¡åˆ†æä¸­åº—é“ºåˆ†æ¡¶ç»Ÿè®¡ä¸¢å¤±çš„é—®é¢˜ï¼Œå¹¶è¡¥å…¨äº†æ™ºèƒ½è¡¥è´§è®¡åˆ’ä¸­çš„çŠ¶æ€å¤‡æ³¨å­—æ®µã€‚

[V1.5.0] 2025-12-05
ä¼ä¸šçº§å®‰å…¨æ¶æ„ä¸æ€§èƒ½é©å‘½
- ä¼ä¸šçº§å®‰å…¨æ¶æ„ (Enterprise Security Architecture)ï¼šå®æ–½äº† PBKDF2-SHA256 å¯†ç åŠ å¯†å­˜å‚¨ä¸é˜²æš´åŠ›ç ´è§£ç­–ç•¥ï¼›æ•æ„Ÿæ“ä½œå¼•å…¥â€œæ“ä½œå¯†ç +ç¡®è®¤çŸ­è¯­â€åŒé‡æ ¡éªŒæœºåˆ¶ï¼Œå…¨é¢æå‡ç³»ç»Ÿå®‰å…¨æ€§ã€‚
- é«˜æ€§èƒ½ ETL å¼•æ“ (High-Performance ETL Engine)ï¼šæ ¸å¿ƒè½¬æ¢é€»è¾‘å‡çº§ä¸º Pandas å‘é‡åŒ–çŸ©é˜µè¿ç®—ï¼Œæ‘’å¼ƒä½æ•ˆå¾ªç¯ï¼Œå¤„ç†é€Ÿåº¦æå‡ 20-50 å€ï¼›å¼•å…¥ Staging Table åŸå­å†™å…¥ç­–ç•¥ï¼Œç¡®ä¿å¤šåº—é“ºæ•°æ®å¹¶å‘ä¸Šä¼ æ—¶çš„ç»å¯¹ä¸€è‡´æ€§ã€‚
- æ·±åº¦å®¡è®¡ç³»ç»Ÿ (Deep Audit System)ï¼šå‘å¸ƒäº†æ˜¾å¾®é•œçº§ä¸šåŠ¡å®¡è®¡æ—¥å¿—ï¼Œæ”¯æŒæ•°æ®å˜æ›´çš„â€œä¿®æ”¹å‰ vs ä¿®æ”¹åâ€é€å­—æ®µæ¯”å¯¹ï¼Œå®ç°å…¨é“¾è·¯æ“ä½œå¯è¿½æº¯ã€‚
- ç”¨æˆ·æƒé™ä¸è¿ç»´å¢å¼º (User Access & Ops Enhancement)ï¼šæ„å»ºäº†åŸºäº RBAC çš„ç»†ç²’åº¦æƒé™æ§åˆ¶ä½“ç³»ï¼Œæ”¯æŒé’ˆå¯¹ ETLã€æŠ¥è¡¨ç­‰æ¨¡å—çš„ç‹¬ç«‹é‰´æƒï¼›æ–°å¢æ•°æ®åº“è¿ç»´â€œå±é™©åŒºåŸŸâ€ï¼Œæ”¯æŒä¸€é”®é‡ç½®æµ‹è¯•ç¯å¢ƒï¼ˆä»…æ¸…ç©ºæµæ°´ï¼Œä¿ç•™ç”¨æˆ·ä¸åŸºç¡€æ¡£æ¡ˆï¼‰ã€‚

[V1.5.1] 2025-12-07
å…¨æ ˆæ¶æ„é‡æ„ä¸åœ¨çº¿æŠ¥è¡¨ä¸­å¿ƒ
- å…¨æ ˆæ¶æ„æ²»ç† (Full-Stack Re-architecture)ï¼šå®Œæˆäº†ä»è„šæœ¬åŒ–å‘å·¥ç¨‹åŒ–çš„æ·±åº¦è½¬å‹ã€‚ç¡®ç«‹äº† UI/Service/Repository ä¸‰å±‚æ¶æ„è¾¹ç•Œï¼Œå½»åº•è§£è€¦äº† UI å±‚å¯¹æ•°æ®åº“çš„ç›´æ¥ä¾èµ–ï¼›æ ‡å‡†åŒ–ç³»ç»Ÿå…¥å£ï¼Œå¹¶å®Œæˆäº†æ ¹ç›®å½•ä¸å·¥å…·è„šæœ¬çš„æ·±åº¦æ¸…æ´—ã€‚
- åœ¨çº¿æŠ¥è¡¨ä¸­å¿ƒ (Online Report Center)ï¼šæ–°å¢æŠ¥è¡¨åœ¨çº¿é¢„è§ˆä¸ç®¡ç†åŠŸèƒ½ã€‚ç”¨æˆ·ç°åœ¨å¯ä»¥é€šè¿‡ APP ç›´æ¥æŸ¥çœ‹å’Œç®¡ç†æ‰€æœ‰ç”Ÿæˆçš„ BI å•†ä¸šæ™ºèƒ½æŠ¥è¡¨ï¼Œå®ç°äº†æ•°æ®æ´å¯Ÿçš„å³æ—¶åœ¨çº¿è®¿é—®ï¼Œæ— éœ€ä¸‹è½½ ZIPã€‚
- ç®—æ³•ç²¾åº¦ä¿®æ­£ (Algorithm Correction)ï¼šä¿®å¤äº†æŠ¥è¡¨ç”Ÿæˆå¼•æ“ä¸­çš„ç»Ÿè®¡é€»è¾‘é”™è¯¯ï¼Œä¼˜åŒ–äº†åˆ©æ¶¦è®¡ç®—ä¸æˆæœ¬åˆ†æ‘Šçš„ç²¾åº¦ï¼Œå½»åº•è§£å†³äº†éƒ¨åˆ† SKU ç»Ÿè®¡åå·®é—®é¢˜ï¼Œç¡®ä¿äº†è´¢åŠ¡æ•°æ®çš„å‡†ç¡®æ€§ã€‚
- ä»£ç ä¸å®‰å…¨æ ‡å‡†åŒ– (Code & Security Standardization)ï¼šç»Ÿä¸€äº†å…¨ç«™çš„åŸºç¡€è®¾æ–½é€»è¾‘ï¼ˆå¦‚ IP è·å–ä¸æ—¥å¿—å¤„ç†ï¼‰ï¼Œæ¶ˆé™¤äº†é‡å¤ä»£ç  (DRY)ï¼›å°†â€œæ“ä½œå¯†ç +ç¡®è®¤çŸ­è¯­â€çš„åŒé‡å®‰å…¨æ ¡éªŒæœºåˆ¶æ¨å¹¿è‡³åº“å­˜ä¸èµ„æ–™ä¿®æ”¹æ¨¡å—ï¼Œå¡«è¡¥äº†å®‰å…¨çŸ­æ¿ã€‚
==================== END SPECIAL FILE: assets/patch_notes.txt ====================


==================== START FILE: stop_server.sh ====================
#!/bin/bash
# stop_server.sh
# Update: åŒ¹é… app.py

echo "ğŸ›‘ æ­£åœ¨åœæ­¢ ERP æœåŠ¡..."

# [Fix] grep "streamlit run app.py"
ERP_PID=$(ps aux | grep "streamlit run app.py" | grep -v grep | awk '{print $2}')

if [ -z "$ERP_PID" ]; then
    echo "âš ï¸  æœªæ‰¾åˆ°æ­£åœ¨è¿è¡Œçš„ Streamlit æœåŠ¡ (app.py)ã€‚"
else
    for pid in $ERP_PID; do
        kill -9 $pid
        echo " å·²ç»ˆæ­¢ Streamlit ä¸»è¿›ç¨‹ (PID: $pid)"
    done
fi

CAF_PID=$(ps aux | grep "caffeinate -s" | grep -v grep | awk '{print $2}')
if [ -z "$CAF_PID" ]; then
    echo "ï¸  æœªæ‰¾åˆ°ç›¸å…³çš„é˜²ä¼‘çœ è¿›ç¨‹ã€‚"
else
    for pid in $CAF_PID; do
        kill $pid
        echo " å·²ç»ˆæ­¢é˜²ä¼‘çœ è¿›ç¨‹ (PID: $pid)"
    done
fi

echo "================================================="
echo "ğŸ‰ æ‰€æœ‰æœåŠ¡å·²åœæ­¢ã€‚"
==================== END FILE: stop_server.sh ====================


==================== START FILE: config.py ====================
import os
import re
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()


class Config:
    # =========================================================================
    # 1. è·¯å¾„é…ç½®
    # =========================================================================
    BASE_DIR = Path(__file__).resolve().parent

    UPLOADER_DIR = BASE_DIR / "uploader"
    OUTPUT_DIR = BASE_DIR / "output"
    BACKUP_DIR = BASE_DIR / "backup"
    ASSETS_DIR = BASE_DIR / "assets"
    LOG_DIR = BASE_DIR / "logs"

    for _dir in [UPLOADER_DIR, OUTPUT_DIR, BACKUP_DIR, LOG_DIR]:
        _dir.mkdir(parents=True, exist_ok=True)

    # =========================================================================
    # 2. ç‰ˆæœ¬æ§åˆ¶ (Version Control)
    # =========================================================================
    PATCH_NOTES_FILE = ASSETS_DIR / "patch_notes.txt"

    # é»˜è®¤å€¼
    APP_VERSION = "V1.0.0"
    VERSION_DATE = "Unknown Date"  # [New] ç‰ˆæœ¬æ—¥æœŸ
    AUTHOR = "Aaron"  # [New] ä½œè€…ç½²å
    PATCH_NOTES_LIST = []

    @classmethod
    def load_version_info(cls):
        """ä» txt æ–‡ä»¶åŠ è½½ç‰ˆæœ¬å·ã€æ—¥æœŸå’Œæ›´æ–°æ—¥å¿—"""
        if not cls.PATCH_NOTES_FILE.exists():
            return

        try:
            with open(cls.PATCH_NOTES_FILE, "r", encoding="utf-8") as f:
                content = f.read().strip()

            # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²æ¯ä¸ªå—
            blocks = content.split('\n\n')
            cls.PATCH_NOTES_LIST = []

            for block in blocks:
                block = block.strip()
                if not block: continue

                # ç¬¬ä¸€è¡Œæå–ç‰ˆæœ¬å·
                if block.startswith("VERSION="):
                    cls.APP_VERSION = block.split("=")[1].strip()
                    continue

                # è§£ææ›´æ–°æ—¥å¿—å—
                # æ ¼å¼: [V1.5.0] 2025-12-05 \n æè¿°å†…å®¹
                lines = block.split('\n', 1)
                if len(lines) < 2: continue

                header = lines[0].strip()
                desc = lines[1].strip()

                # æ­£åˆ™æå–: [V1.5.0] 2025-12-05
                match = re.match(r"\[(.*?)\]\s+(.*)", header)
                if match:
                    ver, date_str = match.groups()

                    # [New] å¦‚æœæ˜¯å½“å‰ç‰ˆæœ¬ï¼Œæå–æ—¥æœŸ
                    if ver == cls.APP_VERSION:
                        cls.VERSION_DATE = date_str

                    cls.PATCH_NOTES_LIST.append({
                        "ver": ver,
                        "date": date_str,
                        "desc": desc
                    })
        except Exception as e:
            print(f"âš ï¸ Error loading patch notes: {e}")

    # =========================================================================
    # 3. æ•°æ®åº“é…ç½®
    # =========================================================================
    DB_HOST = os.getenv("DB_HOST", "localhost")
    DB_PORT = int(os.getenv("DB_PORT", 3306))
    DB_USER = os.getenv("DB_USER", "root")
    DB_PASS = os.getenv("DB_PASS", "")
    DB_NAME = os.getenv("DB_NAME", "MGMT")
    DB_CHARSET = os.getenv("DB_CHARSET", "utf8mb4")

    SQLALCHEMY_URL = (
        f"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}?charset={DB_CHARSET}"
    )

    # =========================================================================
    # 4. å®‰å…¨é…ç½®
    # =========================================================================
    DB_OPERATOR_PWD = os.getenv("DB_OPERATOR_PWD", "1522")

    # =========================================================================
    # 5. ä¸šåŠ¡å‚æ•°é…ç½®
    # =========================================================================
    LEAD_MONTH = 2.0
    MIN_SAFETY_MONTH = 1.0
    LOSS_RATES = {
        "CASE": 0.6, "REQUEST": 0.5, "RETURN": 0.3, "DISPUTE": 1.0
    }

    @classmethod
    def update_params(cls, lead_time=None, safety_month=None, loss_rates=None):
        if lead_time is not None: cls.LEAD_MONTH = lead_time
        if safety_month is not None: cls.MIN_SAFETY_MONTH = safety_month
        if loss_rates is not None: cls.LOSS_RATES.update(loss_rates)


# æ¨¡å—åŠ è½½æ—¶ç«‹å³è¯»å–ç‰ˆæœ¬ä¿¡æ¯
Config.load_version_info()
==================== END FILE: config.py ====================


==================== START FILE: run_server.sh ====================
#!/bin/bash
# run_server.sh
# Eaglestar ERP å¯åŠ¨è„šæœ¬
# Update: æŒ‡å‘ app.py

cd "$(dirname "$0")"
source .venv/bin/activate

nohup caffeinate -s > /dev/null 2>&1 &
CAFFEINATE_PID=$!

nohup streamlit run app.py > streamlit_output.log 2>&1 &
STREAMLIT_PID=$!

echo "================================================="
echo "ğŸš€ ERP å·²åœ¨åå°å¯åŠ¨ (PID: $STREAMLIT_PID)"
echo " é˜²ä¼‘çœ  PID: $CAFFEINATE_PID"
echo "Entry Point: app.py"
echo "================================================="
==================== END FILE: run_server.sh ====================


==================== START FILE: run_tests.sh ====================
#!/bin/bash
# run_tests.sh
# è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œè„šæœ¬

echo "ğŸ§ª æ­£åœ¨åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ..."
export PYTHONPATH=$PYTHONPATH:$(pwd)

# æ£€æŸ¥ pytest æ˜¯å¦å®‰è£…
if ! command -v pytest &> /dev/null; then
    echo " æœªæ‰¾åˆ° pytestã€‚æ­£åœ¨å°è¯•å®‰è£…..."
    pip install pytest
fi

echo "ğŸš€ å¼€å§‹è¿è¡Œå•å…ƒæµ‹è¯•..."
echo "---------------------------------------------------"

# è¿è¡Œ pytestï¼Œ-v æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
pytest tests/test_core_units.py -v

EXIT_CODE=$?

echo "---------------------------------------------------"
if [ $EXIT_CODE -eq 0 ]; then
    echo " æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿé€»è¾‘æ­£å¸¸ã€‚"
else
    echo " æµ‹è¯•å¤±è´¥ï¼è¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚"
fi

exit $EXIT_CODE
==================== END FILE: run_tests.sh ====================


==================== START FILE: .env ====================
# ======================================================
# 1. æ•°æ®åº“é…ç½® (Database Configuration)
# æ ¹æ®ä½ ä¹‹å‰çš„ä»£ç  (DBClient)ï¼Œä½ éœ€è¦è¿æ¥æœ¬åœ° MySQL
# ======================================================
DB_HOST=localhost
DB_PORT=3306
DB_USER=root
DB_PASS=***REDACTED_PASSWORD***
DB_NAME=MGMT
DB_CHARSET=utf8mb4
DB_OPERATOR_PWD=1522
# ======================================================
# 2. é»˜è®¤è´¦å·åˆå§‹åŒ– (Initial User Bootstrap)
# ç³»ç»Ÿé¦–æ¬¡å¯åŠ¨æ—¶ä¼šè‡ªåŠ¨åˆ›å»ºè¿™äº›è´¦å·
# ======================================================
DEFAULT_ADMIN_USERNAME=admin
DEFAULT_ADMIN_PASSWORD=1522P

DEFAULT_USER_USERNAME=user
DEFAULT_USER_PASSWORD=6M130

# ======================================================
# 3. AI æ™ºèƒ½åŠ©æ‰‹é…ç½® (AI Assistant)
# ======================================================
# [å¿…éœ€] Google Gemini (å…è´¹ç‰ˆ API)
# è¯·å» https://aistudio.google.com/ ç”³è¯· Key
GOOGLE_API_KEY=YOUR_GOOGLE_API_KEY_HERE


# ======================================================
# 4. ç³»ç»Ÿè®¾ç½® (System Settings)
# ======================================================
# è‡ªåŠ¨æ³¨é”€æ—¶é—´ (ç§’)ï¼Œé»˜è®¤ 600 (10åˆ†é’Ÿ)
AUTO_LOGOUT_SECONDS=600

# æ—¥å¿—çº§åˆ« (INFO / DEBUG)
LOG_LEVEL=INFO
==================== END FILE: .env ====================


==================== START FILE: app.py ====================
# app.py
"""
Eaglestar ERP - Main Controller (Entry Point) V4.1
Standardized Entry Point.
"""
import time
import streamlit as st
from config import Config
from core.auth_service import AuthService
from core.context import set_current_user
from ui.main_interface import MainInterface
from ui.styles import StyleManager

# ==============================================================================
# 1. å…¨å±€é…ç½® (å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤)
# ==============================================================================
st.set_page_config(
    page_title=f"Eaglestar ERP - {Config.APP_VERSION}",
    page_icon="ğŸ¦…",
    layout="wide",
    initial_sidebar_state="collapsed",
)

# ==============================================================================
# 2. ç³»ç»Ÿåˆå§‹åŒ–
# ==============================================================================
# æ³¨å…¥å…¨å±€ CSS
StyleManager.apply_global_styles()

# åˆå§‹åŒ–æ ¸å¿ƒæœåŠ¡
AuthService.initialize()

# ä¼šè¯å¸¸é‡å®šä¹‰
SESSION_USER = "auth_user"
SESSION_LAST_ACTIVE = "last_active"
SESSION_TOKEN = "token"
AUTO_LOGOUT_SEC = 600  # 10åˆ†é’Ÿ


def check_session_security() -> str:
    """
    [ä¸­é—´ä»¶] ä¼šè¯å®‰å…¨æ£€æŸ¥
    è¿”å›: è¸¢å‡ºåŸå›  (kickout_msg)ï¼Œå¦‚æœä¸º None åˆ™è¡¨ç¤ºä¼šè¯æ­£å¸¸
    """
    if SESSION_USER not in st.session_state:
        return None

    user_data = st.session_state[SESSION_USER]
    username = user_data.get("username")

    # 1. è®¾ç½®ä¸Šä¸‹æ–‡ (Context)
    set_current_user(username, ip=user_data.get("ip"))

    # 2. æ´»è·ƒåº¦ç»­æœŸ (Heartbeat)
    st.session_state[SESSION_LAST_ACTIVE] = time.time()

    # 3. å•ç‚¹ç™»å½•äº’æ–¥æ£€æŸ¥ (SSO Check)
    local_token = st.session_state.get(SESSION_TOKEN)
    if not AuthService.verify_session_token(username, local_token):
        return "âš ï¸ æ‚¨å·²åœ¨å…¶ä»–è®¾å¤‡ç™»å½•ï¼Œå½“å‰ä¼šè¯å·²å¤±æ•ˆ (è¢«è¿«ä¸‹çº¿)ã€‚"

    # 4. è¶…æ—¶è‡ªåŠ¨æ³¨é”€æ£€æŸ¥ (Timeout Check) - é€»è¾‘é¢„ç•™
    return None


def main():
    """ä¸»ç¨‹åºå…¥å£"""

    # --- Phase 1: å®‰å…¨æ£€æŸ¥ ---
    kickout_msg = check_session_security()

    if kickout_msg:
        st.session_state.clear()
        # å¼ºåˆ¶è·³è½¬å›ç™»å½•é¡µå¹¶æ˜¾ç¤ºé”™è¯¯
        MainInterface.render_login_page(SESSION_USER, SESSION_LAST_ACTIVE, SESSION_TOKEN, kickout_msg)
        return

    # --- Phase 2: è·¯ç”±åˆ†å‘ ---
    if SESSION_USER not in st.session_state:
        # æœªç™»å½• -> æ¸²æŸ“ç™»å½•é¡µ
        MainInterface.render_login_page(SESSION_USER, SESSION_LAST_ACTIVE, SESSION_TOKEN)
    else:
        # å·²ç™»å½• -> æ¸²æŸ“ä¸»åº”ç”¨
        try:
            user_data = st.session_state[SESSION_USER]
            MainInterface.render_main_app(user_data, SESSION_USER)
        except Exception as e:
            st.error(f"Critical UI Error: {e}")
            if st.button("Emergency Reset"):
                st.session_state.clear()
                st.rerun()


if __name__ == "__main__":
    main()
==================== END FILE: app.py ====================


==================== START FILE: ui/layout.py ====================
# ui/layout.py

import streamlit as st
import base64
import streamlit.components.v1 as components
from config import Config


# ==============================================================================
# èµ„æºåŠ è½½è¾…åŠ©å‡½æ•°
# ==============================================================================
def get_base64_encoded_file(file_path):
    """è¯»å–æ–‡ä»¶å¹¶è½¬æ¢ä¸º base64 ç¼–ç """
    try:
        with open(file_path, "rb") as f:
            return base64.b64encode(f.read()).decode("utf-8")
    except FileNotFoundError:
        return None


# ==============================================================================
# 1. èƒŒæ™¯åŠ¨ç”»æ¸²æŸ“ (æš´åŠ›ç©¿é€ç‰ˆ)
# ==============================================================================
def set_main_background_video():
    """
    æ¸²æŸ“å…¨å±èƒŒæ™¯è§†é¢‘ï¼Œå¼ºåˆ¶è¦†ç›– Streamlit é»˜è®¤èƒŒæ™¯
    """
    video_path = Config.ASSETS_DIR / "background.mp4"
    video_b64 = get_base64_encoded_file(video_path)

    if not video_b64:
        return

    # æ ¸å¿ƒ CSSï¼šæš´åŠ›å°†æ‰€æœ‰å¯èƒ½é®æŒ¡è§†é¢‘çš„å®¹å™¨è®¾ä¸ºé€æ˜
    css_code = f"""
    <style>
        /* 1. å…¨å±€å®¹å™¨é€æ˜åŒ– */
        .stApp {{
            background: transparent !important;
        }}
        /* 2. æ»šåŠ¨å®¹å™¨é€æ˜åŒ– */
        [data-testid="stAppViewContainer"] {{
            background: transparent !important;
        }}
        /* 3. ä¸»å†…å®¹åŒºé€æ˜åŒ– */
        [data-testid="stMainBlockContainer"] {{
            background: transparent !important;
        }}
        /* 4. é¡¶éƒ¨ Header é€æ˜åŒ– */
        [data-testid="stHeader"] {{
            background: transparent !important;
            visibility: hidden; /* å½»åº•éšè—é»˜è®¤èœå•æ  */
        }}

        /* 5. èƒŒæ™¯è§†é¢‘å®šä½ */
        #bg-video {{
            position: fixed;
            top: 0;
            left: 0;
            width: 100vw;
            height: 100vh;
            object-fit: cover;
            z-index: -100;
        }}

        /* 6. é»‘è‰²é®ç½© */
        #bg-overlay {{
            position: fixed;
            top: 0;
            left: 0;
            width: 100vw;
            height: 100vh;
            background: rgba(0, 0, 0, 0.70);
            z-index: -99;
            backdrop-filter: blur(3px);
        }}

        /* 7. ä¾§è¾¹æ æ ·å¼ */
        [data-testid="stSidebar"] {{
            background-color: rgba(20, 20, 20, 0.9) !important;
            border-right: 1px solid rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
        }}

        /* 8. å…¨å±€å­—ä½“å¢å¼º */
        h1, h2, h3, h4, h5, h6, p, span, div, label, li {{
            color: #E0E0E0 !important;
            text-shadow: 0px 1px 2px rgba(0,0,0,0.8);
        }}

        /* 9. è¾“å…¥æ¡†ç»„ä»¶ç¾åŒ– */
        .stTextInput input, .stNumberInput input, .stDateInput input, .stSelectbox div[data-baseweb="select"] {{
            background-color: rgba(40, 40, 40, 0.6) !important;
            color: white !important;
            border: 1px solid rgba(255, 255, 255, 0.2) !important;
        }}
    </style>
    """

    video_html = f"""
    {css_code}
    <video id="bg-video" autoplay loop muted playsinline>
        <source src="data:video/mp4;base64,{video_b64}" type="video/mp4">
    </video>
    <div id="bg-overlay"></div>
    """
    st.markdown(video_html, unsafe_allow_html=True)


# ==============================================================================
# 2. å¤´éƒ¨æ¸²æŸ“ (Logo + æ—¶é’Ÿ + åŠ¨æ€ç‰ˆæœ¬å· + ä½œè€…ä¿¡æ¯)
# ==============================================================================
def render_header():
    """
    æ¸²æŸ“é¡¶éƒ¨å¯¼èˆªæ 
    """
    logo_path = Config.ASSETS_DIR / "Logo.png"
    logo_b64 = get_base64_encoded_file(logo_path)

    logo_img = (
        f'<img src="data:image/png;base64,{logo_b64}" class="header-logo">'
        if logo_b64 else "ğŸ¦…"
    )

    # CSS
    st.markdown(f"""
        <style>
        .header-wrapper {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 10px 0px 20px 0px;
            border-bottom: 1px solid rgba(255,255,255,0.15);
            margin-bottom: 20px;
        }}
        .header-left {{ display: flex; align-items: center; gap: 15px; }}
        .header-logo {{ height: 50px; width: auto; }}
        .main-title {{ font-size: 24px; font-weight: 700; color: #fff; letter-spacing: 1px; }}
        .sub-title {{ font-size: 12px; color: rgba(255,255,255,0.7) !important; }}
        .meta-info {{ font-size: 10px; color: rgba(255,255,255,0.5) !important; margin-top: 2px; letter-spacing: 0.5px; }}

        .header-right {{ text-align: right; }}
        .company-badge {{
            font-size: 12px;
            font-weight: 600; color: #E74C3C !important;
            background: rgba(0,0,0,0.5); padding: 2px 8px; border-radius: 8px;
            border: 1px solid rgba(231, 76, 60, 0.4);
        }}
        #clock-display {{
            font-family: 'Monaco', monospace;
            font-size: 15px;
            color: #4EC9B0 !important; 
            margin-top: 5px;
            font-weight: bold;
        }}
        </style>

        <div class="header-wrapper">
            <div class="header-left">
                {logo_img}
                <div>
                    <div class="main-title">Eaglestar ERP</div>
                    <div class="sub-title">Enterprise Intelligence Platform {Config.APP_VERSION}</div>
                    <div class="meta-info">ğŸ“… Current Version Released: {Config.VERSION_DATE} &nbsp;|&nbsp; By: {Config.AUTHOR}</div>
                </div>
            </div>
            <div class="header-right">
                <span class="company-badge">ğŸ¦… EAGLESTAR INC.</span>
                <div id="clock-display">Initializing...</div>
            </div>
        </div>
    """, unsafe_allow_html=True)

    # JS æ—¶é’Ÿ
    js_code = """
    <script>
        function updateClock() {
            const now = new Date();
            const dateStr = now.getFullYear() + "-" + 
                            String(now.getMonth()+1).padStart(2, '0') + "-" + 
                            String(now.getDate()).padStart(2, '0');
            const timeStr = String(now.getHours()).padStart(2, '0') + ":" + 
                            String(now.getMinutes()).padStart(2, '0') + ":" + 
                            String(now.getSeconds()).padStart(2, '0');
            try {
                const clock = window.parent.document.getElementById('clock-display');
                if (clock) {
                    clock.innerText = "ğŸ•’ " + dateStr + " " + timeStr;
                }
            } catch(e) {
                console.log("Clock element not found");
            }
        }
        setInterval(updateClock, 1000);
        updateClock();
    </script>
    """
    components.html(js_code, height=0)
==================== END FILE: ui/layout.py ====================


==================== START FILE: ui/main_interface.py ====================
# ui/main_interface.py

import streamlit as st
import time
import requests
import datetime
from config import Config
from core.auth_service import AuthService
from core.context import set_current_user, clear_context
from ui.layout import set_main_background_video, render_header, get_base64_encoded_file

# å¼•å…¥æ‰€æœ‰é¡µé¢ (æ–°å¢ data_visualization)
from ui.pages import (
    home, etl_ingest, reports, db_data_change, db_admin, user_admin, audit_logs, ai_gemini,
    data_visualization  # [New]
)


class MainInterface:
    """
    è´Ÿè´£å¤„ç†ç™»å½•é¡µå’Œä¸»ç•Œé¢çš„æ¸²æŸ“é€»è¾‘
    V3.4 Upgrade: æ–°å¢æ•°æ®å¯è§†åŒ–æ¨¡å—å…¥å£
    """

    # =========================================================================
    # 1. æ™ºèƒ½æ„ŸçŸ¥æ¨¡å—
    # =========================================================================

    @staticmethod
    def get_client_ip() -> str:
        """è·å–å®¢æˆ·ç«¯çœŸå® IP (å®˜æ–¹æ ‡å‡†ç‰ˆ)"""
        try:
            # [Fix] ä½¿ç”¨å®˜æ–¹ st.context.headers æ›¿ä»£æ—§ Hack
            headers = st.context.headers
            if headers:
                # Cloudflare ä¼ é€’çš„çœŸå® IP
                if "Cf-Connecting-Ip" in headers:
                    return headers["Cf-Connecting-Ip"]
                # æ ‡å‡†ä»£ç†å¤´
                if "X-Forwarded-For" in headers:
                    return headers["X-Forwarded-For"].split(",")[0].strip()

            # æœ¬åœ°å…œåº•
            session_info = st.runtime.get_instance().get_client_session_info()
            if hasattr(session_info, 'ip'): return session_info.ip
        except:
            pass
        return "UNKNOWN"

    @staticmethod
    @st.cache_data(ttl=3600)
    def get_smart_info(ip: str):
        info = {
            "city": "Unknown City", "weather_icon": "ğŸŒ¤ï¸", "weather_desc": "Weather n/a",
            "temp": "--", "greeting": "Hello", "tips": "Welcome back!"
        }

        if ip in ["UNKNOWN", "127.0.0.1", "localhost", "::1"]:
            info["city"] = "Localhost"
            return info

        try:
            loc_url = f"http://ip-api.com/json/{ip}"
            loc_res = requests.get(loc_url, timeout=3).json()

            if loc_res.get("status") == "success":
                lat, lon = loc_res["lat"], loc_res["lon"]
                info["city"] = loc_res.get("city", "Unknown")
                timezone = loc_res.get("timezone", "UTC")

                weather_url = f"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true&timezone={timezone}"
                w_res = requests.get(weather_url, timeout=3).json()

                if "current_weather" in w_res:
                    curr = w_res["current_weather"]
                    temp = curr["temperature"]
                    code = curr["weathercode"]
                    is_day = curr["is_day"]

                    info["temp"] = f"{temp}Â°C"

                    if code == 0:
                        info["weather_desc"] = "æ™´æœ—";
                        info["weather_icon"] = "â˜€ï¸" if is_day else "ğŸŒ™"
                    elif code in [1, 2, 3]:
                        info["weather_desc"] = "å¤šäº‘";
                        info["weather_icon"] = "â›…"
                    elif code in [45, 48]:
                        info["weather_desc"] = "æœ‰é›¾";
                        info["weather_icon"] = "ğŸŒ«ï¸"
                    elif 51 <= code <= 67:
                        info["weather_desc"] = "æœ‰é›¨";
                        info["weather_icon"] = "ğŸŒ§ï¸"
                    elif 71 <= code <= 77:
                        info["weather_desc"] = "ä¸‹é›ª";
                        info["weather_icon"] = "â„ï¸"
                    elif code >= 95:
                        info["weather_desc"] = "é›·æš´";
                        info["weather_icon"] = "â›ˆï¸"

                    try:
                        import pytz
                        local_tz = pytz.timezone(timezone)
                        local_hour = datetime.datetime.now(local_tz).hour
                    except:
                        local_hour = datetime.datetime.now().hour

                    if 5 <= local_hour < 12:
                        info["greeting"] = "æ—©ä¸Šå¥½"
                    elif 12 <= local_hour < 14:
                        info["greeting"] = "ä¸­åˆå¥½"
                    elif 14 <= local_hour < 19:
                        info["greeting"] = "ä¸‹åˆå¥½"
                    elif 19 <= local_hour < 23:
                        info["greeting"] = "æ™šä¸Šå¥½"
                    else:
                        info["greeting"] = "å¤œæ·±äº†"

                    tips = []
                    if local_hour >= 23 or local_hour < 5:
                        tips.append("å·¥ä½œè™½é‡è¦ï¼Œä½†ä¹Ÿè¯·æ³¨æ„ä¼‘æ¯å“¦ ğŸŒ™")
                    elif 12 <= local_hour < 14:
                        tips.append("è®°å¾—æŒ‰æ—¶åƒé¥­ï¼Œå°æ†©ä¸€ä¸‹ç²¾åŠ›æ›´å……æ²› â˜•")
                    if "é›¨" in info["weather_desc"] or "é›·" in info["weather_desc"]:
                        tips.append("å¤–é¢ä¸‹é›¨äº†ï¼Œå‡ºè¡Œè®°å¾—å¸¦ä¼ â˜‚ï¸")
                    elif temp < 10:
                        tips.append("æ°”æ¸©è¾ƒä½ï¼Œæ³¨æ„ä¿æš– ğŸ§£")
                    elif temp > 30:
                        tips.append("å¤©æ°”ç‚çƒ­ï¼Œæ³¨æ„é˜²æš‘é™æ¸© ğŸ¥¤")
                    if not tips: tips.append("æ„¿ä½ ä»Šå¤©æ‹¥æœ‰å¥½å¿ƒæƒ… ")
                    info["tips"] = tips[0]
        except Exception:
            pass
        return info

    # =========================================================================
    # 2. ç™»å½•é¡µæ¸²æŸ“
    # =========================================================================

    @staticmethod
    def render_login_page(session_key, last_active_key, token_key, msg_override=None):
        set_main_background_video()
        clear_context()
        _, col_center, _ = st.columns([1, 1.5, 1])

        with col_center:
            logo_path = Config.ASSETS_DIR / "Logo.png"
            logo_b64 = get_base64_encoded_file(logo_path)
            if logo_b64:
                logo_html = f'<img src="data:image/png;base64,{logo_b64}" style="width: 700px;max-width: 100%; height: auto; margin-bottom: 30px;">'
            else:
                logo_html = '<div style="font-size: 320px;line-height: 1.1; margin-bottom: 20px;">ğŸ¦…</div>'

            st.markdown(
                f'<div style="margin-top: 60px; padding: 50px 30px; background: rgba(18, 18, 28, 0.95); border-radius: 20px; border: 1px solid rgba(255, 255, 255, 0.1); box-shadow: 0 25px 60px rgba(0,0,0,0.9); text-align: center; backdrop-filter: blur(12px);">{logo_html}<div style="font-size: 32px; font-weight: 800; color: #FFFFFF; letter-spacing: 2px; text-transform: uppercase;">Eaglestar ERP</div><div style="font-size: 15px; color: #4EC9B0; margin-top: 10px; font-family: \'Courier New\', monospace; font-weight: bold;">Enterprise Edition {Config.APP_VERSION}</div><div style="margin-top: 40px; border-bottom: 1px solid rgba(255,255,255,0.15);"></div></div>',
                unsafe_allow_html=True)

            if msg_override: st.warning(msg_override)
            st.markdown("###")
            with st.form("login_form"):
                u = st.text_input("ç”¨æˆ·å", placeholder="è¯·è¾“å…¥è´¦å·")
                p = st.text_input("å¯†ç ", type="password", placeholder="è¯·è¾“å…¥å¯†ç ")
                st.caption("ğŸ”’ å®‰å…¨æç¤ºï¼šè¿ç»­è¾“é”™ 10 æ¬¡å¯†ç å°†è‡ªåŠ¨é”å®šè´¦å·ã€‚")
                if st.form_submit_button("ç™»å½•ç³»ç»Ÿ (Sign In)", type="primary", use_container_width=True):
                    with st.spinner("æ­£åœ¨éªŒè¯èº«ä»½..."):
                        # [Fix] è°ƒç”¨æ›´æ–°åçš„ get_client_ip
                        ip = MainInterface.get_client_ip()
                        ok, user, msg = AuthService.authenticate(u, p, ip=ip)
                        time.sleep(0.5)

                    if ok and user:
                        token = AuthService.refresh_session_token(user.username)
                        st.session_state[session_key] = {"username": user.username, "is_admin": user.is_admin, "ip": ip}
                        st.session_state[last_active_key] = time.time()
                        st.session_state[token_key] = token

                        set_current_user(user.username, ip=ip)
                        AuthService.record_login_event(user.username, ip)
                        st.success(f" ç™»å½•æˆåŠŸ (IP: {ip})")
                        time.sleep(0.5)
                        st.rerun()
                    else:
                        st.error(f" {msg}")

    # =========================================================================
    # 3. ä¸»ç•Œé¢æ¸²æŸ“ (UI Upgrade)
    # =========================================================================

    @staticmethod
    def render_main_app(current_user, session_key):
        username = current_user.get("username", "Guest")
        is_admin = current_user.get("is_admin", False)
        ip = current_user.get("ip", "UNKNOWN")

        set_current_user(username, ip=ip)

        smart_info = MainInterface.get_smart_info(ip)
        role_tag = 'ğŸ›¡ï¸ ç®¡ç†å‘˜' if is_admin else 'ğŸŸ¢ æ™®é€šç”¨æˆ·'

        set_main_background_video()
        render_header()

        col_nav, col_content = st.columns([0.22, 0.78])

        with col_nav:
            # ç”¨æˆ·ä¿¡æ¯å¡ç‰‡ (ç´§å‡‘HTML)
            card_html = f"""
            <div style="padding: 25px 20px;background: rgba(25, 25, 35, 0.95); border-radius: 16px; border: 1px solid rgba(255,255,255,0.08); margin-bottom: 25px;box-shadow: 0 10px 30px rgba(0,0,0,0.3);">
                <div style="font-size: 18px;font-weight: bold; color: #4EC9B0; margin-bottom: 5px;">{smart_info['weather_icon']} {smart_info['greeting']}</div>
                <div style="font-size: 13px;color: #aaa; margin-bottom: 20px; font-style: italic;">{smart_info['tips']}</div>
                <div style="display: flex;align-items: center; margin-bottom: 15px;">
                    <div style="font-size: 32px;margin-right: 10px;">ğŸ‘¤</div>
                    <div>
                        <div style="font-size: 22px;font-weight: 800; color: #FFF; letter-spacing: 0.5px;">{username}</div>
                        <div style="font-size: 12px;color: #FFD700; border: 1px solid #FFD700; border-radius: 4px; padding: 1px 6px; display: inline-block;margin-top: 4px;">{role_tag}</div>
                    </div>
                </div>
                <hr style="border-color: rgba(255,255,255,0.1);margin: 15px 0;">
                <div style="font-size: 13px;color: #ccc; line-height: 1.8;">
                    <div>ğŸ“ <b>ä½ç½®:</b> {smart_info['city']}</div>
                    <div>ğŸŒ¡ï¸ <b>å¤©æ°”:</b> {smart_info['temp']} {smart_info['weather_desc']}</div>
                    <div>ğŸŒ <b>IP:</b> {ip}</div>
                </div>
            </div>
            """
            st.markdown(card_html, unsafe_allow_html=True)

            if st.button("ğŸšª é€€å‡ºç™»å½•", use_container_width=True):
                st.session_state.clear()
                st.rerun()

            st.markdown("###")
            st.markdown("#### â‰¡ å¯¼èˆªèœå•")

            # [Mod] æ–°å¢ Visuals æ¨¡å—
            menu_items = [
                ("ğŸ  ç³»ç»Ÿé¦–é¡µ (Home)", "home"),
                ("ğŸ¤– Google Gemini (AI)", "gemini"),
                ("ğŸ”Œ ä¸Šä¼ æ•°æ® (ETL)", "etl"),
                ("ğŸ“Š å•†ä¸šæ™ºèƒ½æŠ¥è¡¨ (BI)", "reports")
            ]

            # ä»…ç®¡ç†å‘˜å¯è§
            if is_admin:
                menu_items.append(("ğŸ“ˆ æ•°æ®äº¤äº’å¯è§†åŒ– (Visuals)", "visuals"))

            menu_items.extend([
                ("ğŸ› ï¸ ä¿®æ”¹æ•°æ®åº“ (Data)", "db_modify"),
                ("ğŸ—„ï¸ æ•°æ®åº“è¿ç»´ (DB Ops)", "db_admin")
            ])

            if is_admin:
                menu_items.extend([("ğŸ‘¤ ç”¨æˆ·ç®¡ç† (Admin)", "user_admin"), ("ğŸ“œ å®¡è®¡æ—¥å¿— (Audit)", "audit_logs")])

            sel = st.radio("Go to", [m[0] for m in menu_items], label_visibility="collapsed")
            sel_key = next(item[1] for item in menu_items if item[0] == sel)

        with col_content:
            if not MainInterface._check_access(username, is_admin, sel_key):
                st.error(" æƒé™ä¸è¶³ (Access Denied)");
                st.info("è¯·è”ç³»ç®¡ç†å‘˜å¼€é€šæƒé™ã€‚")
            else:
                if sel_key == "home":
                    home.render()
                elif sel_key == "gemini":
                    ai_gemini.render()
                elif sel_key == "etl":
                    etl_ingest.render()
                elif sel_key == "reports":
                    reports.render()
                # [Mod] æ–°å¢è·¯ç”±åˆ†æ”¯
                elif sel_key == "visuals":
                    data_visualization.render()
                elif sel_key == "db_modify":
                    db_data_change.render()
                elif sel_key == "db_admin":
                    db_admin.render()
                elif sel_key == "user_admin":
                    user_admin.render()
                elif sel_key == "audit_logs":
                    audit_logs.render()

    @staticmethod
    def _check_access(username, is_admin, module_key):
        # åŸºç¡€å…¬å…±æ¨¡å—
        if module_key in ["home", "reports", "gemini"]: return True

        # [New] å¯è§†åŒ–æ¨¡å—ä»…é™ç®¡ç†å‘˜
        if module_key == "visuals":
            return is_admin

        # ç®¡ç†å‘˜æ‹¥æœ‰æ‰€æœ‰æƒé™
        if is_admin: return True

        # æ™®é€šç”¨æˆ·æƒé™è¡¨æ£€æŸ¥
        key_map = {
            "etl": "module.etl",
            "db_modify": "module.db_modify",
            "db_admin": "module.db_admin",
            "user_admin": "module.user_admin",
            "audit_logs": "module.audit_logs"
        }
        db_key = key_map.get(module_key)
        if not db_key: return False
        return AuthService.get_permissions(username).get(db_key, False)
==================== END FILE: ui/main_interface.py ====================


==================== START FILE: ui/utils.py ====================
# ui/utils.py

import sys
import time
import datetime
import re
import html
import io
import csv
import importlib
from pathlib import Path
from typing import List, Tuple, Optional
from contextlib import contextmanager
import pandas as pd
import streamlit as st
import tqdm

# å¼•å…¥é…ç½®ä»¥è·å–é¡¹ç›®æ ¹ç›®å½•
from config import Config

# ==============================================================================
# [Critical Fix] å¥å£®æ€§åŠ å›º
# ==============================================================================
if hasattr(tqdm, 'tqdm') and not isinstance(tqdm.tqdm, type):
    importlib.reload(tqdm)


# ==============================================================================
# 1. é€šç”¨å·¥å…·å‡½æ•° (Common Utilities)
# ==============================================================================
def get_client_ip() -> str:
    """
    [æ ‡å‡†ç‰ˆ] è·å–å®¢æˆ·ç«¯çœŸå® IP
    ä¼˜å…ˆä»è¯·æ±‚å¤´è·å– (Cloudflare/Proxy)ï¼Œå…œåº•ä½¿ç”¨ Session Infoã€‚
    """
    try:
        # 1. å°è¯•ä» Streamlit å®˜æ–¹ Headers è·å– (1.30+)
        headers = st.context.headers
        if headers:
            # Cloudflare
            if "Cf-Connecting-Ip" in headers:
                return headers["Cf-Connecting-Ip"]
            # Standard Proxy
            if "X-Forwarded-For" in headers:
                return headers["X-Forwarded-For"].split(",")[0].strip()

        # 2. å°è¯•ä»åº•å±‚ Session è·å– (Hack for local/older versions)
        session_info = st.runtime.get_instance().get_client_session_info()
        if hasattr(session_info, 'ip'):
            return session_info.ip

    except Exception:
        pass

    return "UNKNOWN"


def parse_compound_csv(file_path: Path) -> List[Tuple[str, pd.DataFrame]]:
    """
    [æ™ºèƒ½è§£æå™¨] è§£æåŒ…å«å¤šä¸ªå­è¡¨çš„å¤æ‚ CSV æ–‡ä»¶ (ä¿®å¤ Expected X fields saw Y é”™è¯¯)

    åŸç†ï¼š
    1. æŒ‰è¡Œè¯»å–æ–‡ä»¶ã€‚
    2. è¯†åˆ«åˆ†éš”ç¬¦ï¼ˆç©ºè¡Œæˆ–æ ‡é¢˜è¡Œï¼‰ã€‚
    3. å°†è¿ç»­çš„æ•°æ®å—åˆ‡å‰²å¹¶ç‹¬ç«‹è½¬ä¸º DataFrameã€‚

    Returns:
        List of (Title, DataFrame)
    """
    if not file_path.exists():
        return []

    tables = []
    current_lines = []
    current_title = "Main Table"

    # å…³é”®è¯è¯†åˆ«æ ‡é¢˜è¡Œ (Profitè¡¨ç”¨ ===, Shippingè¡¨ç”¨ è¡¨X, è‹±æ–‡ç”¨ Table)
    title_pattern = re.compile(r'^(===.+===|è¡¨\d+|Table\s*\d+)', re.IGNORECASE)

    try:
        with open(file_path, "r", encoding="utf-8-sig") as f:
            all_lines = f.readlines()

        for line in all_lines:
            stripped = line.strip()

            # Case 1: ç©ºè¡Œ -> å¯èƒ½æ˜¯åˆ†éš”ç¬¦
            if not stripped:
                if current_lines:
                    # å°è¯•å°†ç¼“å†²åŒºè§£æä¸ºè¡¨
                    try:
                        # è¿‡æ»¤æ‰çº¯æ³¨é‡Šè¡Œ
                        valid_lines = [l for l in current_lines if not l.startswith("#")]
                        if valid_lines:
                            csv_io = io.StringIO("".join(valid_lines))
                            df = pd.read_csv(csv_io)
                            if not df.empty and len(df.columns) > 1:
                                tables.append((current_title, df))
                    except:
                        pass
                    current_lines = []
                continue

            # Case 2: æ ‡é¢˜è¡Œ -> å¼ºåˆ¶åˆ†éš”
            # å¦‚æœè¡Œé‡Œæ²¡æœ‰é€—å·ï¼Œä¸”ç¬¦åˆæ ‡é¢˜ç‰¹å¾
            if "," not in stripped and title_pattern.match(stripped):
                # å…ˆä¿å­˜ä¹‹å‰çš„ buffer
                if current_lines:
                    try:
                        csv_io = io.StringIO("".join(current_lines))
                        df = pd.read_csv(csv_io)
                        if not df.empty and len(df.columns) > 1:
                            tables.append((current_title, df))
                    except:
                        pass
                    current_lines = []

                # æ›´æ–°æ ‡é¢˜ (å»æ‰ ===)
                current_title = stripped.replace("===", "").strip()
                continue

            # Case 3: æŠ¥è¡¨åº•éƒ¨çš„è¯´æ˜æ–‡å­— (Footer)
            # ç‰¹å¾ï¼šå¾ˆé•¿ï¼Œæˆ–è€…ä»¥ "è¯´æ˜"ã€"å¤‡æ³¨" å¼€å¤´ï¼Œä¸”é€—å·å¾ˆå°‘
            if (stripped.startswith("è¯´æ˜") or stripped.startswith(
                    "å¤‡æ³¨") or "é€»è¾‘è¯´æ˜" in stripped) and stripped.count(',') < 2:
                # é‡åˆ° Footerï¼Œç›´æ¥ç»“æŸå½“å‰çš„æ”¶é›†
                if current_lines:
                    try:
                        csv_io = io.StringIO("".join(current_lines))
                        df = pd.read_csv(csv_io)
                        if not df.empty:
                            tables.append((current_title, df))
                    except:
                        pass
                    current_lines = []
                # Footer ä¸ä½œä¸º DataFrame å±•ç¤º
                continue

            # Case 4: æ™®é€šæ•°æ®è¡Œ
            current_lines.append(line)

        # Loop ç»“æŸï¼Œå¤„ç†æœ€åä¸€å—
        if current_lines:
            try:
                csv_io = io.StringIO("".join(current_lines))
                df = pd.read_csv(csv_io)
                if not df.empty:
                    tables.append((current_title, df))
            except:
                pass

    except Exception as e:
        print(f"è§£æ CSV å¤±è´¥: {e}")
        # å…œåº•ï¼šå°è¯•ä½œä¸ºå•è¡¨è¯»å–
        try:
            df = pd.read_csv(file_path)
            return [("Raw Data", df)]
        except:
            return []

    return tables


# ==============================================================================
# 2. æ—¥å¿—å¤„ç†å™¨ (Log Handlers)
# ==============================================================================
class StreamlitLogHandler:
    """
    [UIç»„ä»¶] åŒæµæ—¥å¿—å¤„ç†å™¨ (Dual-Stream Log Handler)
    """

    def __init__(self, container, original_stdout, max_height_css="240px"):
        self.container = container
        self.original_stdout = original_stdout  # ä¿å­˜åŸå§‹æ§åˆ¶å°ç®¡é“
        self.buffer = []
        self.base_dir_str = str(Config.BASE_DIR).replace("\\", "/")

        self.css_style = f"""
            <style>
            .log-box {{
                height: {max_height_css};
                overflow-y: auto;
                background-color: #1e1e1e;
                color: #e0e0e0;
                padding: 12px;
                border-radius: 6px;
                font-family: 'Menlo', 'Consolas', monospace;
                font-size: 13px;
                border: 1px solid #444;
                white-space: pre-wrap;
                word-wrap: break-word;
                line-height: 1.5;
                display: flex;
                flex-direction: column;
            }}
            .log-meta {{ color: #808080; font-size: 0.85em; margin-right: 8px; }}
            .log-success {{ color: #4ec9b0; font-weight: bold; }}
            .log-error {{ color: #f44336; font-weight: bold; }}
            .log-warn {{ color: #cca700; }}
            </style>
        """

    def _sanitize_for_ui(self, text: str) -> str:
        clean_text = text.replace("\\", "/")
        if self.base_dir_str in clean_text:
            clean_text = clean_text.replace(self.base_dir_str, ".")
        clean_text = re.sub(r'/(?:Users|home)/[^/]+', '~', clean_text)

        css_class = ""
        if "" in clean_text or "æˆåŠŸ" in clean_text:
            css_class = "log-success"
        elif "" in clean_text or "å¤±è´¥" in clean_text or "Error" in clean_text:
            css_class = "log-error"
        elif "âš ï¸" in clean_text or "Warning" in clean_text:
            css_class = "log-warn"

        escaped = html.escape(clean_text)
        if css_class:
            return f'<span class="{css_class}">{escaped}</span>'
        else:
            return escaped

    def write(self, text):
        self.original_stdout.write(text)
        if not text or text == "\n" or text == "\r": return

        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        sanitized_msg = self._sanitize_for_ui(text.strip())
        line_html = f'<div><span class="log-meta">[{timestamp}]</span>{sanitized_msg}</div>'
        self.buffer.append(line_html)

        if len(self.buffer) > 80:
            self.buffer.pop(0)

        content = "".join(self.buffer)
        js_scroll = """
        <script>
            try {
                var box = window.parent.document.getElementById("log-container-div");
                if (!box) box = document.getElementById("log-container-div");
                if (box) box.scrollTop = box.scrollHeight;
            } catch(e) {}
        </script>
        """
        full_html = f'{self.css_style}<div id="log-container-div" class="log-box">{content}</div>{js_scroll}'
        self.container.markdown(full_html, unsafe_allow_html=True)

    def flush(self):
        self.original_stdout.flush()


class StreamlitTqdm(tqdm.tqdm):
    def __init__(self, iterable=None, desc=None, total=None, leave=True, file=None,
                 cols=100, mininterval=0.1, maxinterval=10.0, miniters=None,
                 ascii=None, disable=False, unit='it', unit_scale=False,
                 dynamic_ncols=False, smoothing=0.3, bar_format=None, initial=0,
                 position=None, postfix=None, unit_divisor=1000, write_bytes=False,
                 lock_args=None, nrows=None, colour=None, delay=0, gui=False,
                 **kwargs):
        self.st_progress_bar = kwargs.pop('st_progress_bar', None)
        self.st_status_text = kwargs.pop('st_status_text', None)
        super().__init__(iterable, desc, total, leave, file, cols, mininterval,
                         maxinterval, miniters, ascii, disable, unit, unit_scale,
                         dynamic_ncols, smoothing, bar_format, initial, position,
                         postfix, unit_divisor, write_bytes, lock_args, nrows,
                         colour, delay, gui, **kwargs)

    def display(self, msg=None, pos=None):
        if self.total and self.total > 0:
            pct = max(0, min(1.0, self.n / self.total))
        else:
            pct = 0.0
        if self.st_progress_bar:
            self.st_progress_bar.progress(pct)
        if self.st_status_text:
            elapsed_str = self.format_interval(self.format_dict['elapsed'])
            status_msg = f"ğŸ”„ **{self.desc or 'Processing'}** | {int(pct * 100)}% | â³ {elapsed_str}"
            self.st_status_text.markdown(status_msg)
        return True


@contextmanager
def task_monitor(title: str = "ä»»åŠ¡æ‰§è¡Œä¸­"):
    st.markdown(f"### {title}")
    c1, c2 = st.columns([1, 5])
    c1.markdown("**æ€»ä½“è¿›åº¦**")
    main_bar = c2.progress(0)
    c3, c4 = st.columns([1, 5])
    c3.markdown("**å½“å‰ä»»åŠ¡**")
    sub_bar = c4.progress(0)
    status_text = st.empty()
    log_container = st.empty()

    original_stdout = sys.stdout
    original_tqdm = tqdm.tqdm
    logger = StreamlitLogHandler(log_container, original_stdout)
    sys.stdout = logger

    def custom_tqdm_factory(*args, **kwargs):
        kwargs['st_progress_bar'] = sub_bar
        kwargs['st_status_text'] = status_text
        return StreamlitTqdm(*args, **kwargs)

    tqdm.tqdm = custom_tqdm_factory

    class MonitorController:
        def update_main_progress(self, percent: float):
            main_bar.progress(max(0.0, min(1.0, percent)))

        def log(self, msg: str):
            print(msg)

    try:
        yield MonitorController()
    finally:
        sys.stdout = original_stdout
        tqdm.tqdm = original_tqdm
        status_text.empty()
==================== END FILE: ui/utils.py ====================


==================== START FILE: ui/styles.py ====================
# ui/styles.py
import streamlit as st

class StyleManager:
    """
    [UIç»„ä»¶] å…¨å±€æ ·å¼ç®¡ç†å™¨
    èŒè´£ï¼šé›†ä¸­ç®¡ç† CSS æ³¨å…¥ï¼Œå‡€åŒ–ä¸šåŠ¡ä»£ç ã€‚
    """

    # æ ¸å¿ƒç»„ä»¶æ ·å¼ (Tab, Radio, ç­‰)
    _MAIN_STYLES = """
    <style>
        /* éšè—é»˜è®¤ç»„ä»¶ */
        [data-testid="stSidebar"] {display: none;} 
        #MainMenu {visibility: hidden;} 
        footer {visibility: hidden;}

        /* Radio Group ç¾åŒ– */
        div[role="radiogroup"] { 
            padding-left: 10px;
        }
        div[role="radiogroup"] label { 
            transition: all 0.2s cubic-bezier(0.175, 0.885, 0.32, 1.275) !important; 
            border-radius: 8px;
            padding: 8px 12px !important; 
            margin-bottom: 4px; 
            border: 1px solid transparent;
        }
        div[role="radiogroup"] label:hover { 
            transform: scale(1.5) translateX(30px) !important; 
            background: rgba(78, 201, 176, 0.2) !important;
            border: 1px solid rgba(78, 201, 176, 0.5) !important; 
            box-shadow: 0 10px 25px rgba(0,0,0,0.5) !important; 
            z-index: 999 !important; 
            color: #FFFFFF !important;
            font-weight: bold !important; 
        }

        /* Tab ç»„ä»¶ç¾åŒ– */
        button[data-baseweb="tab"] { 
            transition: all 0.2s cubic-bezier(0.175, 0.885, 0.32, 1.275) !important; 
            border-radius: 8px !important;
            margin: 0 4px !important; 
            border: 1px solid transparent !important; 
        }
        button[data-baseweb="tab"]:hover { 
            transform: scale(1.2) translateY(-3px) !important;
            background-color: rgba(78, 201, 176, 0.15) !important; 
            border: 1px solid rgba(78, 201, 176, 0.5) !important; 
            box-shadow: 0 5px 15px rgba(0,0,0,0.4) !important;
            color: #FFFFFF !important; 
            z-index: 999 !important; 
            font-weight: bold !important; 
        }
    </style>
    """

    @classmethod
    def apply_global_styles(cls):
        """æ³¨å…¥å…¨å±€ CSS"""
        st.markdown(cls._MAIN_STYLES, unsafe_allow_html=True)
==================== END FILE: ui/styles.py ====================


==================== START FILE: ui/components/inventory_wizard.py ====================
# ui/components/inventory_wizard.py

import streamlit as st
import time
from datetime import date
from config import Config
from core.etl.inventory import InventoryLoader
from core.services.correction_service import CorrectionService
from core.services.file_service import FileService


class InventoryWizard:
    """
    [UIç»„ä»¶] åº“å­˜åŒæ­¥å‘å¯¼ (Inventory Wizard) - Enterprise V2.0

    èŒè´£:
    1. ä¸Šä¼ åº“å­˜ CSVã€‚
    2. æ ¡éªŒ SKU åˆæ³•æ€§ (ä¸ Data_COGS æ¯”å¯¹)ã€‚
    3. æä¾›éæ³• SKU çš„ä¿®å¤/æ–°å¢å…¥å£ã€‚
    4. æ‰§è¡Œæœ€ç»ˆå…¥åº“åŒæ­¥ã€‚
    """

    def __init__(self):
        self.loader = InventoryLoader()
        self.corrector = CorrectionService()  # ç”¨äºæ¨¡ç³Šæœç´¢å»ºè®®
        self.fs = FileService()
        self._init_state()

    def _init_state(self):
        if 'inv_state' not in st.session_state:
            st.session_state.inv_state = 'UPLOAD'
        if 'inv_df' not in st.session_state:
            st.session_state.inv_df = None
        if 'inv_errors' not in st.session_state:
            st.session_state.inv_errors = []
        if 'inv_error_idx' not in st.session_state:
            st.session_state.inv_error_idx = 0

    def _reset(self):
        for k in ['inv_state', 'inv_df', 'inv_errors', 'inv_error_idx', 'inv_target_month', 'inv_file_path']:
            if k in st.session_state: del st.session_state[k]
        st.rerun()

    def render(self):
        state = st.session_state.inv_state
        if state == 'UPLOAD':
            self._step_upload()
        elif state == 'FIXING':
            self._step_fixing()
        elif state == 'COMMIT':
            self._step_commit()

    # --- Step 1: Upload ---
    def _step_upload(self):
        st.info("æ­¥éª¤ 1: ä¸Šä¼ åº“å­˜ç›˜ç‚¹æ–‡ä»¶ (éœ€åŒ…å« SKU, Quantity åˆ—)")

        c1, c2 = st.columns([2, 1])
        with c1:
            inv_file = st.file_uploader("é€‰æ‹© CSV æ–‡ä»¶", type=['csv'], key="inv_up")
        with c2:
            default_date = date.today().replace(day=1)
            target_date = st.date_input("å½’å±æœˆä»½", value=default_date)
            # æ ¼å¼åŒ–ä¸º YYYY-MM
            target_month = target_date.strftime("%Y-%m")

        if st.button("ğŸ” å¼€å§‹æ ¡éªŒ", type="primary", use_container_width=True):
            if not inv_file:
                st.error("è¯·å…ˆä¸Šä¼ æ–‡ä»¶")
                return

            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            temp_path = Config.UPLOADER_DIR / inv_file.name
            Config.UPLOADER_DIR.mkdir(parents=True, exist_ok=True)
            with open(temp_path, "wb") as f:
                f.write(inv_file.getbuffer())

            # è°ƒç”¨ Service æ ¡éªŒ
            with st.spinner("æ­£åœ¨æ ¡éªŒ SKU..."):
                passed, unknown_skus, df_clean = self.loader.validate_file(temp_path)

            # æ›´æ–°çŠ¶æ€
            st.session_state.inv_df = df_clean
            st.session_state.inv_target_month = target_month
            st.session_state.inv_file_path = temp_path

            if passed:
                st.session_state.inv_state = 'COMMIT'
                st.success(" æ ¡éªŒé€šè¿‡ï¼")
                time.sleep(0.5)
                st.rerun()
            else:
                st.session_state.inv_errors = sorted(unknown_skus)
                st.session_state.inv_error_idx = 0
                st.session_state.inv_state = 'FIXING'
                st.warning(f"âš ï¸ å‘ç° {len(unknown_skus)} ä¸ªæœªçŸ¥ SKUï¼Œè¿›å…¥ä¿®å¤å‘å¯¼ã€‚")
                time.sleep(1)
                st.rerun()

    # --- Step 2: Fix ---
    def _step_fixing(self):
        errors = st.session_state.inv_errors
        idx = st.session_state.inv_error_idx
        total = len(errors)

        if idx >= total:
            st.session_state.inv_state = 'COMMIT'
            st.rerun()
            return

        bad_sku = errors[idx]
        pct = (idx + 1) / total
        st.progress(pct, text=f"ä¿®å¤è¿›åº¦: {idx + 1}/{total}")

        c_left, c_right = st.columns([1, 2])

        with c_left:
            st.error(f" æœªçŸ¥ SKU: **{bad_sku}**")
            # ç»Ÿè®¡è¯¥ SKU åœ¨æ–‡ä»¶ä¸­çš„æ€»æ•°
            df = st.session_state.inv_df
            qty = df[df['SKU'] == bad_sku]['Quantity'].sum()
            st.caption(f"æ–‡ä»¶å†…æ€»æ•°é‡: {qty}")

        with c_right:
            tab_map, tab_new = st.tabs(["ğŸ”— æ˜ å°„åˆ°ç°æœ‰ SKU", "â• æ³¨å†Œä¸ºæ–° SKU"])

            # Option A: Map
            with tab_map:
                suggestions = self.corrector.get_fuzzy_suggestions(bad_sku)
                # ä½¿ç”¨ selectbox æ”¯æŒæœç´¢
                # æ—¢ç„¶æ˜¯ Serviceï¼Œæˆ‘ä»¬åº”è¯¥ä» corrector è·å–æ‰€æœ‰ valid_skusï¼Œä½†è¿™é‡Œä¸ºäº†æ€§èƒ½ï¼Œ
                # æˆ‘ä»¬å‡è®¾ valid_skus å·²ç»åœ¨ corrector.valid_skus ä¸­ç¼“å­˜
                all_valid = sorted(list(self.corrector.valid_skus))

                # é»˜è®¤é€‰é¡¹é€»è¾‘
                default_idx = 0
                if suggestions:
                    try:
                        default_idx = all_valid.index(suggestions[0])
                    except:
                        pass

                target_sku = st.selectbox("é€‰æ‹©æ­£ç¡® SKU:", all_valid, index=default_idx, key=f"sel_{idx}")

                if st.button("ç¡®è®¤æ˜ å°„", key=f"btn_map_{idx}"):
                    # ä¿®æ”¹å†…å­˜ä¸­çš„ DF
                    st.session_state.inv_df['SKU'] = st.session_state.inv_df['SKU'].replace(bad_sku, target_sku)
                    st.toast(f"å·²æ˜ å°„: {bad_sku} -> {target_sku}")
                    st.session_state.inv_error_idx += 1
                    time.sleep(0.2)
                    st.rerun()

            # Option B: Create
            with tab_new:
                opts = self.loader.get_category_options()  # Service Call
                with st.form(key=f"frm_new_{idx}"):
                    c1, c2 = st.columns(2)
                    n_sku = c1.text_input("SKU Name", value=bad_sku)
                    n_cat = c2.selectbox("Category", [""] + opts["Category"])

                    c3, c4 = st.columns(2)
                    n_sub = c3.selectbox("SubCategory", [""] + opts["SubCategory"])
                    n_typ = c4.selectbox("Type", [""] + opts["Type"])

                    c5, c6 = st.columns(2)
                    n_cost = c5.number_input("Cost", 0.0, step=0.01)
                    n_frt = c6.number_input("Freight", 0.0, step=0.01)

                    if st.form_submit_button("æ³¨å†Œå¹¶ä½¿ç”¨"):
                        # è°ƒç”¨ Service æ³¨å†Œ
                        try:
                            self.loader.register_new_sku(n_sku, n_cat, n_sub, n_typ, n_cost, n_frt)
                            # å¦‚æœæ³¨å†Œçš„ SKU å’ŒåŸæ¥çš„ä¸ä¸€æ ·ï¼ˆç”¨æˆ·æ”¹åäº†ï¼‰ï¼Œåˆ™æ›´æ–° DF
                            if n_sku != bad_sku:
                                st.session_state.inv_df['SKU'] = st.session_state.inv_df['SKU'].replace(bad_sku, n_sku)

                            st.success(f"æ–° SKU {n_sku} æ³¨å†ŒæˆåŠŸï¼")
                            st.session_state.inv_error_idx += 1
                            time.sleep(0.5)
                            st.rerun()
                        except Exception as e:
                            st.error(f"æ³¨å†Œå¤±è´¥: {e}")

    # --- Step 3: Commit ---
    def _step_commit(self):
        target_month = st.session_state.inv_target_month
        df = st.session_state.inv_df

        st.success("ğŸ‰ æ‰€æœ‰ SKU æ ¡éªŒå®Œæ¯•ï¼")
        st.metric("å‡†å¤‡å…¥åº“è®°å½•æ•°", len(df))
        st.metric("ç›®æ ‡æœˆä»½åˆ—", target_month)

        # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ (Service Call)
        col_exists = self.loader.check_column_exists(target_month)
        if col_exists:
            st.warning(f"âš ï¸ è­¦å‘Š: æœˆä»½åˆ— `{target_month}` å·²å­˜åœ¨ï¼Œç»§ç»­æ“ä½œå°†è¦†ç›–è¯¥åˆ—çš„ç°æœ‰æ•°æ®ï¼")

        pwd = st.text_input("ğŸ”‘ è¯·è¾“å…¥æ“ä½œå¯†ç :", type="password")

        if st.button("ğŸš€ æ‰§è¡Œå…¥åº“", type="primary", use_container_width=True):
            if pwd != Config.DB_OPERATOR_PWD:
                st.error(" å¯†ç é”™è¯¯")
                return

            with st.status("æ­£åœ¨æ‰§è¡ŒåŒæ­¥...", expanded=True) as status:
                status.write("ğŸ’¾ å†™å…¥æ•°æ®åº“...")
                # Service Call
                msg = self.loader.sync_to_db(df, target_month)

                status.write("ğŸ“¦ å½’æ¡£æ–‡ä»¶...")
                # é‡æ–°ä¿å­˜æœ€æ–°çš„ DF (å¯èƒ½ä¿®æ­£è¿‡ SKU)
                temp_path = st.session_state.inv_file_path
                df.to_csv(temp_path, index=False)

                self.fs.organize_uploader(inventory_ym=target_month.replace("-", "_"))
                self.fs.archive_all()

                status.update(label=" å…¨éƒ¨å®Œæˆ", state="complete")
                st.success(msg)
                time.sleep(2)
                self._reset()

==================== END FILE: ui/components/inventory_wizard.py ====================


==================== START FILE: ui/components/__init__.py ====================


==================== END FILE: ui/components/__init__.py ====================


==================== START FILE: ui/components/transaction_wizard.py ====================
# ui/components/transaction_wizard.py

import streamlit as st
import time
from pathlib import Path
import traceback

from config import Config
from ui.utils import task_monitor
from core.services.file_service import FileService
from core.services.correction_service import CorrectionService
from core.etl.ingest import TransactionLoader, EarningLoader
from core.etl.parser import TransactionParser
from core.etl.transformer import TransactionTransformer


class TransactionWizard:
    """
    [UIç»„ä»¶] äº¤æ˜“æµæ°´çº¿å‘å¯¼ - Enterprise V2.3 (Full UI Lock)

    Fix Log:
    - V2.2: å¼•å…¥ 'etl_processing' çŠ¶æ€é”ï¼Œé˜²æ­¢ç”¨æˆ·åœ¨ ETL è¿è¡Œæ—¶é‡å¤ç‚¹å‡»æŒ‰é’®å¯¼è‡´å¡æ­»ã€‚
    - V2.3: ä¿®å¤æ¸…æ´—å®Œæˆåçš„â€œç»§ç»­æ‰§è¡Œâ€æŒ‰é’®ï¼Œå¢åŠ é˜²æ­¢è¯¯è§¦å˜ç°æœºåˆ¶ã€‚
    """

    def __init__(self):
        self.corrector = CorrectionService()
        self.fs = FileService()
        self._init_state()

    def _init_state(self):
        """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
        if 'trans_state' not in st.session_state:
            st.session_state.trans_state = 'UPLOAD'
        if 'trans_pending_count' not in st.session_state:
            st.session_state.trans_pending_count = 0
        # [New] å¢åŠ è¿è¡Œé”ï¼Œé˜²æ­¢é‡å¤ç‚¹å‡»
        if 'etl_processing' not in st.session_state:
            st.session_state.etl_processing = False

    def _reset_state_callback(self):
        st.session_state.trans_state = 'UPLOAD'
        st.session_state.trans_pending_count = 0
        st.session_state.etl_processing = False

    def _lock_etl(self):
        """[Callback] ç‚¹å‡»æŒ‰é’®ç¬é—´é”å®š UI"""
        st.session_state.etl_processing = True

    def render(self):
        state = st.session_state.trans_state
        if state == 'UPLOAD':
            self._render_upload_step()
        elif state == 'CLEANING':
            self._render_cleaning_step()

    def _render_upload_step(self):
        st.info("ğŸ’¡ ç¬¬ä¸€æ­¥ï¼šè¯·ä¸Šä¼  eBay åŸå§‹æŠ¥è¡¨ (Transaction / Order Earnings)ã€‚")

        # æ–‡ä»¶ä¸Šä¼ å™¨
        uploaded_files = st.file_uploader(
            "æ‹–æ‹½æˆ–ç‚¹å‡»ä¸Šä¼  CSV æ–‡ä»¶",
            type=['csv'],
            accept_multiple_files=True,
            key="trans_uploader"
        )

        # [UI Lock] æŒ‰é’®çŠ¶æ€æ§åˆ¶
        # å¦‚æœæ­£åœ¨å¤„ç†ä¸­ï¼ŒæŒ‰é’®å˜ç°ï¼Œä¸”æ–‡å­—å˜åŒ–
        is_running = st.session_state.etl_processing
        btn_label = "â³ æ­£åœ¨æ‰§è¡Œä¸­..." if is_running else "ğŸš€ å¯åŠ¨ ETL æµæ°´çº¿"

        # æ¸²æŸ“æŒ‰é’® (æ³¨æ„ï¼šä¸å†ç›´æ¥åœ¨ if st.button ä¸­å†™é€»è¾‘)
        st.button(
            btn_label,
            type="primary",
            use_container_width=True,
            disabled=is_running,  # æ ¸å¿ƒï¼šæ ¹æ®çŠ¶æ€ç¦ç”¨
            on_click=self._lock_etl  # æ ¸å¿ƒï¼šç‚¹å‡»å³é”å®š
        )

        # [Execution Block] åŸºäºçŠ¶æ€è§¦å‘é€»è¾‘
        if is_running:
            if not uploaded_files:
                st.error(" è¯·å…ˆä¸Šä¼ æ–‡ä»¶ï¼")
                st.session_state.etl_processing = False  # é‡Šæ”¾é”
                # è¿™é‡Œä¸éœ€è¦ rerunï¼Œå› ä¸ºæŠ¥é”™åç”¨æˆ·éœ€è¦æ“ä½œ
                return

            try:
                # --- ETL æµç¨‹å¼€å§‹ ---
                with task_monitor("ETL å¤„ç†è¿›åº¦ç›‘æ§") as monitor:
                    # 1. æ¥æ”¶æ–‡ä»¶ (10%)
                    monitor.log("ğŸ“‚ [Step 1/4] æ¥æ”¶æ–‡ä»¶...")
                    Config.UPLOADER_DIR.mkdir(parents=True, exist_ok=True)
                    for f in uploaded_files:
                        with open(Config.UPLOADER_DIR / f.name, "wb") as buffer:
                            buffer.write(f.getbuffer())
                    monitor.update_main_progress(0.1)

                    # 2. æ•´ç† (20%)
                    monitor.log("\nğŸ§¹ [Step 2/4] æ–‡ä»¶æ ‡å‡†åŒ–...")
                    self.fs.organize_uploader()
                    monitor.update_main_progress(0.2)

                    # 3. æ‘„å…¥ (40%)
                    monitor.log("\nğŸ“¥ [Step 3/4] æ•°æ®æ‘„å…¥ (Ingest)...")
                    TransactionLoader().run()
                    EarningLoader().run()
                    monitor.update_main_progress(0.4)

                    # 4. è§£æ (60%)
                    monitor.log("\nğŸ§© [Step 4/4] æ™ºèƒ½è§£æ (Parser)...")
                    parser = TransactionParser()
                    res = parser.run()

                    fixed_cnt = len(res.get('auto_fixed', []))
                    pending_cnt = res.get('pending_count', 0)
                    monitor.log(f"   ->  è‡ªåŠ¨ä¿®å¤: {fixed_cnt} æ¡")
                    monitor.log(f"   -> ğŸš© å¾…äººå·¥ä»‹å…¥: {pending_cnt} æ¡")
                    monitor.update_main_progress(0.6)

                    # 5. è½¬æ¢ (100%)
                    st.session_state.trans_pending_count = pending_cnt

                    if pending_cnt > 0:
                        monitor.log(f"\nâš ï¸ å‘ç° {pending_cnt} ä¸ªå¼‚å¸¸æ•°æ®ï¼Œè½¬å…¥æ¸…æ´—æ¨¡å¼ã€‚")
                        time.sleep(1.5)  # ç»™ä¸€ç‚¹æ—¶é—´å±•ç¤ºæ—¥å¿—
                        st.session_state.trans_state = 'CLEANING'
                        # æ³¨æ„ï¼šè¿™é‡Œä¸ç«‹åˆ» rerunï¼Œè€Œæ˜¯è®© finally å—å¤„ç†è§£é”å’Œåˆ·æ–°
                    else:
                        monitor.log("\n æ ¡éªŒé€šè¿‡ï¼Œè¿›å…¥æ ¸å¿ƒè½¬æ¢å¼•æ“...")
                        time.sleep(0.5)
                        self._run_final_transform(monitor)

            except Exception as e:
                st.error(f" ä¸¥é‡é”™è¯¯: {str(e)}")
                # æ‰“å°å †æ ˆåˆ° UI æ–¹ä¾¿è°ƒè¯•
                st.code(traceback.format_exc())

            finally:
                # [Critical] æ— è®ºæˆåŠŸå¤±è´¥ï¼Œå¿…é¡»é‡Šæ”¾é”
                st.session_state.etl_processing = False
                # å¼ºåˆ¶åˆ·æ–° UIï¼Œæ¢å¤æŒ‰é’®çŠ¶æ€æˆ–è·³è½¬é¡µé¢
                st.rerun()

    def _render_cleaning_step(self):
        count = st.session_state.trans_pending_count
        st.warning(f"âš ï¸ æ•°æ®æ¸…æ´—æ¨¡å¼ï¼šå½“å‰å‰©ä½™ {count} æ¡å¼‚å¸¸è®°å½•éœ€è¦å¤„ç†ã€‚")

        row = self.corrector.get_next_pending_issue()
        if row is None:
            st.balloons()
            st.success("ğŸ‰ æ‰€æœ‰å¼‚å¸¸å·²ä¿®å¤ï¼å‡†å¤‡ç»§ç»­æ‰§è¡Œã€‚")

            # --- [Fix Start] å¢åŠ æŒ‰é’®é˜²è¯¯è§¦æœºåˆ¶ ---
            is_running = st.session_state.etl_processing
            btn_label = "â³ æ­£åœ¨æ‰§è¡Œä¸­..." if is_running else "ğŸš€ ç»§ç»­æ‰§è¡Œ (Resume)"

            st.button(
                btn_label,
                type="primary",
                use_container_width=True,
                disabled=is_running,  # è¿è¡Œæ—¶å˜ç°
                on_click=self._lock_etl  # ç‚¹å‡»å³é”å®š
            )

            if is_running:
                try:
                    with task_monitor("æ­£åœ¨å®Œæˆå‰©ä½™æ­¥éª¤...") as monitor:
                        self._run_final_transform(monitor)
                except Exception as e:
                    st.error(f"æ‰§è¡Œå‡ºé”™: {e}")
                    st.code(traceback.format_exc())
                finally:
                    # [ç‰¹åˆ«æ³¨æ„] è¿™é‡Œåªé‡Šæ”¾é”ï¼Œä¸æ‰§è¡Œ st.rerun()
                    # åŸå› ï¼šå¦‚æœ Rerunï¼Œä¸Šé¢ _run_final_transform ç”»å‡ºçš„æˆåŠŸä¿¡æ¯å’Œ"è¿”å›å¼€å§‹"æŒ‰é’®ä¼šè¢«æ¸…ç©ºã€‚
                    # æˆ‘ä»¬å¸Œæœ›é¡µé¢åœç•™åœ¨æˆåŠŸçŠ¶æ€ï¼Œç›´åˆ°ç”¨æˆ·ç‚¹å‡»"è¿”å›å¼€å§‹"ã€‚
                    st.session_state.etl_processing = False
            # --- [Fix End] ---

            return

        error_info = self._find_first_bad_sku(row)
        if not error_info:
            # è‡ªæ„ˆé€»è¾‘ï¼šå¦‚æœæ‰¾ä¸åˆ°é”™è¯¯ä½†è¢«æ ‡è®°äº†ï¼Œè‡ªåŠ¨è·³è¿‡
            self.corrector.mark_as_skipped(row['Order number'])
            st.rerun()
            return

        self._render_fix_card(row, error_info)

    def _render_fix_card(self, row, error_info):
        bad_sku = error_info['bad_sku']
        bad_qty = error_info['bad_qty']

        with st.container(border=True):
            c_head1, c_head2 = st.columns([3, 1])
            with c_head1:
                st.markdown(f"#### ğŸš© å¼‚å¸¸ SKU: `{bad_sku}`")
            with c_head2:
                if st.button("è·³è¿‡æ­¤æ¡", key="btn_skip"):
                    self.corrector.mark_as_skipped(row['Order number'])
                    st.rerun()

            c1, c2 = st.columns(2)
            with c1:
                st.caption("åŸå§‹æ ‡ç­¾")
                st.code(row.get('Custom label', ''), language="text")
            with c2:
                st.caption("å•†å“æ ‡é¢˜")
                st.info(row.get('Item title', '-'))

            st.divider()
            suggestions = self.corrector.get_fuzzy_suggestions(bad_sku)
            opts = suggestions + ["âœï¸ æ‰‹åŠ¨å½•å…¥"]
            sel = st.radio("æ™ºèƒ½æ¨è:", opts, horizontal=True)

            c_in1, c_in2 = st.columns(2)
            with c_in1:
                final_sku = st.text_input("æ­£ç¡® SKU", value=bad_sku if sel == "âœï¸ æ‰‹åŠ¨å½•å…¥" else sel).strip().upper()
            with c_in2:
                final_qty = st.text_input("æ•°é‡", value=bad_qty).strip()

            if st.button(" ç¡®è®¤ä¿®å¤", type="primary", use_container_width=True):
                # ä¿®å¤é€»è¾‘
                if self.corrector.apply_fix(row['Order number'], error_info['index'], row.get('Custom label', ''),
                                            bad_sku, bad_qty, final_sku, str(int(float(final_qty)))):
                    st.toast(f"å·²ä¿®å¤: {final_sku}")
                    time.sleep(0.5)
                    st.rerun()
                else:
                    st.error("ä¿å­˜å¤±è´¥")

    def _run_final_transform(self, monitor):
        """æ‰§è¡Œæœ€åä¸€æ­¥è½¬æ¢ (å¸¦ç»†ç²’åº¦è¿›åº¦)"""
        monitor.log("\nâš™ï¸ [Step 5/5] å¯åŠ¨å‘é‡åŒ–è½¬æ¢å¼•æ“ (Vectorized Transformer)...")

        transformer = TransactionTransformer()

        def update_progress(p, msg):
            # è½¬æ¢é˜¶æ®µå æ€»è¿›åº¦çš„ 30% (0.6 -> 0.9)
            base = 0.6
            scale = 0.3
            real_p = base + (p * scale)
            monitor.update_main_progress(real_p)

        transformer.run(progress_callback=update_progress)

        monitor.log("\nğŸ“¦ [Final] æ–‡ä»¶å½’æ¡£...")
        self.fs.archive_all()

        monitor.update_main_progress(1.0)
        st.balloons()
        st.success(" ETL ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼")
        st.markdown("---")
        # é‡ç½®æŒ‰é’®ï¼Œç‚¹å‡»åé€šè¿‡å›è°ƒæ¸…ç†çŠ¶æ€
        st.button("ğŸ”„ è¿”å›å¼€å§‹", on_click=self._reset_state_callback, use_container_width=True)

    def _find_first_bad_sku(self, row):
        for i in range(1, 11):
            s = str(row.get(f'P_SKU{i}', '')).strip().upper()
            q = str(row.get(f'P_Quantity{i}', '')).strip()
            if not s or s == 'NONE': continue
            if not self.corrector.is_valid_sku(s) or not self._is_valid_qty(q):
                return {"index": i, "bad_sku": s, "bad_qty": q}
        return None

    def _is_valid_qty(self, val):
        try:
            return float(val) > 0
        except:
            return False
==================== END FILE: ui/components/transaction_wizard.py ====================


==================== START FILE: ui/components/safety.py ====================
# ui/components/safety.py

import streamlit as st
from typing import Optional, Tuple

from config import Config
from core.auth_service import AuthService
from core.context import get_current_user
from core.logging_config import get_logger


logger = get_logger(__name__)


def _verify_current_user_password(raw_password: str) -> bool:
    """
    æ ¡éªŒå½“å‰ç™»å½•ç”¨æˆ·å¯†ç æ˜¯å¦æ­£ç¡®ã€‚

    è¯´æ˜ï¼š
    - é€šè¿‡ AuthService å†åšä¸€æ¬¡è®¤è¯ï¼ˆä¸æ”¹å˜ç™»å½•çŠ¶æ€ï¼‰ã€‚
    - åªç”¨æ¥åœ¨æ‰§è¡Œé«˜å±æ“ä½œå‰åš "äºŒæ¬¡ç¡®è®¤"ã€‚
    """
    username = get_current_user()
    if not username:
        st.error("å½“å‰æœªç™»å½•ï¼Œæ— æ³•å®Œæˆå®‰å…¨æ ¡éªŒã€‚")
        return False

    ok, user_obj, msg = AuthService.authenticate(username, raw_password)
    if not ok:
        logger.warning(
            "Dangerous action password verify failed",
            extra={"user": username, "action": "DANGEROUS_VERIFY_FAIL"}
        )
    return ok


def _verify_operator_password(raw_password: str) -> bool:
    """
    ä½¿ç”¨ç‹¬ç«‹çš„è¿ç»´æ“ä½œå¯†ç è¿›è¡Œæ ¡éªŒï¼ˆé€‚ç”¨äº DB çº§åˆ«æ“ä½œï¼‰ã€‚

    å¯†ç åœ¨ç¯å¢ƒå˜é‡ / Config.DB_OPERATOR_PWD ä¸­é…ç½®ï¼š

        export DB_OPERATOR_PWD=xxxxxx
    """
    if not Config.DB_OPERATOR_PWD:
        st.error(" ç³»ç»Ÿæœªé…ç½® DB_OPERATOR_PWDï¼Œç¦æ­¢æ‰§è¡Œæ­¤ç±»é«˜å±æ“ä½œã€‚")
        return False

    return raw_password == Config.DB_OPERATOR_PWD


def confirm_dangerous_action(
    title: str,
    description: str,
    *,
    confirm_phrase: str = "CONFIRM",
    require_current_password: bool = True,
    require_operator_password: bool = False,
    key_prefix: str = "danger_confirm",
) -> Tuple[bool, Optional[str]]:
    """
    é«˜å±æ“ä½œç»Ÿä¸€ç¡®è®¤ç»„ä»¶ã€‚

    ä½¿ç”¨æ–¹å¼ï¼ˆåœ¨é¡µé¢é‡Œï¼‰ç¤ºä¾‹ï¼š
        ok, note = confirm_dangerous_action(
            title="âš ï¸ ä½ æ­£åœ¨æ‰§è¡Œæ•°æ®åº“è¿˜åŸ",
            description="è¯¥æ“ä½œä¼šç”¨å†å²å¤‡ä»½è¦†ç›–å½“å‰æ•°æ®åº“ï¼Œä¸”ä¸å¯æ’¤é”€ã€‚",
            confirm_phrase="RESTORE",
            require_current_password=True,
            require_operator_password=True,
            key_prefix="restore_db"
        )
        if ok:
            # æ‰§è¡ŒçœŸæ­£çš„å±é™©åŠ¨ä½œ
            do_restore()

    å‚æ•°ï¼š
        title:         ç¡®è®¤åŒºåŸŸæ ‡é¢˜
        description:   å¯¹æœ¬æ¬¡æ“ä½œçš„é£é™©è¯´æ˜ï¼ˆä¼šå±•ç¤ºç»™ç”¨æˆ·çœ‹ï¼‰
        confirm_phrase:è¦æ±‚ç”¨æˆ·æ‰‹å·¥è¾“å…¥çš„ç¡®è®¤çŸ­è¯­ï¼ˆå¤§å°å†™æ•æ„Ÿï¼‰
        require_current_password:
                       æ˜¯å¦è¦æ±‚è¾“å…¥å½“å‰ç™»å½•è´¦å·å¯†ç 
        require_operator_password:
                       æ˜¯å¦è¦æ±‚è¾“å…¥ç‹¬ç«‹çš„è¿ç»´æ“ä½œå¯†ç  DB_OPERATOR_PWD
        key_prefix:    ç”¨äºåŒºåˆ†ä¸åŒç¡®è®¤åŒºåŸŸï¼Œé¿å…ç»„ä»¶ key å†²çª

    è¿”å›ï¼š
        (confirmed, note)
        confirmed: True è¡¨ç¤ºæ‰€æœ‰æ ¡éªŒé€šè¿‡ï¼Œå¯ä»¥æ‰§è¡Œé«˜å±æ“ä½œï¼›
        note     : ç”¨æˆ·è¾“å…¥çš„å¤‡æ³¨ï¼ˆå¯å†™å…¥æ—¥å¿—ï¼‰ã€‚
    """
    username = get_current_user() or "-"
    st.markdown(f"### {title}")
    st.warning(description)

    with st.expander("ğŸ›¡ å®‰å…¨æ ¡éªŒ / Security Check", expanded=True):
        # å¤‡æ³¨è®°å½•
        note = st.text_area(
            "æ“ä½œå¤‡æ³¨ï¼ˆå¯é€‰ï¼‰ï¼š",
            help="å»ºè®®ç®€å•æè¿°æœ¬æ¬¡æ“ä½œåŸå› ï¼Œä¾‹å¦‚ï¼šå›æ»šåˆ° 11:30 å¤‡ä»½ä»¥ä¿®å¤é”™è¯¯å¯¼å…¥ã€‚",
            key=f"{key_prefix}_note",
        )

        # 1) å½“å‰ç”¨æˆ·å¯†ç ç¡®è®¤
        current_pwd_ok = True
        current_pwd_input = ""
        if require_current_password:
            current_pwd_input = st.text_input(
                "è¯·è¾“å…¥å½“å‰è´¦å·å¯†ç ä»¥ç¡®è®¤èº«ä»½ï¼š",
                type="password",
                key=f"{key_prefix}_current_pwd",
            )
            current_pwd_ok = False  # é»˜è®¤ Falseï¼Œç‚¹å‡»æŒ‰é’®æ—¶å†æ ¡éªŒ

        # 2) è¿ç»´æ“ä½œå¯†ç ï¼ˆç‹¬ç«‹äºç”¨æˆ·å¯†ç ï¼‰
        operator_pwd_ok = True
        operator_pwd_input = ""
        if require_operator_password:
            operator_pwd_input = st.text_input(
                "è¯·è¾“å…¥è¿ç»´æ“ä½œå¯†ç  (DB Operator Password)ï¼š",
                type="password",
                key=f"{key_prefix}_operator_pwd",
            )
            operator_pwd_ok = False

        # 3) äººå·¥è¾“å…¥ç¡®è®¤çŸ­è¯­
        phrase_input = st.text_input(
            f"è¯·åœ¨ä¸‹æ–¹è¾“å…¥ç¡®è®¤çŸ­è¯­ï¼š`{confirm_phrase}`ï¼Œä»¥ç»§ç»­æ“ä½œï¼š",
            key=f"{key_prefix}_phrase",
        )

        confirmed = False
        if st.button(" æˆ‘å·²çŸ¥æ™“é£é™©ï¼Œä»ç„¶è¦æ‰§è¡Œè¯¥æ“ä½œ", key=f"{key_prefix}_btn", use_container_width=True):
            # åˆ†æ­¥æ ¡éªŒ
            # 3.1 æ ¡éªŒå½“å‰ç™»å½•ç”¨æˆ·å¯†ç 
            if require_current_password:
                if not current_pwd_input:
                    st.error("è¯·å…ˆè¾“å…¥å½“å‰è´¦å·çš„å¯†ç ã€‚")
                    return False, None
                if not _verify_current_user_password(current_pwd_input):
                    st.error("å½“å‰è´¦å·å¯†ç é”™è¯¯ï¼Œå®‰å…¨æ ¡éªŒå¤±è´¥ã€‚")
                    return False, None
                current_pwd_ok = True

            # 3.2 æ ¡éªŒè¿ç»´æ“ä½œå¯†ç 
            if require_operator_password:
                if not operator_pwd_input:
                    st.error("è¯·å…ˆè¾“å…¥è¿ç»´æ“ä½œå¯†ç ã€‚")
                    return False, None
                if not _verify_operator_password(operator_pwd_input):
                    st.error("è¿ç»´æ“ä½œå¯†ç é”™è¯¯ï¼Œå®‰å…¨æ ¡éªŒå¤±è´¥ã€‚")
                    return False, None
                operator_pwd_ok = True

            # 3.3 æ ¡éªŒç¡®è®¤çŸ­è¯­
            if phrase_input != confirm_phrase:
                st.error(f"ç¡®è®¤çŸ­è¯­ä¸æ­£ç¡®ï¼Œè¯·è¾“å…¥ï¼š`{confirm_phrase}`ã€‚")
                return False, None

            # æ‰€æœ‰æ ¡éªŒé€šè¿‡
            confirmed = True
            st.success(" å®‰å…¨æ ¡éªŒé€šè¿‡ï¼Œå¯ä»¥æ‰§è¡Œåç»­æ“ä½œã€‚")

            logger.info(
                "Dangerous action confirmed",
                extra={
                    "user": username,
                    "action": "DANGEROUS_CONFIRMED",
                },
            )

        return confirmed, note if confirmed else None


==================== END FILE: ui/components/safety.py ====================


==================== START FILE: ui/pages/user_admin.py ====================
# ui/pages/user_admin.py

import pandas as pd
import streamlit as st
import time

from core.auth_service import AuthService
from core.logging_config import get_logger

logger = get_logger(__name__)

# æƒé™å®šä¹‰
PERMISSIONS_MAP = {
    "module.home": "ğŸ  è®¿é—®é¦–é¡µ (Home)",
    "module.etl": "ğŸ”Œ ä¸Šä¼ æ•°æ® (ETL)",
    "module.reports": "ğŸ“Š å•†ä¸šæ™ºèƒ½æŠ¥è¡¨ (Reports)",
    "module.db_modify": "ğŸ› ï¸ ä¿®æ”¹æ•°æ®åº“ (Modify Data)",
    "module.db_admin": "ğŸ—„ï¸ æ•°æ®åº“è¿ç»´ (DB Admin)",
    "module.user_admin": "ğŸ‘¤ ç”¨æˆ·ç®¡ç† (User Admin)",
    "module.audit_logs": "ğŸ“œ å®¡è®¡æ—¥å¿— (Audit Logs)"
}


def _ensure_admin():
    user = st.session_state.get("auth_user")
    if not user or not user.get("is_admin"):
        st.error(" æƒé™ä¸è¶³ (Admin Only)")
        st.stop()
    return user


def _load_user_list():
    df = AuthService.list_users()
    if df.empty: return df

    def get_status_icon(row):
        if row["is_locked"]: return "ğŸ”´ é”å®š"
        if row["is_admin"]: return "ğŸ›¡ï¸ ç®¡ç†å‘˜"
        return "ğŸŸ¢ ç”¨æˆ·"

    df["çŠ¶æ€"] = df.apply(get_status_icon, axis=1)
    return df


def render():
    current_admin = _ensure_admin()

    st.title("ğŸ‘¤ ç”¨æˆ·ä¸æƒé™ç®¡ç† (User Admin)")
    st.caption("åˆ›å»ºæ–°ç”¨æˆ·ã€é‡ç½®å¯†ç ã€é…ç½®åŠŸèƒ½è®¿é—®æƒé™åŠæŸ¥çœ‹ç™»å½•å®¡è®¡ã€‚")

    col_list, col_detail = st.columns([1.5, 2.5])

    # --- å·¦ä¾§åˆ—è¡¨ ---
    with col_list:
        st.subheader("ğŸ‘¥ ç”¨æˆ·åå½•")
        df_users = _load_user_list()

        if df_users.empty:
            st.info("æš‚æ— ç”¨æˆ·")
            selected_username = None
        else:
            st.dataframe(
                df_users[["username", "çŠ¶æ€", "failed_attempts"]],
                use_container_width=True,
                hide_index=True,
                column_config={
                    "username": "ç”¨æˆ·å",
                    "failed_attempts": st.column_config.NumberColumn("å¤±è´¥æ¬¡æ•°", help="è¿ç»­ç™»å½•å¤±è´¥æ¬¡æ•°")
                }
            )
            user_options = df_users["username"].tolist()
            selected_username = st.selectbox("ğŸ‘‰ é€‰æ‹©è¦ç®¡ç†çš„ç”¨æˆ·:", user_options, key="user_selector")

        st.markdown("---")
        with st.expander("â• æ³¨å†Œæ–°ç”¨æˆ·", expanded=False):
            with st.form("create_user_form"):
                new_u = st.text_input("ç”¨æˆ·å", placeholder="user_new")
                new_p = st.text_input("åˆå§‹å¯†ç ", type="password")
                new_is_admin = st.checkbox("è®¾ä¸ºç®¡ç†å‘˜ (Admin)", value=False)

                if st.form_submit_button("åˆ›å»ºç”¨æˆ·", type="primary", use_container_width=True):
                    if not new_u or not new_p:
                        st.error("è¯·å¡«å†™å®Œæ•´ä¿¡æ¯")
                    else:
                        ok, msg = AuthService.create_user(new_u, new_p, new_is_admin)
                        if ok:
                            st.success(f"ç”¨æˆ· {new_u} åˆ›å»ºæˆåŠŸï¼")
                            time.sleep(1)
                            st.rerun()
                        else:
                            st.error(msg)

    # --- å³ä¾§è¯¦æƒ… ---
    with col_detail:
        if not selected_username:
            st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§é€‰æ‹©ä¸€ä¸ªç”¨æˆ·è¿›è¡Œæ“ä½œã€‚")
            st.stop()

        user_info = df_users[df_users["username"] == selected_username].iloc[0]
        is_self = (selected_username == current_admin.get("username"))

        st.subheader(f"âš™ï¸ é…ç½®: {selected_username}")

        # 1. çŠ¶æ€å¡ç‰‡
        with st.container(border=True):
            c1, c2, c3 = st.columns(3)
            with c1:
                st.metric("è´¦å·è§’è‰²", "ç®¡ç†å‘˜" if user_info["is_admin"] else "æ™®é€šç”¨æˆ·")
            with c2:
                st.metric("è´¦å·çŠ¶æ€", "å·²é”å®š" if user_info["is_locked"] else "æ­£å¸¸",
                          delta="-é”å®š" if user_info["is_locked"] else "Active")
            with c3:
                st.metric("ç™»å½•å¤±è´¥", f"{user_info['failed_attempts']} æ¬¡")

            if not is_self:
                if user_info["is_locked"]:
                    if st.button("ğŸ”“ è§£é”è´¦å·", use_container_width=True):
                        AuthService.set_lock_state(selected_username, False)
                        st.success("è´¦å·å·²è§£é”ï¼")
                        time.sleep(0.5)
                        st.rerun()
                else:
                    if st.button("ğŸš« é”å®šè´¦å·", type="primary", use_container_width=True):
                        AuthService.set_lock_state(selected_username, True)
                        st.warning("è´¦å·å·²é”å®šï¼")
                        time.sleep(0.5)
                        st.rerun()

        # 2. [New] ç™»å½•è¡Œä¸ºç»Ÿè®¡å¡ç‰‡
        stats = AuthService.get_user_login_stats(selected_username)
        with st.container(border=True):
            st.markdown("#### ğŸ“Š ç™»å½•è¶³è¿¹ (Audit Trail)")
            c1, c2 = st.columns(2)
            c1.metric("æ€»ç™»å½•æ¬¡æ•°", stats["total_logins"])
            c2.metric("ä½¿ç”¨è¿‡çš„ IP æ•°", len(stats["ip_history"]))

            with st.expander("æŸ¥çœ‹ IP è®¿é—®è¯¦æƒ…", expanded=False):
                if not stats["ip_history"].empty:
                    st.dataframe(
                        stats["ip_history"],
                        use_container_width=True,
                        column_config={
                            "ip_address": "IP åœ°å€",
                            "count": st.column_config.NumberColumn("ç™»å½•æ¬¡æ•°", format="%d"),
                            "last_seen": st.column_config.DatetimeColumn("æœ€è¿‘ç™»å½•æ—¶é—´", format="YYYY-MM-DD HH:mm:ss")
                        }
                    )
                else:
                    st.caption("æš‚æ— ç™»å½•è®°å½•")

        # 3. å¯†ç é‡ç½®
        with st.expander("ğŸ”‘ é‡ç½®å¯†ç  (Password Reset)", expanded=False):
            if is_self:
                st.warning("å‡ºäºå®‰å…¨è€ƒè™‘ï¼Œè¯·åœ¨ä¸ªäººè®¾ç½®é¡µé¢ä¿®æ”¹å¯†ç ï¼ˆæœ¬ç³»ç»Ÿæš‚æœªå¼€æ”¾è‡ªä¿®æ”¹ï¼‰ã€‚")
            else:
                new_pwd_input = st.text_input("è¾“å…¥æ–°å¯†ç :", key="new_pwd_input", type="password")
                if st.button("ç¡®è®¤é‡ç½®å¯†ç ", type="secondary"):
                    if new_pwd_input:
                        ok, msg = AuthService.reset_password(selected_username, new_pwd_input)
                        if ok:
                            st.success("å¯†ç å·²é‡ç½®ï¼è¯·å¤åˆ¶ä¸‹æ–¹å¯†ç å‘ç»™ç”¨æˆ·ï¼š")
                            st.code(new_pwd_input, language="text")
                        else:
                            st.error(msg)
                    else:
                        st.error("å¯†ç ä¸èƒ½ä¸ºç©º")

        # 4. æƒé™è®¾ç½®
        if user_info["is_admin"]:
            st.info("ğŸ›¡ï¸ è¯¥ç”¨æˆ·æ˜¯ç®¡ç†å‘˜ï¼Œé»˜è®¤æ‹¥æœ‰æ‰€æœ‰ç³»ç»Ÿæƒé™ï¼Œæ— éœ€å•ç‹¬é…ç½®ã€‚")
        else:
            st.markdown("#### ğŸ”’ æƒé™è®¾ç½® (Permissions)")
            current_perms = AuthService.get_permissions(selected_username)
            with st.form("perm_form"):
                st.write("è¯·å‹¾é€‰å…è®¸è®¿é—®çš„æ¨¡å—ï¼š")
                new_perms = {}
                cols = st.columns(2)
                for i, (key, label) in enumerate(PERMISSIONS_MAP.items()):
                    is_sensitive = "admin" in key or "modify" in key
                    default_val = current_perms.get(key, False)
                    with cols[i % 2]:
                        new_perms[key] = st.checkbox(label, value=default_val, key=f"perm_{key}")
                        if is_sensitive: st.caption("âš ï¸ æ•æ„Ÿæƒé™")

                st.markdown("---")
                if st.form_submit_button("ğŸ’¾ ä¿å­˜æƒé™è®¾ç½®", type="primary", use_container_width=True):
                    AuthService.set_permissions(selected_username, new_perms)
                    st.success(f"ç”¨æˆ· {selected_username} çš„æƒé™å·²æ›´æ–°ï¼")
                    time.sleep(1)
                    st.rerun()

==================== END FILE: ui/pages/user_admin.py ====================


==================== START FILE: ui/pages/home.py ====================
# ui/pages/home.py

import streamlit as st
import base64
import re
import streamlit.components.v1 as components
from config import Config


def get_base64_encoded_image(image_path):
    try:
        with open(image_path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode('utf-8')
    except FileNotFoundError:
        return None


def _parse_text_to_html(raw_text):
    """
    [æ™ºèƒ½è§£æå™¨ V2.2]
    """
    raw_text = raw_text.strip()

    # å¦‚æœå·²ç»æ˜¯ HTMLï¼Œç›´æ¥è¿”å›
    if re.search(r"<[a-z/]+>", raw_text):
        return raw_text

    lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
    if not lines:
        return ""

    html_parts = []

    # 1. æ ‡é¢˜
    title = lines[0].replace("<b>", "").replace("</b>", "")
    html_parts.append(f"<b>{title}</b>")

    # 2. åˆ—è¡¨
    rest_lines = lines[1:]
    if rest_lines:
        html_parts.append("<ul>")
        for line in rest_lines:
            content = line.lstrip("-").lstrip("*").strip()
            if "ï¼š" in content:
                parts = content.split("ï¼š", 1)
                content = f"<b>{parts[0]}ï¼š</b>{parts[1]}"
            elif ":" in content:
                if "http" not in content[:4]:
                    parts = content.split(":", 1)
                    content = f"<b>{parts[0]}:</b>{parts[1]}"

            html_parts.append(f"<li>{content}</li>")
        html_parts.append("</ul>")

    return "".join(html_parts)


def render():
    # 1. Logo
    logo_path = Config.ASSETS_DIR / "Logo.png"
    logo_b64 = get_base64_encoded_image(logo_path)

    # 2. CSS (ä¿æŒæ ·å¼ä¸å˜)
    st.markdown("""
        <style>
        .welcome-wrapper {
            display: flex;
            flex-direction: column; align-items: center; justify-content: center;
            height: 70vh; text-align: center; animation: fadeIn 1.2s ease-in-out;
        }
        .big-logo {
            width: 80%;
            max-width: 380px; height: auto; margin-bottom: 30px;
            filter: drop-shadow(0 0 30px rgba(78, 201, 176, 0.15)); transition: transform 0.5s;
        }
        .big-logo:hover { transform: scale(1.02); }
        .welcome-title {
            font-size: 36px; font-weight: 200; color: #FFFFFF; letter-spacing: 4px;
            margin-bottom: 10px; text-shadow: 0 4px 10px rgba(0,0,0,0.5);
        }
        .welcome-subtitle {
            font-size: 16px; color: rgba(255, 255, 255, 0.5); font-family: 'Consolas', monospace;
            background: rgba(0,0,0,0.3); padding: 8px 20px; border-radius: 20px;
            border: 1px solid rgba(255,255,255,0.05);
        }
        .patch-panel {
            background-color: rgba(25, 25, 25, 0.6);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-top: 3px solid #4EC9B0; border-radius: 6px; 
            height: 65vh; overflow-y: scroll; padding: 0; margin-top: 2vh;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3); backdrop-filter: blur(10px);
            scrollbar-width: none; -ms-overflow-style: none;
        }
        .patch-panel::-webkit-scrollbar { display: none; }
        .patch-header {
            position: sticky; top: 0; background-color: rgba(30, 30, 30, 0.95);
            padding: 15px 20px; border-bottom: 1px solid rgba(255,255,255,0.1);
            font-size: 16px; font-weight: 700; color: #4EC9B0;
            display: flex; justify-content: space-between; align-items: center; z-index: 10;
        }
        .patch-content { padding: 20px; }
        .patch-item {
            margin-bottom: 25px; position: relative; padding-left: 20px;
            border-left: 2px solid rgba(255,255,255,0.1);
        }
        .patch-item::before {
            content: ''; position: absolute; left: -6px; top: 0;
            width: 10px; height: 10px; background: #333; border: 2px solid #4EC9B0; border-radius: 50%;
        }
        .patch-ver {
            display: block; font-size: 15px; font-weight: bold; color: #E0E0E0; margin-bottom: 4px;
        }
        .patch-date { font-size: 12px; color: #666; margin-left: 10px; font-weight: normal; }
        .patch-desc { font-size: 13px; color: #B0B0B0; line-height: 1.6; }
        .patch-desc ul { padding-left: 18px; margin: 5px 0; }
        .patch-desc li { margin-bottom: 4px; }
        .patch-desc b { color: #DCDCDC; font-weight: 700; }
        @keyframes fadeIn { from { opacity: 0; transform: translateY(10px); } to { opacity: 1; transform: translateY(0); } }
        </style>
    """, unsafe_allow_html=True)

    # 3. å¸ƒå±€
    col_left, col_right = st.columns([1.4, 1])

    with col_left:
        img_tag = f'<img src="data:image/png;base64,{logo_b64}" class="big-logo">' if logo_b64 else "<h1>ğŸ¦…</h1>"
        st.markdown(
            f"""<div class="welcome-wrapper">{img_tag}<div class="welcome-title">Eaglestar Pro</div><div class="welcome-subtitle">â¬…ï¸ è¯·åœ¨å·¦ä¾§å¯¼èˆªæ é€‰æ‹©åŠŸèƒ½æ¨¡å—</div></div>""",
            unsafe_allow_html=True)

    with col_right:
        latest_ver = Config.APP_VERSION
        patch_list = Config.PATCH_NOTES_LIST

        # å¼ºåˆ¶æ’åºï¼šæ—¥æœŸä»å°åˆ°å¤§ (Old -> New)
        if patch_list:
            try:
                patch_list = sorted(patch_list, key=lambda x: x['date'])
            except:
                pass

        items_html = ""
        if not patch_list:
            items_html = "<div style='padding:20px; color:#666;'>æš‚æ— æ›´æ–°è®°å½•</div>"
        else:
            for note in patch_list:
                formatted_desc = _parse_text_to_html(note['desc'])
                # [Fix] ç´§å‡‘æ‹¼æ¥ï¼Œä¸ç•™æ¢è¡Œç¬¦ï¼Œé˜²æ­¢ Markdown è¯¯åˆ¤ä»£ç å—
                items_html += f'<div class="patch-item"><span class="patch-ver">{note["ver"]} <span class="patch-date">{note["date"]}</span></span><div class="patch-desc">{formatted_desc}</div></div>'

        double_content_html = items_html + items_html if patch_list else items_html

        # [Fix] ç´§å‡‘æ‹¼æ¥å®¹å™¨ HTML
        final_html = f'<div class="patch-panel" id="auto-scroll-panel"><div class="patch-header"><span>ğŸ“… æ›´æ–°æ—¥å¿— (Changelog)</span><span style="font-size:12px; opacity:0.6;">Current: {latest_ver}</span></div><div class="patch-content">{double_content_html}</div></div>'

        st.markdown(final_html, unsafe_allow_html=True)

    # 4. JS æ»šåŠ¨é€»è¾‘
    js_code = """
    <script>
        function setupAutoScroll() {
            const panel = window.parent.document.getElementById('auto-scroll-panel');
            if (!panel) { setTimeout(setupAutoScroll, 100); return; }

            let isPaused = false;
            let resumeTimeout = null;
            const speed = 0.3; 
            let scrollAccumulator = panel.scrollTop;

            function step() {
                if (!isPaused) {
                    scrollAccumulator += speed;
                    panel.scrollTop = scrollAccumulator;
                    if (panel.scrollTop >= (panel.scrollHeight / 2)) {
                        scrollAccumulator = 0;
                        panel.scrollTop = 0;
                    }
                } else {
                    scrollAccumulator = panel.scrollTop;
                }
                requestAnimationFrame(step);
            }

            const pause = () => { isPaused = true; if (resumeTimeout) clearTimeout(resumeTimeout); };
            const resume = () => {
                if (resumeTimeout) clearTimeout(resumeTimeout);
                resumeTimeout = setTimeout(() => { isPaused = false; }, 1000);
            };

            panel.addEventListener('mouseenter', pause);
            panel.addEventListener('mouseleave', resume);
            panel.addEventListener('wheel', () => { pause(); resume(); });
            panel.addEventListener('touchstart', pause);
            panel.addEventListener('touchend', resume);

            requestAnimationFrame(step);
        }
        setupAutoScroll();
    </script>
    """
    components.html(js_code, height=0)

==================== END FILE: ui/pages/home.py ====================


==================== START FILE: ui/pages/etl_ingest.py ====================
# ui/pages/etl_ingest.py

import streamlit as st
from config import Config

# å¼•å…¥ UI ç»„ä»¶
from ui.components.inventory_wizard import InventoryWizard
from ui.components.transaction_wizard import TransactionWizard


def render():
    st.title("ğŸ”Œ ä¸Šä¼ æ•°æ® (ETL)")

    # åŠ¨æ€ç‰ˆæœ¬å·
    st.caption(f"Enterprise Data Integration Pipeline {Config.APP_VERSION}")

    st.markdown("---")

    tab1, tab2 = st.tabs(["ä¸Šä¼ æ•°æ® (Data)", "ä¸Šä¼ åº“å­˜ (Inventory)"])

    # ç»„ä»¶åŒ–è°ƒç”¨
    with tab1:
        wizard_trans = TransactionWizard()
        wizard_trans.render()

    with tab2:
        wizard_inv = InventoryWizard()
        wizard_inv.render()

==================== END FILE: ui/pages/etl_ingest.py ====================


==================== START FILE: ui/pages/reports.py ====================
# ui/pages/reports.py

import os
import datetime
import streamlit as st
from typing import List, Tuple, Type
from pathlib import Path
import traceback

# -----------------------------------------------------------------------------
# 1. æ ¸å¿ƒä¾èµ–
# -----------------------------------------------------------------------------
from config import Config
from ui.utils import task_monitor, parse_compound_csv  # [Fix] å¼•å…¥æ–°è§£æå™¨

# å¼•å…¥æŠ¥è¡¨æ–‡ä»¶ç®¡ç†å™¨
from core.services.report_manager import ReportFileManager

# å¼•å…¥æ‰€æœ‰ä¸šåŠ¡åˆ†ææœåŠ¡
from core.services.sales_analyzer import SalesQtyAnalyzer
from core.services.profit_sku import SkuProfitAnalyzer
from core.services.profit_listing import ListingProfitAnalyzer
from core.services.profit_combo import ComboProfitAnalyzer
from core.services.crm import CustomerAnalyzer
from core.services.logistics import ShippingAnalyzer
from core.services.prediction import PredictionService
from core.services.ordering import OrderingService
from core.services.inventory_snapshot import InventorySnapshot


class ReportPage:
    """
    [UIå±‚] å•†ä¸šæ™ºèƒ½åˆ†ææŠ¥å‘Šé¡µé¢ (BI Reports) - Enterprise V2.2

    å‡çº§ç‰¹æ€§:
    - [Fix] å¢åŠ  UI çŠ¶æ€é”ï¼Œé˜²æ­¢æŒ‰é’®åœ¨è¿è¡Œä¸­è¢«é‡å¤ç‚¹å‡»ã€‚
    - é›†æˆåœ¨çº¿æŠ¥è¡¨é¢„è§ˆä¸­å¿ƒ (Report Center)ã€‚
    - æ”¯æŒå¤šè¡¨å †å  CSV çš„æ™ºèƒ½è§£æã€‚
    """

    def __init__(self):
        self.report_manager = ReportFileManager()
        self._init_state()

        # å®šä¹‰åˆ†æä»»åŠ¡æ¸…å• (é¡ºåºæ‰§è¡Œ)
        self.TASK_REGISTRY = [
            ("ğŸ“¦ SKU é”€é‡ç»Ÿè®¡ (Sales Analysis)", SalesQtyAnalyzer),
            ("ğŸ’° SKU åˆ©æ¶¦ä¸è¯Šæ–­ (SKU Profit)", SkuProfitAnalyzer),
            ("ğŸ”— Listing è¡¨ç°åˆ†æ (Listing Profit)", ListingProfitAnalyzer),
            ("ğŸ Combo ç­–ç•¥åˆ†æ (Combo Profit)", ComboProfitAnalyzer),
            ("ğŸ‘¥ å®¢æˆ·ç”»åƒä¸é£é™© (CRM Analysis)", CustomerAnalyzer),
            ("ğŸšš ç‰©æµæ•ˆç›Šè¯Šæ–­ (Logistics)", ShippingAnalyzer),
            ("ğŸ¤– AI é”€é‡é¢„æµ‹ (AI Prediction)", PredictionService),
            ("ğŸ›’ æ™ºèƒ½è¡¥è´§è®¡ç®— (Smart Ordering)", OrderingService),
            ("ğŸ¦ åº“å­˜èµ„äº§å¿«ç…§ (Asset Snapshot)", InventorySnapshot)
        ]

    def _init_state(self):
        """åˆå§‹åŒ– UI çŠ¶æ€é”"""
        if 'report_processing' not in st.session_state:
            st.session_state.report_processing = False

    def _lock_ui(self):
        """[Callback] ç‚¹å‡»å³é”å®š"""
        st.session_state.report_processing = True

    # ... [é…ç½®é¢æ¿ä»£ç ä¿æŒä¸å˜] ...
    def _render_config_panel(self) -> dict:
        st.markdown("#### âš™ï¸ ä¸šåŠ¡å‚æ•°é…ç½® (Business Parameters)")
        st.caption("æç¤º: è°ƒæ•´ä»¥ä¸‹å‚æ•°å°†ç›´æ¥å½±å“ **å‡€åˆ©æ¶¦** å’Œ **å»ºè®®è¡¥è´§é‡** çš„è®¡ç®—ç»“æœã€‚")

        with st.expander("å±•å¼€è¯¦ç»†å‚æ•°è®¾ç½®", expanded=True):
            col_loss, col_supply = st.columns([1.2, 1])

            with col_loss:
                st.markdown("**ğŸ“‰ è€—æŸç‡æ ‡å‡† (Loss Rates)**")
                lr_case = st.slider("æŠ•è¯‰ (Case) è€—æŸç‡", 0.0, 1.0, Config.LOSS_RATES.get('CASE', 0.6), 0.01)
                lr_req = st.slider("ç”³è¯· (Request) è€—æŸç‡", 0.0, 1.0, Config.LOSS_RATES.get('REQUEST', 0.5), 0.01)
                lr_ret = st.slider("é€€è´§ (Return) è€—æŸç‡", 0.0, 1.0, Config.LOSS_RATES.get('RETURN', 0.3), 0.01)
                lr_disp = st.slider("äº‰è®® (Dispute) è€—æŸç‡", 0.0, 1.0, Config.LOSS_RATES.get('DISPUTE', 1.0), 0.01)

            with col_supply:
                st.markdown("**ğŸ“¦ ä¾›åº”é“¾å‚æ•°**")
                lead_time = st.number_input("è®¢è´§æå‰æœŸ (Lead Month)", min_value=0.0, value=float(Config.LEAD_MONTH),
                                            step=0.1)
                safety_month = st.number_input("æœ€ä½å®‰å…¨åº“å­˜ (Safety Month)", min_value=0.0,
                                               value=float(Config.MIN_SAFETY_MONTH), step=0.1)

        return {
            "loss_rates": {"CASE": lr_case, "REQUEST": lr_req, "RETURN": lr_ret, "DISPUTE": lr_disp},
            "lead_time": lead_time,
            "safety_month": safety_month
        }

    # ... [ç®¡é“é€»è¾‘ä¿æŒä¸å˜] ...
    def _run_pipeline(self, start_date: datetime.date, end_date: datetime.date):
        suffix = f"{start_date.strftime('%Y_%m_%d')}->{end_date.strftime('%Y_%m_%d')}"
        total_steps = len(self.TASK_REGISTRY)

        self.report_manager.clear_old_reports()

        with task_monitor(f"ğŸš€ å…¨é‡åˆ†æå¼•æ“å¯åŠ¨ ({start_date} è‡³ {end_date})") as monitor:
            for idx, (task_name, ServiceClass) in enumerate(self.TASK_REGISTRY):
                step_index = idx + 1
                progress_pct = idx / total_steps
                monitor.update_main_progress(progress_pct)
                monitor.log(f"\n>>> [Step {step_index}/{total_steps}] æ­£åœ¨æ‰§è¡Œ: {task_name}...")

                try:
                    service_instance = ServiceClass(start_date, end_date, suffix)
                    service_instance.run()
                    monitor.log(f" {task_name} æ‰§è¡ŒæˆåŠŸã€‚")
                except Exception as e:
                    monitor.log(f" {task_name} å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
                    monitor.log(traceback.format_exc())

            monitor.update_main_progress(1.0)
            st.session_state.analysis_finished = True

    def _render_report_center(self):
        """[Tab] æŠ¥è¡¨ä¸­å¿ƒï¼šåœ¨çº¿é¢„è§ˆä¸ç®¡ç†"""
        st.info("ğŸ’¡ è¿™é‡Œå¯ä»¥ç›´æ¥é¢„è§ˆç”Ÿæˆçš„æŠ¥è¡¨ï¼Œæ— éœ€ä¸‹è½½ã€‚æ”¯æŒå¤æ‚çš„å¤šè¡¨æŠ¥è¡¨è§£æã€‚")

        files = self.report_manager.get_generated_files()
        if not files:
            st.warning("æš‚æ— æŠ¥è¡¨æ–‡ä»¶ï¼Œè¯·å…ˆåœ¨ [ç”ŸæˆæŠ¥è¡¨] é¡µé¢è¿è¡Œåˆ†æã€‚")
            if st.button("ğŸ”„ åˆ·æ–°åˆ—è¡¨"): st.rerun()
            return

        col_list, col_view = st.columns([1, 3])

        with col_list:
            st.markdown("### ğŸ“‘ æ–‡ä»¶åˆ—è¡¨")
            selected_file = st.radio("é€‰æ‹©æ–‡ä»¶é¢„è§ˆ:", files, label_visibility="collapsed")

            st.markdown("---")
            # æ‰¹é‡ä¸‹è½½/åˆ é™¤
            zip_bytes = self.report_manager.create_zip_archive()
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')
            st.download_button(
                "ğŸ“¦ æ‰“åŒ…ä¸‹è½½æ‰€æœ‰ (.zip)",
                data=zip_bytes,
                file_name=f"Reports_{timestamp}.zip",
                mime="application/zip",
                use_container_width=True
            )

            # åˆ é™¤æŒ‰é’®ä¹ŸåŠ ä¸Šé˜²è¯¯è§¦
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æŠ¥è¡¨", type="secondary", use_container_width=True):
                self.report_manager.clear_old_reports()
                st.toast("å·²æ¸…ç©º")
                time.sleep(0.5)
                st.rerun()

        with col_view:
            if selected_file:
                file_path = Path(self.report_manager.get_file_path(selected_file))

                st.markdown(f"### ğŸ“„ é¢„è§ˆ: `{selected_file}`")
                with open(file_path, "rb") as fp:
                    st.download_button("ğŸ“¥ ä¸‹è½½æ­¤æ–‡ä»¶", fp, file_name=selected_file, mime="text/csv")

                st.divider()

                # [æ ¸å¿ƒè°ƒç”¨] ä½¿ç”¨æ™ºèƒ½è§£æå™¨
                tables = parse_compound_csv(file_path)

                if not tables:
                    st.error("æ— æ³•è§£ææ–‡ä»¶å†…å®¹æˆ–æ–‡ä»¶ä¸ºç©ºã€‚")
                else:
                    for title, df in tables:
                        with st.expander(f"ğŸ“Š {title} ({len(df)} è¡Œ)", expanded=True):
                            st.dataframe(df, use_container_width=True)

    def render(self):
        st.title("ğŸ“Š å•†ä¸šæ™ºèƒ½æŠ¥è¡¨ (BI Reports)")
        st.caption(f"Enterprise Analytics Engine {Config.APP_VERSION}")

        tab_gen, tab_view = st.tabs(["ğŸš€ ç”ŸæˆæŠ¥è¡¨ (Generator)", "ğŸ“‚ æŠ¥è¡¨ä¸­å¿ƒ (Report Center)"])

        # --- Tab 1: ç”Ÿæˆ ---
        with tab_gen:
            with st.container(border=True):
                st.markdown("#### ğŸ“… åˆ†æå‘¨æœŸé€‰æ‹©")
                c1, c2 = st.columns(2)
                with c1:
                    today = datetime.date.today()
                    first_day_curr = today.replace(day=1)
                    last_day_prev = first_day_curr - datetime.timedelta(days=1)
                    first_day_prev = last_day_prev.replace(day=1)
                    d_start = st.date_input("å¼€å§‹æ—¥æœŸ (Start Date)", first_day_prev)
                with c2:
                    d_end = st.date_input("ç»“æŸæ—¥æœŸ (End Date)", last_day_prev)

            with st.container(border=True):
                params = self._render_config_panel()

            st.markdown("---")

            # å¯åŠ¨é€»è¾‘ (å«çŠ¶æ€é”)
            is_running = st.session_state.report_processing

            # æ ¡éªŒæ—¥æœŸ
            date_error = (d_start > d_end)
            if date_error:
                st.error("âš ï¸ å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸï¼")

            # æŒ‰é’®çŠ¶æ€æ§åˆ¶
            btn_label = "â³ æ­£åœ¨åˆ†æä¸­..." if is_running else "ğŸš€ ç¡®è®¤å‚æ•°å¹¶å¯åŠ¨åˆ†æ"
            btn_disabled = is_running or date_error

            st.button(
                btn_label,
                type="primary",
                use_container_width=True,
                disabled=btn_disabled,
                on_click=self._lock_ui  # ç‚¹å‡»å³é”å®š
            )

            # [UI Lock Block] å¦‚æœçŠ¶æ€ä¸º Trueï¼Œæ‰§è¡Œé€»è¾‘
            if is_running and not date_error:
                try:
                    Config.update_params(
                        lead_time=params["lead_time"],
                        safety_month=params["safety_month"],
                        loss_rates=params["loss_rates"]
                    )
                    st.toast(" å‚æ•°å·²æ›´æ–°ï¼Œå¼•æ“å¯åŠ¨ä¸­...")
                    st.session_state.analysis_finished = False

                    self._run_pipeline(d_start, d_end)

                    st.balloons()
                    st.success("ğŸ‰ æ‰€æœ‰åˆ†æä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼è¯·å‰å¾€ [æŠ¥è¡¨ä¸­å¿ƒ] æŸ¥çœ‹æˆ–ä¸‹è½½ç»“æœã€‚")

                except Exception as e:
                    st.error(f"æ‰§è¡Œå¤±è´¥: {e}")
                    st.code(traceback.format_exc())

                finally:
                    # [Critical] é‡Šæ”¾é”å¹¶åˆ·æ–°é¡µé¢
                    st.session_state.report_processing = False
                    # ç¨å¾®å»¶è¿Ÿä¸€ä¸‹è®©ç”¨æˆ·çœ‹åˆ°æˆåŠŸçš„ balloonï¼Œç„¶åå† rerun æ¢å¤æŒ‰é’®çŠ¶æ€
                    time.sleep(2)
                    st.rerun()

        # --- Tab 2: æŸ¥çœ‹ ---
        with tab_view:
            self._render_report_center()


def render():
    ReportPage().render()
==================== END FILE: ui/pages/reports.py ====================


==================== START FILE: ui/pages/db_data_change.py ====================
# ui/pages/db_data_change.py

import streamlit as st
import pandas as pd
import time
from config import Config
from core.services.data_manager import DataManager
from ui.components.safety import confirm_dangerous_action


def render():
    st.title("ğŸ› ï¸ æ•°æ®ä¿®æ”¹ä¸­å¿ƒ (Data Modification)")
    st.caption("å…è®¸æˆæƒç”¨æˆ·æ‰‹åŠ¨ä¿®æ­£åº“å­˜æ•°æ®æˆ–ç»´æŠ¤ SKU åŸºç¡€èµ„æ–™ã€‚")

    # åˆå§‹åŒ– Service
    mgr = DataManager()

    tab_inv, tab_cogs = st.tabs(["ğŸ“¦ ä¿®æ”¹åº“å­˜ (Inventory)", "ğŸ“ èµ„æ–™ç»´æŠ¤ (Master Data)"])

    # =========================================================================
    # TAB 1: ä¿®æ”¹åº“å­˜ (Inventory) - åˆ†æ­¥å‘å¯¼æ¨¡å¼
    # =========================================================================
    with tab_inv:
        # åˆå§‹åŒ– Session State
        if 'inv_mod_step' not in st.session_state: st.session_state.inv_mod_step = 1
        if 'inv_mod_date' not in st.session_state: st.session_state.inv_mod_date = None
        if 'inv_mod_sku' not in st.session_state: st.session_state.inv_mod_sku = None

        # --- Step 1: é€‰æ‹©æœˆä»½ ---
        if st.session_state.inv_mod_step == 1:
            st.info("Step 1/3: è¯·é€‰æ‹©è¦ä¿®æ”¹çš„æœˆä»½åˆ—")
            cols = mgr.get_inventory_columns()

            if not cols:
                st.error("âš ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰æœ‰æ•ˆçš„æœˆä»½åˆ—ï¼Œè¯·å…ˆä¸Šä¼ åº“å­˜æ–‡ä»¶ã€‚")
            else:
                sel_date = st.selectbox("é€‰æ‹©æ—¥æœŸåˆ—:", cols)
                if st.button("ä¸‹ä¸€æ­¥ â¡ï¸", key="btn_step1"):
                    st.session_state.inv_mod_date = sel_date
                    st.session_state.inv_mod_step = 2
                    st.rerun()

        # --- Step 2: é€‰æ‹© SKU ---
        elif st.session_state.inv_mod_step == 2:
            st.success(f"å½“å‰é€‰ä¸­æœˆä»½: **{st.session_state.inv_mod_date}**")
            st.info("Step 2/3: è¯·é€‰æ‹©è¦ä¿®æ”¹çš„ SKU")

            all_skus = mgr.get_all_skus()
            sel_sku = st.selectbox("é€‰æ‹© SKU (æ”¯æŒæœç´¢):", all_skus)

            c_back, c_next = st.columns([1, 5])
            with c_back:
                if st.button("â¬…ï¸ è¿”å›"):
                    st.session_state.inv_mod_step = 1
                    st.rerun()
            with c_next:
                if st.button("ä¸‹ä¸€æ­¥ â¡ï¸", key="btn_step2"):
                    st.session_state.inv_mod_sku = sel_sku
                    st.session_state.inv_mod_step = 3
                    st.rerun()

        # --- Step 3: ä¿®æ”¹æ•°å€¼å¹¶æäº¤ ---
        elif st.session_state.inv_mod_step == 3:
            d = st.session_state.inv_mod_date
            s = st.session_state.inv_mod_sku

            st.markdown(f"#### ğŸ¯ ä¿®æ”¹ç›®æ ‡: `{s}` @ `{d}`")

            # è·å–å½“å‰å€¼
            curr_val = mgr.get_inventory_value(d, s)

            with st.container(border=True):
                c1, c2 = st.columns(2)
                with c1:
                    st.metric("å½“å‰åº“å­˜", curr_val)
                with c2:
                    new_q = st.number_input("æ–°åº“å­˜æ•°é‡", min_value=0, value=curr_val, step=1)

                st.markdown("---")

                # [Security Upgrade] ä½¿ç”¨ç»Ÿä¸€å®‰å…¨ç»„ä»¶
                confirmed, note = confirm_dangerous_action(
                    title="ğŸ”’ å®‰å…¨ç¡®è®¤ (Security Check)",
                    description=f"æ‚¨å³å°†ä¿®æ”¹å†å²åº“å­˜æ•°æ®ã€‚\nSKU: **{s}** | Month: **{d}** | Change: **{curr_val} -> {new_q}**",
                    confirm_phrase="UPDATE",
                    require_current_password=True,
                    require_operator_password=True,
                    key_prefix="inv_mod_confirm"
                )

                if confirmed:
                    ok, msg = mgr.update_inventory_qty(d, s, int(new_q))
                    if ok:
                        st.balloons()
                        st.success(msg)
                        time.sleep(2)
                        # è¿”å›ç¬¬ä¸€æ­¥
                        st.session_state.inv_mod_step = 1
                        st.rerun()
                    else:
                        st.error(msg)

            if st.button("â¬…ï¸ å–æ¶ˆä¿®æ”¹", use_container_width=True):
                st.session_state.inv_mod_step = 2
                st.rerun()

    # =========================================================================
    # TAB 2: èµ„æ–™ç»´æŠ¤ (COGS) - æ‰¹é‡ç¼–è¾‘ & æ–°å¢
    # =========================================================================
    with tab_cogs:
        mode = st.radio("æ“ä½œæ¨¡å¼:", ["ğŸ“ æ‰¹é‡ç¼–è¾‘ (Batch Edit)", "â• æ–°å¢ SKU (Add New)"], horizontal=True)
        st.divider()

        # å‡†å¤‡ä¸‹æ‹‰é€‰é¡¹
        opt_cats = [""] + mgr.get_distinct_values('Category')
        opt_subs = [""] + mgr.get_distinct_values('SubCategory')
        opt_types = [""] + mgr.get_distinct_values('Type')

        # --- Mode A: æ‰¹é‡ç¼–è¾‘ ---
        if mode == "ğŸ“ æ‰¹é‡ç¼–è¾‘ (Batch Edit)":
            st.info("ğŸ’¡ è¯´æ˜: å¯ç›´æ¥åœ¨ä¸‹æ–¹è¡¨æ ¼ä¸­ä¿®æ”¹ Cost æˆ– Freightï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è®¡ç®— Cogã€‚")

            # åŠ è½½æ•°æ®
            if 'df_cogs_edit' not in st.session_state:
                st.session_state.df_cogs_edit = mgr.get_cogs_data()

            # ç¼–è¾‘å™¨
            edited_df = st.data_editor(
                st.session_state.df_cogs_edit,
                key="cogs_editor",
                use_container_width=True,
                height=500,
                column_config={
                    "SKU": st.column_config.TextColumn("SKU", disabled=True),
                    "Cog": st.column_config.NumberColumn("Cog (Auto)", disabled=True, format="%.2f"),
                    "Cost": st.column_config.NumberColumn("Cost", min_value=0, format="%.2f", required=True),
                    "Freight": st.column_config.NumberColumn("Freight", min_value=0, format="%.2f", required=True),
                    "Category": st.column_config.SelectboxColumn("Category", options=opt_cats),
                    "SubCategory": st.column_config.SelectboxColumn("SubCategory", options=opt_subs),
                    "Type": st.column_config.SelectboxColumn("Type", options=opt_types),
                },
                hide_index=True
            )

            # å³æ—¶è®¡ç®—
            recalc_cog = (pd.to_numeric(edited_df['Cost'], errors='coerce').fillna(0) +
                          pd.to_numeric(edited_df['Freight'], errors='coerce').fillna(0)).round(2)

            if not edited_df['Cog'].equals(recalc_cog):
                edited_df['Cog'] = recalc_cog
                st.session_state.df_cogs_edit = edited_df
                st.rerun()

            # [Security Upgrade] æ‰¹é‡æ›´æ–°ç¡®è®¤
            st.markdown("### ğŸ’¾ æäº¤æ›´æ”¹")

            confirmed, note = confirm_dangerous_action(
                title="ğŸ”’ åŒæ­¥ç¡®è®¤",
                description="æ‚¨æ­£åœ¨æ‰¹é‡ä¿®æ”¹ SKU åŸºç¡€èµ„æ–™ (Master Data)ã€‚\nè¯·ç¡®ä¿å·²æ ¸å¯¹æ‰€æœ‰æˆæœ¬å˜æ›´ã€‚",
                confirm_phrase="SYNC",
                require_current_password=True,
                require_operator_password=True,
                key_prefix="cogs_batch_confirm"
            )

            if confirmed:
                with st.spinner("æ­£åœ¨åŒæ­¥..."):
                    ok, msg = mgr.update_cogs_batch(edited_df)
                if ok:
                    st.success(msg)
                    st.balloons()
                    # å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
                    st.session_state.df_cogs_edit = mgr.get_cogs_data()
                    time.sleep(1.5)
                    st.rerun()
                else:
                    st.error(msg)

        # --- Mode B: æ–°å¢ SKU ---
        else:
            st.markdown("#### ğŸ†• å½•å…¥æ–°äº§å“")
            st.warning("âš ï¸ æ³¨æ„: æ–°å¢ SKU ä¼šè‡ªåŠ¨åŒæ­¥åˆ°åº“å­˜è¡¨ï¼Œæ‰€æœ‰å†å²æœˆä»½åº“å­˜é»˜è®¤ä¸º 0ã€‚")

            with st.container(border=True):
                c1, c2, c3 = st.columns(3)
                with c1:
                    n_sku = st.text_input("SKU (å¿…å¡«, å”¯ä¸€)", placeholder="ä¾‹å¦‚: AA-1234").strip()
                with c2:
                    n_cat = st.selectbox("Category", opt_cats, key="new_cat")
                with c3:
                    n_sub = st.selectbox("SubCategory", opt_subs, key="new_sub")

                c4, c5, c6 = st.columns(3)
                with c4:
                    n_type = st.selectbox("Type", opt_types, key="new_type")
                with c5:
                    n_cost = st.number_input("Cost", min_value=0.0, step=0.01, format="%.2f")
                with c6:
                    n_frt = st.number_input("Freight", min_value=0.0, step=0.01, format="%.2f")

                st.info(f"ğŸ’° é¢„è®¡ Cog: **{(n_cost + n_frt):.2f}**")
                st.markdown("---")

                # [Security Upgrade] æ–°å¢ SKU ç¡®è®¤
                confirmed, note = confirm_dangerous_action(
                    title="ğŸ”’ åˆ›å»ºç¡®è®¤",
                    description=f"å³å°†åˆ›å»ºæ–° SKU: **{n_sku}**ã€‚\næ­¤æ“ä½œå°†å½±å“åº“å­˜è¡¨ç»“æ„ã€‚",
                    confirm_phrase="CREATE",
                    require_current_password=True,
                    require_operator_password=True,
                    key_prefix="sku_create_confirm"
                )

                if confirmed:
                    if not n_sku:
                        st.error(" SKU ä¸èƒ½ä¸ºç©º")
                    else:
                        data = {
                            "SKU": n_sku, "Category": n_cat, "SubCategory": n_sub,
                            "Type": n_type, "Cost": n_cost, "Freight": n_frt
                        }
                        with st.spinner("æ­£åœ¨å†™å…¥..."):
                            ok, msg = mgr.add_new_sku(data)

                        if ok:
                            st.success(msg)
                            st.balloons()
                            # æ¸…ç†ç¼–è¾‘ç¼“å­˜
                            if 'df_cogs_edit' in st.session_state:
                                del st.session_state.df_cogs_edit
                            time.sleep(1.5)
                            st.rerun()
                        else:
                            st.error(msg)
==================== END FILE: ui/pages/db_data_change.py ====================


==================== START FILE: ui/pages/db_admin.py ====================
# ui/pages/db_admin.py

import os
import re
import time
from datetime import datetime
from typing import List, Dict, Tuple

import streamlit as st

# ç¡®ä¿ Config è¢«æ­£ç¡®å¯¼å…¥
from config import Config
from core.services.database_service import DatabaseService
from ui.components.safety import confirm_dangerous_action


def parse_filename_to_display(filename: str) -> str:
    """
    [UIè¾…åŠ©] è§£æå¤‡ä»½æ–‡ä»¶å -> äººç±»å¯è¯»æ ¼å¼
    Input:  20251205_174730_init.sql
    Output: ğŸ•’ 2025-12-05 17:47:30 | ğŸ·ï¸ init
    """
    # å»æ‰è·¯å¾„ï¼Œåªå–æ–‡ä»¶å
    filename = os.path.basename(filename)

    # å»æ‰æ‰©å±•å
    base = filename.rsplit(".", 1)[0]
    parts = base.split("_", 2)

    if len(parts) >= 2:
        date_str, time_str = parts[0], parts[1]
        tag = parts[2] if len(parts) > 2 else ""

        # ä¸¥æ ¼æ ¡éªŒæ˜¯å¦ä¸ºæ•°å­—ï¼Œé˜²æ­¢è§£æéæ ‡å‡†æ–‡ä»¶æŠ¥é”™
        if date_str.isdigit() and time_str.isdigit():
            try:
                dt = datetime.strptime(date_str + time_str, "%Y%m%d%H%M%S")
                time_label = dt.strftime("%Y-%m-%d %H:%M:%S")

                if tag:
                    return f"ğŸ•’ {time_label} | ğŸ·ï¸ {tag}"
                else:
                    return f"ğŸ•’ {time_label}"
            except:
                pass

    # å¦‚æœè§£æå¤±è´¥ï¼Œä¸ºäº†å®‰å…¨ï¼Œä¸æ˜¾ç¤ºåŸå§‹æ–‡ä»¶åï¼Œæ˜¾ç¤º"æœªçŸ¥å¤‡ä»½"
    return "â“ æœªçŸ¥æ ¼å¼å¤‡ä»½ (Unknown Backup)"


def _build_backup_options(file_list: List[str]) -> Tuple[List[str], Dict[str, str]]:
    """
    æ„å»ºä¸‹æ‹‰èœå•é€‰é¡¹
    Returns: (UIæ˜¾ç¤ºåˆ—è¡¨, æ˜ å°„å­—å…¸{æ˜¾ç¤ºå: ç‰©ç†æ–‡ä»¶å})
    """
    options_map = {}
    for f in file_list:
        display_name = parse_filename_to_display(f)
        # å¦‚æœæœ‰é‡å¤çš„æ—¶é—´ç‚¹ï¼ˆæå°‘è§ï¼‰ï¼Œè¿½åŠ åç¼€é˜²æ­¢å­—å…¸Keyå†²çª
        if display_name in options_map:
            display_name += " (Copy)"
        options_map[display_name] = f

    # æŒ‰æ ‡ç­¾æ’åº (æ—¶é—´å€’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢)
    sorted_labels = sorted(options_map.keys(), reverse=True)
    return sorted_labels, options_map


def render():
    st.title("ğŸ—„ï¸ æ•°æ®åº“è¿ç»´ä¸­å¿ƒ (DB Admin)")
    st.caption("æä¾›æ•°æ®åº“å¿«ç…§å¤‡ä»½ã€ç¾éš¾æ¢å¤åŠç¯å¢ƒé‡ç½®åŠŸèƒ½ã€‚")

    service = DatabaseService()

    tab_backup, tab_restore, tab_manage, tab_danger = st.tabs([
        "ğŸ“¤ åˆ›å»ºå¤‡ä»½ (Backup)",
        "ğŸ“¥ ç¾éš¾æ¢å¤ (Restore)",
        "ğŸ—‚ï¸ å¤‡ä»½ç®¡ç† (Manage)",
        "â˜¢ï¸ å±é™©åŒºåŸŸ (Danger Zone)"
    ])

    # =========================================================================
    # Tab 1: åˆ›å»ºå¤‡ä»½
    # =========================================================================
    with tab_backup:
        st.info("ğŸ’¡ è¯´æ˜ï¼šæ­¤æ“ä½œå°†ä¸ºå½“å‰æ•°æ®åº“çš„æ‰€æœ‰è¡¨åˆ›å»ºå…¨é‡å¿«ç…§ (System Snapshot)ã€‚")

        with st.container(border=True):
            c1, c2 = st.columns([3, 1])
            with c1:
                tag = st.text_input("å¿«ç…§å¤‡æ³¨ (å¯é€‰)", placeholder="ä¾‹å¦‚: Version_1.5_Update")
            with c2:
                st.write("")
                st.write("")
                if st.button("ğŸ“¸ ç«‹å³å¤‡ä»½", type="primary", use_container_width=True):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆå¿«ç…§..."):
                        ok, msg = service.create_backup(tag)
                    if ok:
                        st.success(msg)  # msg åœ¨ service å±‚å·²ç»è„±æ•ï¼ŒåªæŠ¥å¤§å°
                        time.sleep(1)
                        st.rerun()
                    else:
                        st.error(msg)

    # =========================================================================
    # Tab 2: ç¾éš¾æ¢å¤ (å®‰å…¨è„±æ•)
    # =========================================================================
    with tab_restore:
        st.warning("âš ï¸ **é«˜å±è­¦å‘Š**ï¼šè¿˜åŸæ“ä½œå°† **å®Œå…¨è¦†ç›–** å½“å‰æ•°æ®åº“ï¼Œä¸”ä¸å¯æ’¤é”€ï¼")

        files = service.list_backups()
        if not files:
            st.info("æš‚æ— å¯ç”¨è¿˜åŸç‚¹ã€‚")
        else:
            labels, map_files = _build_backup_options(files)

            selected_label = st.selectbox("è¯·é€‰æ‹©è¿˜åŸæ—¶é—´ç‚¹:", labels)
            # è·å–ç‰©ç†æ–‡ä»¶åç”¨äºåç«¯æ“ä½œï¼Œä½†åœ¨å‰ç«¯éšè—
            real_filename = map_files[selected_label]

            st.divider()

            # [Fix] å®‰å…¨ç»„ä»¶ï¼šæè¿°ä¸­ä½¿ç”¨ selected_label (æ ¼å¼åŒ–æ—¶é—´)ï¼Œè€Œé real_filename
            confirmed, note = confirm_dangerous_action(
                title="ğŸ”´ æœ€ç»ˆç¡®è®¤ï¼šæ‰§è¡Œæ•°æ®åº“å›æ»š",
                description=f"å³å°†è¿˜åŸè‡³æ—¶é—´ç‚¹: **{selected_label}**ã€‚\nè¯·ç¡®è®¤å½“å‰æ²¡æœ‰å…¶ä»–ç”¨æˆ·æ­£åœ¨æ“ä½œã€‚",
                confirm_phrase="RESTORE",
                require_current_password=True,
                require_operator_password=True,
                key_prefix="db_restore"
            )

            if confirmed:
                progress_bar = st.progress(0.0, text="å‡†å¤‡å¼€å§‹è¿˜åŸ...")

                def update_ui(p):
                    progress_bar.progress(max(0.0, min(p, 1.0)), text=f"è¿˜åŸè¿›åº¦: {int(p * 100)}%")

                # ä¼ é€’çœŸå®æ–‡ä»¶åç»™åç«¯
                ok, msg = service.restore_backup_with_progress(real_filename, update_ui)

                if ok:
                    st.balloons()
                    st.success(f" æ•°æ®åº“å·²æˆåŠŸå›æ»šè‡³: {selected_label}")
                else:
                    st.error(msg)

    # =========================================================================
    # Tab 3: å¤‡ä»½ç®¡ç† (å®‰å…¨è„±æ•)
    # =========================================================================
    with tab_manage:
        files = service.list_backups()
        if not files:
            st.info("æš‚æ— å†å²å¤‡ä»½ã€‚")
        else:
            st.markdown("#### ğŸ—‘ï¸ å†å²å¿«ç…§ç®¡ç†")

            # [Fix] è¡¨æ ¼å±•ç¤ºï¼šåˆ—å‡ºè§£æåçš„æ—¶é—´ç‚¹ï¼Œéšè—ç‰©ç†æ–‡ä»¶å
            df_files = []
            for f in files:
                try:
                    f_path = Config.BACKUP_DIR / f
                    size_kb = os.path.getsize(f_path) / 1024
                    display_name = parse_filename_to_display(f)

                    df_files.append({
                        "å¤‡ä»½æ—¶é—´ç‚¹ / å¤‡æ³¨": display_name,
                        "æ–‡ä»¶å¤§å°": f"{size_kb:.1f} KB"
                    })
                except:
                    pass

            st.dataframe(df_files, use_container_width=True)
            st.markdown("---")

            # åˆ é™¤æ“ä½œåŒº
            labels, map_files = _build_backup_options(files)
            target_label = st.selectbox("é€‰æ‹©è¦åˆ é™¤çš„å¤‡ä»½:", labels, key="del_sel")
            target_file = map_files[target_label]

            # [Fix] æŒ‰é’®æ–‡æœ¬åªæ˜¾ç¤ºæ—¶é—´ç‚¹
            if st.button(f"ğŸ—‘ï¸ åˆ é™¤: {target_label}", type="secondary"):
                ok, msg = service.delete_backup(target_file)
                if ok:
                    st.toast(f"å·²åˆ é™¤å¤‡ä»½ç‚¹: {target_label}")
                    time.sleep(1)
                    st.rerun()
                else:
                    st.error(msg)

    # =========================================================================
    # Tab 4: å±é™©åŒºåŸŸ (Reset)
    # =========================================================================
    with tab_danger:
        st.error("ğŸ’€ **æåº¦å±é™©åŒº**ï¼šä»¥ä¸‹æ“ä½œå…·æœ‰ç ´åæ€§ï¼Œè¯·è°¨æ…æ‰§è¡Œã€‚")

        with st.container(border=True):
            st.markdown("#### ğŸ§¹ ä¸€é”®æ¸…ç©ºæµ‹è¯•æ•°æ® (Reset Environment)")
            st.markdown(
                """
                æ­¤æ“ä½œå°† **æ°¸ä¹…æ¸…ç©º** ä»¥ä¸‹ä¸šåŠ¡æ•°æ®è¡¨ï¼Œä»…ä¿ç•™ç”¨æˆ·è´¦å·å’Œ SKU åŸºç¡€èµ„æ–™ï¼š
                - äº¤æ˜“æµæ°´ (Transaction)
                - èµ„é‡‘æµæ°´ (Order Earning)
                - æŠ¥è¡¨æºæ•°æ® (Clean Log)
                - åº“å­˜æ•°æ® (Inventory)
                """
            )

            confirmed, note = confirm_dangerous_action(
                title="âš ï¸ ç¡®è®¤æ¸…ç©ºä¸šåŠ¡æ•°æ®",
                description="æ­¤æ“ä½œä¸å¯æ’¤é”€ï¼å»ºè®®å…ˆåœ¨ [å¤‡ä»½] é¡µé¢åˆ›å»ºä¸€ä¸ªå¿«ç…§ã€‚",
                confirm_phrase="RESET",
                require_current_password=True,
                require_operator_password=True,
                key_prefix="db_reset"
            )

            if confirmed:
                with st.spinner("æ­£åœ¨é‡ç½®æ•°æ®åº“ç¯å¢ƒ..."):
                    ok, msg = service.reset_business_data()

                if ok:
                    st.success(msg)
                    st.balloons()
                else:
                    st.error(msg)

==================== END FILE: ui/pages/db_admin.py ====================


==================== START FILE: ui/pages/audit_logs.py ====================
# ui/pages/audit_logs.py
"""
å®‰å…¨å®¡è®¡æ—¥å¿—æŸ¥çœ‹é¡µé¢ (Audit Logs Center) - Enterprise V2.3

Fix Log:
- UIå¢å¼º: åœ¨å®¡è®¡åˆ—è¡¨ä¸­æ˜¾å¼å¢åŠ  "æ¥æº IP (Source IP)" åˆ—ï¼Œå®ç°å®Œæ•´çš„æ“ä½œæº¯æºã€‚
"""

from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
import streamlit as st

from config import Config
from core.logging_config import get_logger

logger = get_logger(__name__)


# ==============================================================================
# æƒé™æ ¡éªŒ
# ==============================================================================
def _ensure_admin():
    user = st.session_state.get("auth_user")
    if not user or not user.get("is_admin"):
        st.error(" æƒé™ä¸è¶³ï¼šä»…ç®¡ç†å‘˜å¯æŸ¥çœ‹å®¡è®¡æ—¥å¿—ã€‚")
        st.stop()
    return user


# ==============================================================================
# æ—¥å¿—è§£æå¼•æ“
# ==============================================================================
def _parse_log_line(line: str) -> Dict[str, Any]:
    """
    è§£ææ ‡å‡†æ—¥å¿—æ ¼å¼:
    YYYY-MM-DD ... | user=X | action=Y | ... | ip=Z | Message
    """
    line = line.strip()
    if not line: return {}

    # æ ¼å¼å‚è€ƒ core/logging_config.py
    # ä½¿ç”¨ " | " åˆ†å‰²ï¼Œæœ€åä¸€éƒ¨åˆ†æ˜¯ message
    parts = line.split(" | ")

    if len(parts) < 4:
        return {"time": "-", "level": "-", "user": "-", "ip": "-", "action": "-", "message": line}

    try:
        ts = parts[0].strip()
        level = parts[1].strip()
        module = parts[2].strip()

        message = parts[-1]
        kv_parts = parts[3:-1]  # ä¸­é—´éƒ¨åˆ†æ˜¯é”®å€¼å¯¹

        meta = {}
        for item in kv_parts:
            if "=" in item:
                k, v = item.split("=", 1)
                meta[k.strip()] = v.strip()

        return {
            "time": ts,
            "level": level,
            "module": module,
            "user": meta.get("user", "-"),
            "ip": meta.get("ip", "-"),  # æå– IP
            "action": meta.get("action", "-"),
            "table": meta.get("table", "-"),
            "message": message,
            "raw": line
        }
    except:
        return {"time": "-", "level": "-", "user": "-", "ip": "-", "message": line}


def _load_logs(log_file: str, max_lines: int = 1000) -> pd.DataFrame:
    path = Path(Config.LOG_DIR) / log_file
    if not path.exists():
        return pd.DataFrame()

    try:
        with path.open("r", encoding="utf-8", errors="ignore") as f:
            lines = f.readlines()[-max_lines:]
        lines.reverse()
        parsed_data = [_parse_log_line(l) for l in lines]
        return pd.DataFrame(parsed_data)
    except Exception as e:
        logger.error(f"è¯»å–æ—¥å¿—å¤±è´¥: {e}")
        return pd.DataFrame()


# ==============================================================================
# é¡µé¢æ¸²æŸ“
# ==============================================================================
def render():
    _ensure_admin()
    st.title("ğŸ“œ å®‰å…¨å®¡è®¡æ—¥å¿— (Audit Logs)")
    st.caption("è¿½è¸ªç³»ç»Ÿå†…çš„æ‰€æœ‰æ•æ„Ÿæ“ä½œä¸æ•°æ®å˜æ›´ï¼ŒåŒ…å«æ“ä½œäºº IP æº¯æºã€‚")

    # --- è¿‡æ»¤å™¨ ---
    with st.container(border=True):
        c1, c2, c3 = st.columns([1, 1, 1])
        with c1:
            log_source = st.radio("æ—¥å¿—æ¥æº:", ["ğŸ“¢ ä¸šåŠ¡æ“ä½œæ—¥å¿— (Business)", "ğŸ›¡ï¸ æ•°æ®åº“åº•å±‚æ—¥å¿— (DB Kernel)"])
        with c2:
            show_read = st.checkbox("æ˜¾ç¤ºåªè¯»æ“ä½œ (SELECT/READ)", value=False)
        with c3:
            limit = st.select_slider("åŠ è½½è¡Œæ•°:", options=[100, 500, 1000, 2000], value=500)

    # --- åŠ è½½ ---
    filename = "app.log" if "Business" in log_source else "db_audit.log"
    with st.spinner(f"æ­£åœ¨åŠ è½½ {filename} ..."):
        df = _load_logs(filename, limit)

    if df.empty:
        st.info("æš‚æ— æ—¥å¿—è®°å½•ã€‚")
        return

    # --- è¿‡æ»¤ ---
    df = df[df["action"] != "-"]
    if not show_read:
        mask_read = df["action"].str.upper().str.contains("READ|SELECT", na=False)
        df = df[~mask_read]

    search_term = st.text_input("ğŸ” æœç´¢ (æ”¯æŒ User / IP / Action / Content):", placeholder="ä¾‹å¦‚: 192.168 æˆ– UPDATE")
    if search_term:
        mask_search = df.apply(lambda x: x.astype(str).str.contains(search_term, case=False).any(), axis=1)
        df = df[mask_search]

    st.markdown(f"**å…±æ‰¾åˆ° {len(df)} æ¡è®°å½•**")

    # --- [Fix] è¡¨æ ¼å±•ç¤ºï¼šåŠ å…¥ IP åˆ— ---
    if "Business" in log_source:
        cols = ["time", "user", "ip", "action", "message"]
    else:
        cols = ["time", "user", "ip", "action", "table", "message"]

    st.dataframe(
        df[cols],
        use_container_width=True,
        hide_index=True,
        column_config={
            "time": st.column_config.TextColumn("æ—¶é—´", width="medium"),
            "user": st.column_config.TextColumn("æ“ä½œäºº", width="small"),
            "ip": st.column_config.TextColumn("æ¥æº IP", width="medium"),  # æ–°å¢åˆ—
            "action": st.column_config.TextColumn("åŠ¨ä½œ", width="small"),
            "table": st.column_config.TextColumn("è¡¨å", width="small"),
            "message": st.column_config.TextColumn("è¯¦ç»†å†…å®¹ (Details)", width="large"),
        }
    )

    with st.expander("æŸ¥çœ‹åŸå§‹æ•°æ® (CSV Export)"):
        st.code(df.to_csv(index=False), language="csv")

==================== END FILE: ui/pages/audit_logs.py ====================


==================== START FILE: ui/pages/etl_cleaning.py ====================
# ui/pages/etl_cleaning.py

import streamlit as st
import time
from core.services.correction_service import CorrectionService


class DataCleaningPage:
    """
    [UIå±‚] æ•°æ®æ¸…æ´—å‘å¯¼ (Data Cleaning Wizard) - Refactored V2.0

    æ¶æ„å˜æ›´:
    - ç§»é™¤äº†ç›´æ¥çš„ DB è®¿é—® (Raw SQL)ã€‚
    - ç§»é™¤äº†ç›´æ¥çš„ ETL Parser è°ƒç”¨ã€‚
    - æ‰€æœ‰ä¸šåŠ¡é€»è¾‘ä»£ç†ç»™ CorrectionServiceã€‚
    """

    def __init__(self):
        # ä»…ä¾èµ– Serviceï¼Œä¸ä¾èµ– Repository æˆ– ETL
        self.corrector = CorrectionService()

    def _find_first_bad_sku(self, row) -> dict:
        """åœ¨è¡Œä¸­å®šä½ç¬¬ä¸€ä¸ªé”™è¯¯çš„å­—æ®µ (UI è¾…åŠ©é€»è¾‘)"""
        for i in range(1, 11):
            sku_col = f'P_SKU{i}'
            qty_col = f'P_Quantity{i}'

            # å…¼å®¹æ—§ç‰ˆæœ¬åˆ—å (è™½ç„¶ Service å±‚åº”è¯¥å¤„ç†ï¼Œä½† UI å±‚åšä¸ªé˜²å¾¡)
            if sku_col not in row and f"P_SKU{i}" in row:
                sku_col = f"P_SKU{i}"
                qty_col = f"P_Quantity{i}"

            if sku_col not in row: continue

            sku_val = str(row[sku_col]).strip().upper()
            qty_val = str(row[qty_col]).strip()

            # è·³è¿‡ç©ºæ•°æ®
            if not sku_val or sku_val == 'NONE': continue

            # æ£€æŸ¥ SKU æ˜¯å¦æœ‰æ•ˆ (è°ƒç”¨ Service)
            is_sku_valid = self.corrector.is_valid_sku(sku_val)

            # æ£€æŸ¥ æ•°é‡ æ˜¯å¦æœ‰æ•ˆ (è°ƒç”¨ Service è¾…åŠ©æ–¹æ³•)
            is_qty_valid = self.corrector.validate_quantity(qty_val)

            # åªè¦æœ‰ä¸€ä¸ªæ— æ•ˆï¼Œå°±è¿”å›è¯¥ç»„ç´¢å¼•
            if not is_sku_valid or not is_qty_valid:
                return {
                    "index": i,
                    "bad_sku": sku_val,
                    "bad_qty": qty_val,
                    "is_sku_error": not is_sku_valid,
                    "is_qty_error": not is_qty_valid
                }
        return None

    def render(self):
        st.subheader("ğŸ› ï¸ æ•°æ®æ¸…æ´—å‘å¯¼ (Data Cleaning Wizard)")

        col1, col2 = st.columns([3, 1])
        with col1:
            st.info("ï¸ ç³»ç»Ÿå°†å¼•å¯¼æ‚¨é€ä¸ªä¿®å¤å¼‚å¸¸æ•°æ®ã€‚æ‚¨çš„é€‰æ‹©å°†è¢«ç³»ç»Ÿè®°å¿†ï¼Œä¸‹æ¬¡è‡ªåŠ¨åº”ç”¨ã€‚")
        with col2:
            if st.button("ğŸ”„ é‡æ–°æ‰«æå…¨åº“", use_container_width=True):
                with st.spinner("ğŸ”„ æ­£åœ¨é‡æ–°æ‰«æä¸è‡ªåŠ¨ä¿®å¤..."):
                    # [è°ƒç”¨ Service] è¿è¡Œè§£æå™¨
                    report = self.corrector.run_auto_parser()

                logs = report.get("auto_fixed", [])
                pending = report.get("pending_count", 0)

                if logs:
                    st.success(f" è‡ªåŠ¨ä¿®å¤äº† {len(logs)} ä¸ªå†å²é—®é¢˜ï¼")
                    with st.expander("æŸ¥çœ‹ä¿®å¤è¯¦æƒ…", expanded=False):
                        for log in logs:
                            st.markdown(f"- è®¢å• `{log['order']}`: {log['msg']}")
                elif pending == 0:
                    st.success("ğŸ‰ æ•°æ®å·²å…¨éƒ¨æ¸…æ´—å®Œæ¯•ï¼")
                else:
                    st.info(f"æ‰«æå®Œæˆï¼Œå‰©ä½™ {pending} ä¸ªå¾…å¤„ç†é¡¹ã€‚")

                time.sleep(1)
                st.rerun()

        # 1. è·å–å¼‚å¸¸æ•°æ® (è°ƒç”¨ Service)
        row = self.corrector.get_next_pending_issue()

        if row is None:
            st.balloons()
            st.success("ğŸ‰ æ‰€æœ‰å¼‚å¸¸æ•°æ®å¤„ç†å®Œæ¯•ï¼æ‚¨å¯ä»¥è¿›è¡Œä¸‹ä¸€æ­¥åˆ†æäº†ã€‚")
            return

        error_info = self._find_first_bad_sku(row)

        # è‡ªæ„ˆæœºåˆ¶: å¦‚æœæ ‡è®°äº†é”™è¯¯ä½†æ‰¾ä¸åˆ°é”™ï¼Œè°ƒç”¨ Service æ ‡è®°è·³è¿‡
        if not error_info:
            self.corrector.mark_as_skipped(row['Order number'])
            st.rerun()
            return

        # 2. æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
        bad_sku = error_info['bad_sku']
        bad_qty = error_info['bad_qty']
        is_sku_error = error_info['is_sku_error']
        is_qty_error = error_info['is_qty_error']

        order_id = row['Order number']
        label = row['Custom label']
        title = row['Item title']

        # 3. æ¸²æŸ“ç•Œé¢
        with st.container(border=True):
            # é¡¶éƒ¨ä¿¡æ¯æ 
            st.markdown(f"### ğŸš© å‘ç°å¼‚å¸¸: `{bad_sku}` (Qty: {bad_qty})")

            c_info1, c_info2 = st.columns(2)
            with c_info1:
                st.caption("Custom Label (åŸå§‹æ ‡ç­¾)")
                st.code(label, language="text")
            with c_info2:
                st.caption("Item Title (å•†å“æ ‡é¢˜)")
                st.info(title)

            st.divider()

            # --- æ ¸å¿ƒäº¤äº’åŒº ---
            st.markdown("#### ğŸ”§ é€‰æ‹©ä¿®å¤æ–¹å¼")

            # A. å‡†å¤‡é€‰é¡¹ (è°ƒç”¨ Service è·å–å»ºè®®)
            suggestions = self.corrector.get_fuzzy_suggestions(bad_sku)

            radio_options = [s for s in suggestions]
            manual_option_label = "âœï¸ æ‰‹åŠ¨å½•å…¥ (Manual Input)"
            radio_options.append(manual_option_label)

            # B. æ¸²æŸ“å•é€‰æ¡†
            selected_option = st.radio(
                "è¯·é€‰æ‹©æ­£ç¡®çš„æ•°æ®:",
                options=radio_options,
                index=0,
                horizontal=False
            )

            st.markdown("---")

            # C. åŠ¨æ€è¾“å…¥åŒº
            final_sku = None
            final_qty = None
            can_submit = False

            # åˆ†æ”¯ 1: ç”¨æˆ·é€‰æ‹©äº† "æ‰‹åŠ¨å½•å…¥"
            if selected_option == manual_option_label:
                c_in1, c_in2 = st.columns(2)

                with c_in1:
                    if is_sku_error:
                        input_sku = st.text_input("æ­£ç¡® SKU", value="", placeholder="è¯·è¾“å…¥ Data_COGS ä¸­å­˜åœ¨çš„ SKU")
                    else:
                        input_sku = st.text_input("SKU (æ— éœ€ä¿®æ”¹)", value=bad_sku, disabled=True)

                with c_in2:
                    if is_qty_error:
                        input_qty = st.text_input("æ­£ç¡®æ•°é‡", value="1")
                    else:
                        input_qty = st.text_input("æ•°é‡ (æ— éœ€ä¿®æ”¹)", value=bad_qty, disabled=True)

                final_sku = input_sku.strip().upper()
                final_qty = input_qty.strip()
                can_submit = True

            # åˆ†æ”¯ 2: ç”¨æˆ·é€‰æ‹©äº† "æ™ºèƒ½æ¨è"
            else:
                final_sku = selected_option
                if is_qty_error:
                    final_qty = "1"
                    st.caption(f"ğŸ’¡ åŸæ•°é‡ `{bad_qty}` æ— æ•ˆï¼Œæ™ºèƒ½ä¿®å¤å°†é»˜è®¤è®¾ä¸º `1`ã€‚")
                else:
                    final_qty = bad_qty
                can_submit = True

            # D. æäº¤æŒ‰é’®ä¸å¤„ç†
            if can_submit:
                if st.button(" ç¡®è®¤å¹¶ä¿®å¤ (Confirm & Fix)", type="primary", use_container_width=True):
                    # æœ€ç»ˆæ ¡éªŒé€»è¾‘
                    errors = []

                    # æ ¡éªŒ SKU
                    if not final_sku:
                        errors.append("SKU ä¸èƒ½ä¸ºç©º")
                    elif not self.corrector.is_valid_sku(final_sku):
                        errors.append(f"SKU `{final_sku}` æ— æ•ˆ (æœªåœ¨ Data_COGS ä¸­æ‰¾åˆ°)")

                    # æ ¡éªŒ Quantity
                    if not self.corrector.validate_quantity(final_qty):
                        errors.append(f"æ•°é‡ `{final_qty}` æ— æ•ˆ (å¿…é¡»æ˜¯å¤§äº0çš„æ•´æ•°)")

                    if errors:
                        for e in errors:
                            st.error(f" {e}")
                    else:
                        clean_qty = str(int(float(final_qty)))
                        # [è°ƒç”¨ Service] æäº¤ä¿®å¤
                        success = self.corrector.apply_fix(
                            order_id, error_info['index'],
                            label, bad_sku, bad_qty,
                            final_sku, clean_qty
                        )

                        if success:
                            st.toast(f" å·²ä¿®æ­£: {final_sku} x {clean_qty}")
                            time.sleep(0.5)
                            st.rerun()
                        else:
                            st.error(" æ•°æ®åº“æ›´æ–°å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—ã€‚")


def render():
    DataCleaningPage().render()
==================== END FILE: ui/pages/etl_cleaning.py ====================


==================== START FILE: ui/pages/data_visualization.py ====================
# ui/pages/data_visualization.py

import streamlit as st
import datetime
import pandas as pd
import numpy as np
import altair as alt
from config import Config
from core.services.visual_service import VisualService

# --- MAPPING DICTIONARIES (New Chinese Names) ---
ACTION_MAP_UI_TO_DB = {
    "äº§å“é”€å”®": "Sales", "è®¢å•å–æ¶ˆ": "Cancel", "æ— å¹³å°ä»‹å…¥ä¸»åŠ¨é€€è´§": "Return",
    "å¹³å°ä»‹å…¥ç”¨æˆ·é€€è´§": "Request", "å¹³å°ä»‹å…¥å¼ºåˆ¶é€€è´§": "Case", "ç¬¬ä¸‰æ–¹ä»…é€€æ¬¾": "Dispute"
}
ACTION_MAP_DB_TO_UI = {v: k for k, v in ACTION_MAP_UI_TO_DB.items()}

SHIP_MAP_UI_TO_DB = {
    "æ™®é€šé‚®é€’": "ShipRegular", "é‚®è´¹ç½šæ¬¾": "ShipUnder",
    "é‚®è´¹è¶…æ”¯": "ShipOver", "åŒ…é€€è´§é‚®è´¹": "ShipReturn"
}

FEE_MAP_UI_TO_DB = {
    "äº§å“æˆæœ¬": "COGS", "å¹³å°è´¹ç”¨": "PlatformFee"
}


@st.cache_resource
def get_visual_service_v7():
    return VisualService()


@st.cache_data(ttl=600)
def load_data_v6(start_date, end_date, stores):
    service = get_visual_service_v7()
    return service.load_and_aggregate(start_date, end_date, stores)


def render():
    st.title("ğŸ“ˆ æ•°æ®äº¤äº’å¯è§†åŒ– (Interactive Visualization)")
    st.caption(f"Enterprise 3D Analytics Cockpit {Config.APP_VERSION}")

    st.markdown("""
        <style>
        .block-container { padding-top: 2rem !important; padding-bottom: 2rem !important; }
        .control-panel {
            background-color: rgba(30, 30, 40, 0.8);
            padding: 20px;
            border-radius: 12px;
            border: 1px solid rgba(78, 201, 176, 0.3);
            box-shadow: 0 0 20px rgba(0,0,0,0.5);
        }
        div[data-testid="stRadio"] > label { font-weight: bold; color: #4EC9B0; }
        </style>
    """, unsafe_allow_html=True)

    with st.container():
        c_title, c_clear = st.columns([5, 1])
        with c_title: pass
        with c_clear:
            if st.button("ğŸ§¹ å¼ºåˆ¶åˆ·æ–°"):
                st.cache_data.clear()
                st.cache_resource.clear()
                st.rerun()

        c1, c2, c3 = st.columns([1.5, 2, 1])
        with c1:
            st.markdown("### ğŸ‘ï¸ ç»Ÿè®¡æ–¹å¼ (Method)")
            view_mode = st.radio(
                "View Mode",
                ["Amount", "Quantity", "Order", "Percentage"],
                horizontal=True,
                label_visibility="collapsed"
            )
        with c2:
            st.markdown("### ğŸ“… æ—¶é—´åŒºé—´ (Time Range)")
            end_def = datetime.date.today()
            start_def = end_def - datetime.timedelta(days=30)
            date_range = st.date_input("Select Range", value=(start_def, end_def), label_visibility="collapsed")
        with c3:
            st.markdown("### ğŸª åº—é“º (Store)")
            stores = st.multiselect("Stores", ["esplus", "88"], default=["esplus", "88"], label_visibility="collapsed")

    st.markdown("---")

    col_filters, col_main = st.columns([1, 3.5])

    disable_ship_filters = (view_mode == "Quantity")
    disable_fee_filters = (view_mode == "Quantity") or (view_mode == "Order")

    action_ui_list = list(ACTION_MAP_UI_TO_DB.keys())
    ship_ui_list = list(SHIP_MAP_UI_TO_DB.keys())
    fee_ui_list = list(FEE_MAP_UI_TO_DB.keys())

    with col_filters:
        st.markdown('<div class="control-panel">', unsafe_allow_html=True)
        st.markdown("#### ğŸ› ï¸ é«˜çº§ç­›é€‰ (Filters)")

        with st.expander("ğŸ“¦ é”€å”®ç±»å‹ (Sales Type)", expanded=True):
            actions = st.multiselect("Action Types", action_ui_list, default=[ACTION_MAP_DB_TO_UI["Sales"]])

        with st.expander("ğŸšš é‚®è´¹ç±»å‹ (Shipping)", expanded=False):
            if disable_ship_filters:
                st.caption("ğŸš« è¯¥æ¨¡å¼ä¸‹ä¸å¯ç”¨")
                ship_types = []
            else:
                ship_types = st.multiselect("Shipping Types", ship_ui_list, default=[])

        with st.expander("ğŸ’¸ é”€å”®è´¹ç”¨ (Fees)", expanded=False):
            if disable_fee_filters:
                st.caption("ğŸš« è¯¥æ¨¡å¼ä¸‹ä¸å¯ç”¨")
                fees = []
            else:
                fees = st.multiselect("Fee Types", fee_ui_list, default=[])

        st.info(f"æ¨¡å¼: {view_mode}")
        st.markdown('</div>', unsafe_allow_html=True)

    df_agg = pd.DataFrame()

    if len(date_range) == 2 and stores:
        start, end = date_range
        with st.spinner("ğŸš€ æ­£åœ¨æ‰§è¡Œå‘é‡åŒ–èšåˆè®¡ç®—..."):
            try:
                result = load_data_v6(start, end, stores)
                if isinstance(result, tuple) and len(result) == 2:
                    df_agg, _ = result  # å¿½ç•¥ SQL
                elif isinstance(result, pd.DataFrame):
                    df_agg = result
                else:
                    st.error(" æ•°æ®æ ¼å¼é”™è¯¯")
            except Exception as e:
                st.error(f"Error: {e}")

    with col_main:
        if df_agg.empty:
            st.warning("æ— æ•°æ®ã€‚è¯·æ£€æŸ¥æ—¶é—´èŒƒå›´æˆ–åº—é“ºã€‚")
        else:
            st.success(f" è®¡ç®—å®Œæˆ ({len(df_agg)} è¡Œ)")

            show_cols = []

            # --- Percentage Mode ---
            if view_mode == "Percentage":
                denom_col = "Sales_Amount"
                if denom_col not in df_agg.columns:
                    denom = pd.Series(0.0)
                else:
                    denom = df_agg[denom_col].replace(0, np.nan)

                for act_ui in actions:
                    act_db = ACTION_MAP_UI_TO_DB.get(act_ui, act_ui)
                    num_col = f"{act_db}_Amount"
                    if num_col in df_agg.columns:
                        pct_col_ui = f"{act_ui} å æ¯”"
                        df_agg[pct_col_ui] = (df_agg[num_col] / denom).fillna(0)
                        show_cols.append(pct_col_ui)

                for ship_ui in ship_types:
                    key_db = SHIP_MAP_UI_TO_DB.get(ship_ui, ship_ui)
                    num_col = f"Total_{key_db}"
                    if num_col in df_agg.columns:
                        pct_col_ui = f"{ship_ui} å æ¯”"
                        df_agg[pct_col_ui] = (df_agg[num_col] / denom).fillna(0)
                        show_cols.append(pct_col_ui)

                for fee_ui in fees:
                    key_db = FEE_MAP_UI_TO_DB.get(fee_ui, fee_ui)
                    num_col = f"Total_{key_db}"
                    if num_col in df_agg.columns:
                        pct_col_ui = f"{fee_ui} å æ¯”"
                        df_agg[pct_col_ui] = (df_agg[num_col] / denom).fillna(0)
                        show_cols.append(pct_col_ui)

            # --- Standard Modes ---
            else:
                suffix = view_mode
                for act_ui in actions:
                    act_db = ACTION_MAP_UI_TO_DB.get(act_ui, act_ui)
                    col = f"{act_db}_{suffix}"
                    if col in df_agg.columns:
                        df_agg[act_ui] = df_agg[col]
                        show_cols.append(act_ui)

                if not disable_ship_filters:
                    prefix = "Total_Order" if view_mode == "Order" else "Total"
                    for ship_ui in ship_types:
                        key_db = SHIP_MAP_UI_TO_DB.get(ship_ui, ship_ui)
                        col = f"{prefix}_{key_db}"
                        if col in df_agg.columns:
                            df_agg[ship_ui] = df_agg[col]
                            show_cols.append(ship_ui)

                if not disable_fee_filters:
                    for fee_ui in fees:
                        key_db = FEE_MAP_UI_TO_DB.get(fee_ui, fee_ui)
                        col = f"Total_{key_db}"
                        if col in df_agg.columns:
                            df_agg[fee_ui] = df_agg[col]
                            show_cols.append(fee_ui)

            # --- Chart Rendering (Altair) ---
            if show_cols:
                # å‡†å¤‡æ•°æ®
                df_chart = df_agg[['DateStr'] + show_cols].rename(columns={'DateStr': 'æ—¥æœŸ'})
                df_chart_melt = df_chart.melt('æ—¥æœŸ', var_name='æŒ‡æ ‡', value_name='æ•°å€¼')

                # è®¾ç½® Y è½´æ ¼å¼
                if view_mode == "Amount":
                    y_format = '$.2s';
                    y_title = "é‡‘é¢ ($)"
                elif view_mode == "Percentage":
                    y_format = '.1%';
                    y_title = "å æ¯” (%)"
                elif view_mode == "Quantity":
                    y_format = ',.0f';
                    y_title = "æ•°é‡"
                elif view_mode == "Order":
                    y_format = ',.0f';
                    y_title = "è®¢å•æ•°"
                else:
                    y_format = ',.0f';
                    y_title = "æ•°å€¼"

                # åŸºç¡€å›¾è¡¨å¯¹è±¡
                base = alt.Chart(df_chart_melt).encode(
                    x=alt.X('æ—¥æœŸ:T', title=None),
                    tooltip=['æ—¥æœŸ:T', 'æŒ‡æ ‡', alt.Tooltip('æ•°å€¼', format=y_format, title=y_title)]
                )

                # å›¾å±‚1: ä¸»æŠ˜çº¿ (åœ†æ»‘)
                line_chart = base.mark_line(interpolate='monotone', strokeWidth=3).encode(
                    y=alt.Y('æ•°å€¼:Q', title=y_title, axis=alt.Axis(format=y_format)),
                    color=alt.Color('æŒ‡æ ‡', legend=alt.Legend(orient='bottom', title=None))
                )

                # æ¡ä»¶: ä»…é€‰ä¸­ 1 ä¸ªé¡¹ç›®æ—¶ï¼Œæ˜¾ç¤ºè¾…åŠ©çº¿
                if len(show_cols) == 1:
                    # å›¾å±‚2: è¶‹åŠ¿çº¿ (Trendline) - ç°è‰²è™šçº¿
                    # ä½¿ç”¨ transform_calculate æ³¨å…¥ "è¶‹åŠ¿çº¿" æ ‡ç­¾ä»¥æ˜¾ç¤ºåœ¨å›¾ä¾‹ä¸­
                    trend_line = base.transform_regression(
                        'æ—¥æœŸ', 'æ•°å€¼', method="linear"
                    ).transform_calculate(
                        Trend="'è¶‹åŠ¿çº¿'"
                    ).mark_line(
                        strokeDash=[5, 5], color='gray', opacity=0.8
                    ).encode(
                        # æ˜ å°„åˆ°ä¸€ä¸ªå¸¸é‡é¢œè‰²ï¼Œä¸ºäº†è®©å®ƒå‡ºç°åœ¨ Legend
                        color=alt.Color('Trend:N', legend=alt.Legend(orient='bottom', title=None),
                                        scale=alt.Scale(range=['gray']))
                    )

                    # å›¾å±‚3: å¹³å‡çº¿ (Average Rule) - æ©™è‰²è™šçº¿
                    # ä½¿ç”¨ transform_joinaggregate è®¡ç®—å…¨å±€å¹³å‡å€¼
                    avg_line = base.transform_joinaggregate(
                        mean_val='mean(æ•°å€¼)'
                    ).transform_calculate(
                        Avg="'å¹³å‡çº¿'"
                    ).mark_rule(
                        strokeDash=[4, 2], color='#FFA500'
                    ).encode(
                        y='mean_val:Q',
                        color=alt.Color('Avg:N', legend=alt.Legend(orient='bottom', title=None),
                                        scale=alt.Scale(range=['#FFA500']))
                    )

                    final_chart = (line_chart + trend_line + avg_line).resolve_scale(color='independent')
                else:
                    final_chart = line_chart

                final_chart = final_chart.properties(
                    padding={'left': 60, 'right': 20, 'top': 10, 'bottom': 10}
                )
                st.altair_chart(final_chart, use_container_width=True)
            else:
                st.info(f"è¯·å‹¾é€‰å·¦ä¾§ç­›é€‰å™¨ä»¥æ˜¾ç¤º {view_mode} æ•°æ®ã€‚")

            # [Removed] åŸå§‹æ•°æ®æ¡†å·²å½»åº•ç§»é™¤

    if not stores:
        st.toast("è¯·é€‰æ‹©åº—é“º", icon="ğŸš¨")

==================== END FILE: ui/pages/data_visualization.py ====================


==================== START FILE: ui/pages/ai_gemini.py ====================
# ui/pages/ai_gemini.py

import os
import time
import streamlit as st
import google.generativeai as genai
import requests
import datetime
from config import Config
from core.context import get_current_user
from core.services.chat_service import ChatService
from ui.utils import get_client_ip  # [DRY Refactor] å¼•å…¥å…¬å…±å·¥å…·


@st.cache_data(ttl=600)
def get_environment_context(ip: str) -> str:
    """è·å–ç¯å¢ƒä¸Šä¸‹æ–‡ (ç¼“å­˜)"""
    context_str = f"ç”¨æˆ· IP: {ip}\n"
    if ip in ["UNKNOWN", "127.0.0.1", "localhost"]:
        return context_str + "ä½ç½®: æœ¬åœ°å¼€å‘ç¯å¢ƒ (æœªçŸ¥ä½ç½®)"
    try:
        loc_res = requests.get(f"http://ip-api.com/json/{ip}", timeout=2).json()
        if loc_res.get("status") == "success":
            city = loc_res.get("city", "Unknown")
            timezone = loc_res.get("timezone", "UTC")
            context_str += f"ç”¨æˆ·ä½ç½®: {city} (æ—¶åŒº: {timezone})\n"

            # è·å–å¤©æ°”
            lat, lon = loc_res["lat"], loc_res["lon"]
            w_url = f"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true&timezone={timezone}"
            w_res = requests.get(w_url, timeout=2).json()
            if "current_weather" in w_res:
                curr = w_res["current_weather"]
                context_str += f"å½“å‰å¤©æ°”: æ°”æ¸© {curr['temperature']}Â°C\n"

            # è·å–æ—¶é—´
            try:
                import pytz
                local_tz = pytz.timezone(timezone)
                now = datetime.datetime.now(local_tz).strftime("%Y-%m-%d %H:%M:%S")
                context_str += f"å½“åœ°æ—¶é—´: {now}\n"
            except:
                pass
    except:
        pass
    return context_str


def render():
    # 1. åˆå§‹åŒ–æœåŠ¡
    user = get_current_user() or "anonymous"
    chat_service = ChatService(user)

    # 2. API Key æ£€æŸ¥
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        st.error(" æœªé…ç½® GOOGLE_API_KEYã€‚")
        return

    # 3. CSS ä¼˜åŒ–
    st.markdown("""
        <style>
        .chat-row {
            padding: 10px;
            border-radius: 5px;
            margin-bottom: 5px;
            background-color: rgba(255, 255, 255, 0.05);
            transition: background-color 0.3s;
            cursor: pointer;
        }
        .chat-row:hover {
            background-color: rgba(255, 255, 255, 0.1);
        }
        .chat-active {
            background-color: rgba(78, 201, 176, 0.2) !important;
            border-left: 3px solid #4EC9B0;
        }
        </style>
    """, unsafe_allow_html=True)

    # 4. å¸ƒå±€ï¼šå·¦ä¾§å†å²ï¼Œå³ä¾§å¯¹è¯
    col_sidebar, col_chat = st.columns([1, 3.5])

    # ===========================
    # å·¦ä¾§ï¼šä¼šè¯åˆ—è¡¨
    # ===========================
    with col_sidebar:
        if st.button("â• æ–°å»ºå¯¹è¯", use_container_width=True, type="primary"):
            chat_service.create_session()
            st.rerun()

        st.markdown("---")
        st.caption("ğŸ•’ å†å²è®°å½•")

        # è·å–æ‰€æœ‰ä¼šè¯
        sessions = chat_service.get_all_sessions()
        active_info = chat_service.get_active_session()
        active_id = active_info["id"]

        # æ¸²æŸ“åˆ—è¡¨
        with st.container(height=600, border=False):
            for s in sessions:
                sid = s["id"]
                title = s["title"]
                is_active = (sid == active_id)

                # ä½¿ç”¨åˆ—å¸ƒå±€å®ç°ï¼šæ ‡é¢˜ + åˆ é™¤æŒ‰é’®
                c_text, c_del = st.columns([4, 1])

                # æ ‡é¢˜æŒ‰é’® (åˆ‡æ¢ä¼šè¯)
                btn_type = "secondary" if not is_active else "primary"
                if c_text.button(f"ğŸ’¬ {title}", key=f"sel_{sid}", help=s["updated_at"], use_container_width=True,
                                 type=btn_type):
                    chat_service.set_active_session(sid)
                    st.rerun()

                # åˆ é™¤æŒ‰é’®
                if c_del.button("ğŸ—‘ï¸", key=f"del_{sid}", help="åˆ é™¤æ­¤ä¼šè¯"):
                    chat_service.delete_session(sid)
                    st.rerun()

    # ===========================
    # å³ä¾§ï¼šèŠå¤©çª—å£
    # ===========================
    with col_chat:
        current_session = active_info["data"]

        # æ ‡é¢˜æ 
        st.markdown(f"### ğŸ¤– {current_session.get('title', 'Google Gemini')}")
        st.caption("Powered by Gemini 2.0 Flash | ğŸ”’ ç‹¬ç«‹è®°å¿†ç©ºé—´")

        # è·å–ç¯å¢ƒä¸Šä¸‹æ–‡ (æ‡’åŠ è½½)
        if "ai_context_str" not in st.session_state:
            # [Refactor] è°ƒç”¨å…¬å…± IP å‡½æ•°
            ip = get_client_ip()
            st.session_state.ai_context_str = get_environment_context(ip)

        # åˆå§‹åŒ–æ¨¡å‹
        try:
            genai.configure(api_key=api_key)
            system_instruction = f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸š ERP æ™ºèƒ½åŠ©æ‰‹ (Eaglestar ERP Assistant)ã€‚
            ã€å½“å‰ç”¨æˆ·ç¯å¢ƒä¿¡æ¯ã€‘
            {st.session_state.ai_context_str}
            ã€ä½ çš„èŒè´£ã€‘
            1. å›ç­”å…³äºåº“å­˜ç®¡ç†ã€ä¾›åº”é“¾ã€æ•°æ®åˆ†æçš„é€šç”¨é—®é¢˜ã€‚
            2. ä¿æŒå›ç­”ç®€æ´ã€ä¸“ä¸šã€‚
            """
            model = genai.GenerativeModel(model_name='gemini-2.0-flash', system_instruction=system_instruction)
        except Exception as e:
            st.error(f"æ¨¡å‹è¿æ¥å¤±è´¥: {e}")
            return

        # æ¸²æŸ“æ¶ˆæ¯æµ
        messages = current_session.get("messages", [])
        for msg in messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        # å¤„ç†è¾“å…¥
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
            # 1. è®°å½•ç”¨æˆ·æ¶ˆæ¯
            chat_service.add_message(active_id, "user", prompt)
            with st.chat_message("user"):
                st.markdown(prompt)

            # 2. ç”Ÿæˆå›å¤
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                try:
                    # æ„é€ ä¸Šä¸‹æ–‡ (Gemini æ ¼å¼)
                    chat_history = []
                    for m in messages:
                        role = "user" if m["role"] == "user" else "model"
                        chat_history.append({"role": role, "parts": [m["content"]]})

                    chat = model.start_chat(history=chat_history)
                    response = chat.send_message(prompt, stream=True)

                    for chunk in response:
                        if chunk.text:
                            full_response += chunk.text
                            message_placeholder.markdown(full_response + "â–Œ")

                    message_placeholder.markdown(full_response)

                    # 3. è®°å½•åŠ©æ‰‹å›å¤
                    chat_service.add_message(active_id, "assistant", full_response)

                    # å¦‚æœæ˜¯ç¬¬ä¸€è½®å¯¹è¯ï¼Œåˆ·æ–°é¡µé¢ä»¥æ›´æ–°å·¦ä¾§æ ‡é¢˜
                    if len(messages) == 0:
                        st.rerun()

                except Exception as e:
                    st.error(f"AI å“åº”å¼‚å¸¸: {e}")
==================== END FILE: ui/pages/ai_gemini.py ====================


==================== START FILE: tools/restore_project.py ====================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# tools/restore_project.py
"""
ä» project_all_code_*.txt æ¢å¤æ•´ä¸ªé¡¹ç›®ç»“æ„ã€‚
ä½ç½®ï¼štools/restore_project.py
"""

import sys
import re
from pathlib import Path

# åŒ¹é… START/END æ ‡è®°
START_PATTERN = re.compile(r"^=+\s*START (?:SPECIAL )?FILE:\s*(.+?)\s*=+\s*$")
END_PATTERN = re.compile(r"^=+\s*END FILE:\s*(.+?)\s*=+\s*$")


def parse_project_root_hint(line: str) -> Path | None:
    prefix = "Project Root:"
    if line.startswith(prefix):
        path_str = line[len(prefix):].strip()
        if path_str:
            return Path(path_str).expanduser()
    return None


def write_file(base_dir: Path, rel_path: str, content: str):
    target_path = base_dir / rel_path
    # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢å†™å…¥åˆ°é¡¹ç›®æ ¹ç›®å½•ä¹‹å¤–
    try:
        target_path.resolve().relative_to(base_dir.resolve())
    except ValueError:
        print(f"âš ï¸ è·³è¿‡éæ³•è·¯å¾„ (è¶Šç•Œ): {rel_path}")
        return

    target_path.parent.mkdir(parents=True, exist_ok=True)
    with target_path.open("w", encoding="utf-8") as f:
        f.write(content)


def restore_from_txt(txt_path: Path, base_dir: Path | None = None):
    if not txt_path.is_file():
        print(f" æ‰¾ä¸åˆ° TXT æ–‡ä»¶: {txt_path}")
        sys.exit(1)

    print(f"ğŸ“„ æ­£åœ¨è¯»å–å¿«ç…§ TXT: {txt_path}")

    detected_root: Path | None = None
    current_file: str | None = None
    buffer: list[str] = []
    files_written = 0

    # 1. æ‰«æ Project Root
    with txt_path.open("r", encoding="utf-8") as f:
        for line in f:
            if detected_root is None:
                detected_root = parse_project_root_hint(line)
            if detected_root is not None:
                break

    # 2. ç¡®å®š Base Dir
    if base_dir is None:
        if detected_root is not None:
            base_dir = detected_root
            print(f"ğŸ“ ä½¿ç”¨ TXT è®°å½•çš„æ ¹ç›®å½•: {base_dir}")
        else:
            # [Fix] è„šæœ¬åœ¨ tools/ ä¸‹ï¼Œé»˜è®¤è¿˜åŸåˆ°é¡¹ç›®æ ¹ç›®å½• (tools çš„ä¸Šä¸€çº§)
            base_dir = Path(__file__).resolve().parent.parent
            print(f"âš ï¸ TXT æœªè®°å½• Rootï¼Œé»˜è®¤è¿˜åŸåˆ°: {base_dir}")
    else:
        base_dir = base_dir.resolve()
        print(f"ğŸ“ ä½¿ç”¨æŒ‡å®šæ ¹ç›®å½•: {base_dir}")

    # 3. è§£æå¹¶å†™å…¥
    with txt_path.open("r", encoding="utf-8") as f:
        for raw_line in f:
            line = raw_line.rstrip("\n")

            m_start = START_PATTERN.match(line.strip())
            if m_start:
                if current_file is not None:
                    # å¼ºåˆ¶å†™å…¥ä¸Šä¸€ä¸ªæœªé—­åˆæ–‡ä»¶
                    write_file(base_dir, current_file, "".join(buffer))
                    files_written += 1

                current_file = m_start.group(1).strip()
                buffer = []
                print(f"ğŸŸ¢ æ¢å¤: {current_file}")
                continue

            m_end = END_PATTERN.match(line.strip())
            if m_end and current_file is not None:
                write_file(base_dir, current_file, "".join(buffer))
                files_written += 1
                current_file = None
                buffer = []
                continue

            if current_file is not None:
                buffer.append(raw_line)

    if current_file is not None and buffer:
        write_file(base_dir, current_file, "".join(buffer))
        files_written += 1

    print("=====================================")
    print(f"ğŸ‰ æ¢å¤å®Œæˆï¼Œå…±å†™å…¥ {files_written} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“¦ é¡¹ç›®ä½ç½®: {base_dir}")


def auto_detect_txt_in_root() -> Path | None:
    """
    [Fix] åœ¨é¡¹ç›®æ ¹ç›®å½• (tools/..) å¯»æ‰¾ project_all_code_*.txt
    """
    project_root = Path(__file__).resolve().parent.parent
    # ä¼˜å…ˆæ‰¾ VersionHistory ä¸‹çš„ï¼Œæˆ–è€…æ ¹ç›®å½•ä¸‹çš„
    candidates = sorted(project_root.glob("project_all_code_*.txt"))
    if not candidates:
        # å†è¯•ç€æ‰¾ VersionHistory æ–‡ä»¶å¤¹
        vh_dir = project_root / "VersionHistory"
        if vh_dir.exists():
            candidates = sorted(vh_dir.glob("project_all_code_*.txt"))

    return candidates[-1] if candidates else None  # å–æœ€æ–°çš„


def main():
    if len(sys.argv) >= 2:
        txt_path = Path(sys.argv[1]).expanduser()
        base_dir = Path(sys.argv[2]).expanduser() if len(sys.argv) >= 3 else None
    else:
        txt_path = auto_detect_txt_in_root()
        base_dir = None
        if txt_path is None:
            print(" æœªåœ¨é¡¹ç›®æ ¹ç›®å½•æ‰¾åˆ° project_all_code_*.txt")
            print("   ç”¨æ³•: python tools/restore_project.py <path_to_txt> [target_dir]")
            sys.exit(1)
        print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°æœ€æ–°å¿«ç…§: {txt_path.name}")

    restore_from_txt(txt_path, base_dir)


if __name__ == "__main__":
    main()
==================== END FILE: tools/restore_project.py ====================


==================== START FILE: tools/scan_old_refs.py ====================
# tools/scan_old_refs.py
import os

# è¦æŸ¥æ‰¾çš„æ—§æ–‡ä»¶å
TARGET_STRING = "run_test_app.py"

# å¿½ç•¥çš„ç›®å½• (ä¿ç•™ .vscode å’Œ .idea ä»¥ä¾¿æ£€æŸ¥ IDE é…ç½®)
IGNORE_DIRS = {'.git', '.venv', '__pycache__', 'logs', 'backup', 'VersionHistory'}


def scan_project():
    # è·å–é¡¹ç›®æ ¹ç›®å½• (tools çš„ä¸Šä¸€çº§)
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    print(f"ğŸ” æ­£åœ¨æ‰«æé¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"ğŸ¯ å¯»æ‰¾æ®‹ç•™å¼•ç”¨: '{TARGET_STRING}'\n")

    found_count = 0

    for root, dirs, files in os.walk(project_root):
        # è¿‡æ»¤å¿½ç•¥çš„ç›®å½•
        dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]

        for file in files:
            # è·³è¿‡è„šæœ¬è‡ªå·±
            if file == "scan_old_refs.py": continue

            file_path = os.path.join(root, file)

            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    if TARGET_STRING in content:
                        print(f"ğŸš¨ å‘ç°æ®‹ç•™! -> {os.path.relpath(file_path, project_root)}")
                        # æ‰“å°å…·ä½“è¡Œå†…å®¹
                        f.seek(0)
                        for i, line in enumerate(f):
                            if TARGET_STRING in line:
                                print(f"   Line {i + 1}: {line.strip()}")
                        print("-" * 40)
                        found_count += 1
            except Exception as e:
                # è·³è¿‡äºŒè¿›åˆ¶æ–‡ä»¶æˆ–æ— æ³•è¯»å–çš„æ–‡ä»¶
                pass

    if found_count == 0:
        print(" æ­å–œï¼é¡¹ç›®ä¸­æœªå‘ç°ä»»ä½•æ—§æ–‡ä»¶åå¼•ç”¨ã€‚")
        print("ğŸ‘‰ å¦‚æœä¾ç„¶æŠ¥é”™ï¼Œè¯·æ£€æŸ¥ IDE (PyCharm/VSCode) çš„ 'Run Configuration' è®¾ç½®ã€‚")
    else:
        print(f" å…±å‘ç° {found_count} å¤„æ®‹ç•™ï¼Œè¯·æ‰‹åŠ¨ä¿®æ”¹ã€‚")


if __name__ == "__main__":
    scan_project()
==================== END FILE: tools/scan_old_refs.py ====================


==================== START FILE: tools/check_models.py ====================
# tools/check_models.py
import os
import google.generativeai as genai
from dotenv import load_dotenv

# åŠ è½½ .env ç¯å¢ƒå˜é‡
load_dotenv()

api_key = os.getenv("GOOGLE_API_KEY")

if not api_key:
    print(" é”™è¯¯ï¼šæœªæ‰¾åˆ° GOOGLE_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
else:
    genai.configure(api_key=api_key)
    print("ğŸ” æ­£åœ¨æŸ¥è¯¢å¯ç”¨æ¨¡å‹åˆ—è¡¨...\n")

    try:
        count = 0
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                print(f" æ¨¡å‹åç§°: {m.name}")
                print(f"   æè¿°: {m.description}")
                print("-" * 40)
                count += 1

        if count == 0:
            print("âš ï¸ æœªæ‰¾åˆ°æ”¯æŒ generateContent çš„æ¨¡å‹ã€‚")
    except Exception as e:
        print(f" æŸ¥è¯¢å¤±è´¥: {e}")

==================== END FILE: tools/check_models.py ====================


==================== START FILE: tools/code_backup.py ====================
# tools/code_backup.py
"""
ä»£ç å·¥ç¨‹ä¸€é”®å¤‡ä»½ / è¿˜åŸå·¥å…· (Python Only Logic - V2.1 Fix)

æ ¸å¿ƒé€»è¾‘ï¼š
1. å¤‡ä»½ï¼šå…¨é‡å¤‡ä»½æ‰€æœ‰é¡¹ç›®æ–‡ä»¶ (ZIP + TXT)ã€‚
2. è¿˜åŸï¼šã€ä»…åˆ é™¤ã€‘é¡¹ç›®ä¸­çš„ .py æ–‡ä»¶ï¼Œä¿ç•™æ‰€æœ‰èµ„æº/é…ç½®/æ•°æ®åº“ï¼Œç„¶åè§£å‹è¦†ç›–ã€‚

Fix Log:
- V2.1: ä¿®å¤æ‰«ææ ¹ç›®å½•æ—¶å‡ºç°çš„ IndexError (tuple index out of range) é—®é¢˜ã€‚
"""

import os
import sys
import shutil
import zipfile
from pathlib import Path
from datetime import datetime

# ============= åŸºç¡€è·¯å¾„é…ç½® =============
CURRENT_SCRIPT_PATH = Path(__file__).resolve()
BASE_DIR = CURRENT_SCRIPT_PATH.parent.parent  # é¡¹ç›®æ ¹ç›®å½•
BACKUP_ROOT = BASE_DIR / "backup"
SNAPSHOT_DIR = BACKUP_ROOT / "code_snapshots"

SNAPSHOT_DIR.mkdir(parents=True, exist_ok=True)

# ============= è±å…åå• =============
# å“ªæ€•æ˜¯ .py æ–‡ä»¶ï¼Œå¦‚æœåœ¨è¿™äº›ç›®å½•é‡Œï¼Œä¹Ÿç»å¯¹ä¸åˆ 
EXCLUDE_DIRS = {
    "backup",
    "output",
    "logs",
    "__pycache__",
    ".git",
    ".venv",
    ".idea",
    ".pytest_cache",
    ".vscode",
}


# ============= æ ¸å¿ƒå·¥å…·å‡½æ•° =============

def _print_progress(current: int, total: int, prefix: str = ""):
    if total == 0:
        percent = 100
    else:
        percent = int(current / total * 100)

    bar_len = 30
    filled = int(bar_len * percent / 100)
    bar = "â–ˆ" * filled + "-" * (bar_len - filled)
    print(f"\r{prefix} |{bar}| {percent}%", end="", flush=True)
    if current >= total:
        print()


def get_project_version():
    """è¯»å– patch_notes.txt è·å–ç‰ˆæœ¬å·"""
    patch_file = BASE_DIR / "assets" / "patch_notes.txt"
    version = "Unknown"
    if patch_file.exists():
        try:
            with open(patch_file, "r", encoding="utf-8") as f:
                line = f.readline().strip()
                if line.startswith("VERSION="):
                    version = line.split("=")[1].strip()
        except:
            pass
    return version


def _is_safe_to_touch(path: Path) -> bool:
    """å®‰å…¨æ£€æŸ¥ï¼šè¿”å› False ä»£è¡¨ç»å¯¹ç¦æ­¢æ“ä½œ"""
    # 1. ä¿æŠ¤å¤‡ä»½ç›®å½•
    if BACKUP_ROOT in path.parents or path == BACKUP_ROOT:
        return False

    # 2. ä¿æŠ¤è„šæœ¬è‡ªå·±
    if path.resolve() == CURRENT_SCRIPT_PATH:
        return False

    # 3. ä¿æŠ¤æ’é™¤ç›®å½•
    try:
        rel = path.relative_to(BASE_DIR)
        # [Fix] å¢åŠ é•¿åº¦æ£€æŸ¥ï¼Œé˜²æ­¢æ ¹ç›®å½•(.)çš„ parts ä¸ºç©ºæ—¶æŠ¥é”™
        if len(rel.parts) > 0 and rel.parts[0] in EXCLUDE_DIRS:
            return False
    except ValueError:
        return False

    return True


def _collect_project_files():
    """æ”¶é›†ç”¨äºå¤‡ä»½çš„æ–‡ä»¶ (å…¨é‡)"""
    files = []
    print("ğŸ” æ‰«æé¡¹ç›®æ–‡ä»¶...")
    for root, dirs, filenames in os.walk(BASE_DIR):
        root_path = Path(root)

        # å‰ªæç›®å½•
        # [Fix] å¢åŠ  len æ£€æŸ¥
        try:
            rel_root = root_path.relative_to(BASE_DIR)
            if len(rel_root.parts) > 0 and rel_root.parts[0] in EXCLUDE_DIRS:
                dirs[:] = []
                continue
        except ValueError:
            pass

        if not _is_safe_to_touch(root_path):
            dirs[:] = []
            continue

        for name in filenames:
            fpath = root_path / name
            # æ’é™¤ä¸´æ—¶æ–‡ä»¶
            if fpath.name == ".DS_Store" or fpath.suffix == ".pyc":
                continue
            if _is_safe_to_touch(fpath):
                files.append(fpath)
    return files


# ============= åŠŸèƒ½ 1: å¤‡ä»½ (ä¿æŒå…¨é‡) =============

def create_text_dump(files: list, timestamp: str, label: str = ""):
    version = get_project_version()
    tag = f"_{label}" if label else ""
    txt_name = f"project_code_{version}_{timestamp}{tag}.txt"
    txt_path = SNAPSHOT_DIR / txt_name

    print(f"ğŸ“„ ç”Ÿæˆ TXT å¤‡ä»½: {txt_name}")
    try:
        with open(txt_path, "w", encoding="utf-8") as f:
            f.write(f"Version: {version}\nTime: {timestamp}\nFiles: {len(files)}\n\n")

            # ä¼˜å…ˆæ”¾ patch_notes
            sorted_files = sorted(files, key=lambda x: 0 if "patch_notes.txt" in x.name else 1)

            for i, fpath in enumerate(sorted_files):
                # è·³è¿‡éæ–‡æœ¬æ–‡ä»¶
                if fpath.suffix.lower() in ['.png', '.jpg', '.zip', '.pdf', '.pyc', '.mp4']: continue

                rel = fpath.relative_to(BASE_DIR)
                f.write(f"{'=' * 20} START FILE: {rel} {'=' * 20}\n")
                try:
                    with open(fpath, "r", encoding="utf-8", errors="ignore") as rf:
                        f.write(rf.read())
                except Exception as e:
                    f.write(f"[Error: {e}]")
                f.write(f"\n{'=' * 20} END FILE: {rel} {'=' * 20}\n\n")
                _print_progress(i + 1, len(files), "ğŸ“„ TXT")
    except Exception as e:
        print(f" TXT ç”Ÿæˆå¤±è´¥: {e}")


def create_snapshot(label: str = ""):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    tag = f"_{label}" if label else ""
    zip_name = f"code_snapshot_{timestamp}{tag}.zip"
    zip_path = SNAPSHOT_DIR / zip_name

    files = _collect_project_files()
    if not files:
        print(" æœªæ‰¾åˆ°æ–‡ä»¶")
        return

    print(f"ğŸ“¦ ç”Ÿæˆ ZIP å¿«ç…§: {zip_name}")
    try:
        with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
            for i, fpath in enumerate(files):
                rel = fpath.relative_to(BASE_DIR)
                zf.write(fpath, arcname=str(rel))
                _print_progress(i + 1, len(files), "ğŸ“¦ ZIP")
    except Exception as e:
        print(f" ZIP å¤±è´¥: {e}")
        return

    create_text_dump(files, timestamp, label)
    print(f"\n å¤‡ä»½å®Œæˆ: {zip_name}")


# ============= åŠŸèƒ½ 2: è¿˜åŸ (ä»…æ¸…æ´— Py) =============

def _cleanup_python_only():
    """
    ã€å®‰å…¨è¿˜åŸç­–ç•¥ã€‘
    åªåˆ é™¤ .py æ–‡ä»¶ï¼Œä¿ç•™å…¶ä»–æ‰€æœ‰æ–‡ä»¶ï¼ˆå›¾ç‰‡ã€é…ç½®ã€æ•°æ®ç­‰ï¼‰ã€‚
    """
    print("ğŸ§¹ æ­£åœ¨æ¸…ç†æ—§ä»£ç  (ä»…åˆ é™¤ .py æ–‡ä»¶)...")

    files_to_del = []

    for root, dirs, filenames in os.walk(BASE_DIR):
        root_path = Path(root)

        # è·³è¿‡ä¿æŠ¤ç›®å½•
        if not _is_safe_to_touch(root_path):
            dirs[:] = []  # åœæ­¢è¿›å…¥å­ç›®å½•
            continue

        for name in filenames:
            fpath = root_path / name

            # === æ ¸å¿ƒé€»è¾‘: åªåˆ  .py ===
            if fpath.suffix == '.py':
                # å†æ¬¡ç¡®è®¤ä¸æ˜¯è„šæœ¬è‡ªå·±
                if _is_safe_to_touch(fpath):
                    files_to_del.append(fpath)

    total = len(files_to_del)
    for i, fpath in enumerate(files_to_del):
        try:
            os.remove(fpath)
        except Exception:
            pass
        _print_progress(i + 1, total, "ğŸ§¹ æ¸…ç† Py")

    print("\n æ—§ä»£ç æ¸…ç†å®Œæ¯• (èµ„æºæ–‡ä»¶å·²ä¿ç•™)ã€‚")


def restore_snapshot():
    zips = sorted(SNAPSHOT_DIR.glob("*.zip"), key=lambda x: x.stat().st_mtime)
    if not zips:
        print(" æ— å¤‡ä»½")
        return

    print("ğŸ“œ å¯ç”¨å¤‡ä»½:")
    for i, f in enumerate(zips):
        mb = f.stat().st_size / (1024 * 1024)
        print(f"[{i + 1}] {f.name} ({mb:.2f}MB)")

    idx_str = input("\nè¯·è¾“å…¥åºå· (å›è½¦å–æ¶ˆ): ")
    if not idx_str.isdigit(): return
    idx = int(idx_str) - 1
    if idx < 0 or idx >= len(zips): return
    target_zip = zips[idx]

    print(f"\nâš ï¸  å‡†å¤‡è¿˜åŸ: {target_zip.name}")
    print("ğŸ‘‰ ç­–ç•¥: ä»…è¦†ç›–ä»£ç ï¼Œä¿ç•™æ•°æ®ã€‚")
    if input("ç¡®è®¤è¾“å…¥ YES: ").strip() != "YES": return

    print("\nğŸ›¡ï¸  Step 1: è‡ªåŠ¨å¤‡ä»½å½“å‰çŠ¶æ€...")
    create_snapshot(label="pre_restore")

    print("\nğŸ§¹ Step 2: æ¸…ç†æ—§ Python æ–‡ä»¶...")
    _cleanup_python_only()

    print("\nğŸ“¦ Step 3: è§£å‹è¦†ç›–...")
    try:
        with zipfile.ZipFile(target_zip, 'r') as zf:
            for member in zf.infolist():
                if ".." in member.filename: continue
                # å¦‚æœå¤‡ä»½åŒ…é‡Œæœ‰ code_backup.pyï¼Œè·³è¿‡è¦†ç›–è‡ªå·±
                if "code_backup.py" in member.filename:
                    if (BASE_DIR / member.filename).resolve() == CURRENT_SCRIPT_PATH:
                        print("   ğŸ›¡ï¸ è·³è¿‡è¦†ç›–æœ¬è„šæœ¬")
                        continue
                zf.extract(member, BASE_DIR)
        print("\nğŸ‰ è¿˜åŸæˆåŠŸï¼")
    except Exception as e:
        print(f" è¿˜åŸå‡ºé”™: {e}")


if __name__ == "__main__":
    if len(sys.argv) > 1:
        cmd = sys.argv[1].lower()
        if cmd == "backup":
            create_snapshot()
        elif cmd == "restore":
            restore_snapshot()
        else:
            print("Args: backup | restore")
    else:
        print("å¤‡ä»½: python tools/code_backup.py backup")
        print("æ¢å¤: python tools/code_backup.py restore")
==================== END FILE: tools/code_backup.py ====================


==================== START FILE: tools/fix_date_format.py.py ====================
# tools/fix_date_format.py
"""
æ•°æ®åº“ç»´æŠ¤è„šæœ¬ï¼šç»Ÿä¸€ Data_Clean_Log æ—¥æœŸæ ¼å¼
Safe Version: Uses DBClient (No hardcoded passwords)
"""
import sys
import os
from pathlib import Path
import pandas as pd
from sqlalchemy import text
from tqdm import tqdm

# ==============================================================================
# 1. ç¯å¢ƒå¼•å¯¼ (Bootstrap)
# ç¡®ä¿èƒ½å¯¼å…¥é¡¹ç›®æ ¹ç›®å½•çš„ core æ¨¡å—
# ==============================================================================
current_dir = Path(__file__).resolve().parent  # tools/
project_root = current_dir.parent  # root/
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

from core.repository.db_client import DBClient

# ==============================================================================
# 2. é…ç½®ä¸é€»è¾‘
# ==============================================================================
TABLE_NAME = "Data_Clean_Log"
CHUNK_SIZE = 5000

DATE_FORMATS = [
    '%Y-%m-%d',  # æ ‡å‡†æ ¼å¼
    '%b %d, %Y',  # æ—§ eBay/è‹±æ–‡ç¼©å†™æ ¼å¼ (å¦‚: Jan 15, 2023)
    '%m/%d/%Y',  # å¸¸è§ç¾å¼æ ¼å¼
    '%Y%m%d',  # æ— åˆ†éš”ç¬¦æ ¼å¼
]


def unify_date_format_optimized():
    """
    ç»Ÿä¸€ Data_Clean_Log è¡¨ä¸­ order date å­—æ®µä¸ºæ ‡å‡†çš„ YYYY-MM-DD æ ¼å¼ã€‚
    ç­–ç•¥ï¼šå…¨è¡¨è¯»å– -> å†…å­˜æ¸…æ´— -> æ¸…ç©ºæ•°æ®åº“ -> åˆ†å—å†™å›ã€‚
    """
    print(f"ğŸš€ [Tool] å¼€å§‹è¿æ¥æ•°æ®åº“å¹¶è¯»å– {TABLE_NAME}...")

    # [Security Fix] ä½¿ç”¨ç»Ÿä¸€çš„ DBClient è·å–å¼•æ“ï¼Œå¤ç”¨ .env é…ç½®
    engine = DBClient.get_engine()

    try:
        # 1. è¯»å–å…¨è¡¨æ•°æ®
        df = pd.read_sql(f"SELECT * FROM `{TABLE_NAME}`", engine)

        if df.empty or 'order date' not in df.columns:
            print("âš ï¸ è¡¨æ ¼ä¸ºç©ºæˆ–ç¼ºå¤± 'order date' åˆ—ï¼Œè·³è¿‡ç»´æŠ¤ã€‚")
            return

        original_count = len(df)
        print(f"ğŸ“Š å·²åŠ è½½ {original_count} è¡Œæ•°æ®ã€‚")
        print("ğŸ”§ æ­£åœ¨æ‰§è¡Œå†…å­˜ä¸­çš„å¤šé‡æ ¼å¼è§£æ...")

        # 2. å†…å­˜æ¸…æ´—
        df['cleaned_date'] = pd.NaT

        for fmt in tqdm(DATE_FORMATS, desc="æ ¼å¼å°è¯•è§£æ", unit="æ ¼å¼"):
            failed_mask = df['cleaned_date'].isna()
            if not failed_mask.any():
                break

            df.loc[failed_mask, 'cleaned_date'] = pd.to_datetime(
                df.loc[failed_mask, 'order date'],
                format=fmt,
                errors='coerce'
            )

        # å…œåº•ï¼šè‡ªåŠ¨æ¨æ–­
        failed_mask_final = df['cleaned_date'].isna()
        if failed_mask_final.any():
            df.loc[failed_mask_final, 'cleaned_date'] = pd.to_datetime(
                df.loc[failed_mask_final, 'order date'],
                errors='coerce'
            )

        # 3. å‡†å¤‡å›å†™
        valid_mask = df['cleaned_date'].notna()
        cleaned_count = valid_mask.sum()

        if cleaned_count == 0:
            print(" è½¬æ¢å¤±è´¥ï¼šæ²¡æœ‰ä¸€è¡Œæ—¥æœŸèƒ½è¢«æ­£ç¡®è§£æã€‚ä¸­æ­¢æ“ä½œã€‚")
            return

        df_cleaned = df[valid_mask].copy()
        df_cleaned['order date'] = df_cleaned['cleaned_date'].dt.strftime('%Y-%m-%d')

        cols_to_keep = [col for col in df.columns if col != 'cleaned_date']
        final_df = df_cleaned[cols_to_keep]

        print(f"\n æˆåŠŸè§£æ {cleaned_count} / {original_count} è¡Œã€‚å¼€å§‹æ¸…ç©ºåŸè¡¨å¹¶åˆ†å—å›å†™...")

        # 4. å†™å…¥æ•°æ®åº“
        with engine.begin() as conn:
            conn.execute(text(f"TRUNCATE TABLE `{TABLE_NAME}`"))
            print(f"ğŸ§¹ å·²æ¸…ç©ºè¡¨ `{TABLE_NAME}`ã€‚")

            total_chunks = (len(final_df) + CHUNK_SIZE - 1) // CHUNK_SIZE
            written_rows = 0

            with tqdm(total=len(final_df), desc="æ•°æ®åº“å†™å…¥è¿›åº¦", unit="è¡Œ") as pbar:
                for i in range(total_chunks):
                    start = i * CHUNK_SIZE
                    end = start + CHUNK_SIZE
                    chunk_df = final_df.iloc[start:end]

                    chunk_df.to_sql(
                        TABLE_NAME,
                        conn,
                        if_exists='append',
                        index=False,
                        chunksize=CHUNK_SIZE
                    )
                    pbar.update(len(chunk_df))
                    written_rows += len(chunk_df)

        print(f"\nğŸ‰ ç»´æŠ¤å®Œæˆã€‚å…±è®¡æ¸…æ´—å¹¶å›å†™ {written_rows} æ¡è®°å½•åˆ° YYYY-MM-DD æ ¼å¼ã€‚")

    except Exception as e:
        print(f" æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    unify_date_format_optimized()
==================== END FILE: tools/fix_date_format.py.py ====================


==================== START FILE: tools/add_indices_manual.py ====================
# tools/add_indices_manual.py
import sys
import os
from pathlib import Path
from sqlalchemy import text

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ pythonpathï¼Œç¡®ä¿èƒ½å¯¼å…¥ core æ¨¡å—
current_dir = Path(__file__).resolve().parent
project_root = current_dir.parent
sys.path.append(str(project_root))

from core.repository.db_client import DBClient
from core.logging_config import get_logger

logger = get_logger("DB_Migration")


def add_indices_to_existing_table():
    table_name = "Data_Clean_Log"

    print(f"ğŸ› ï¸ æ­£åœ¨ä¸ºç°æœ‰è¡¨ `{table_name}` æ·»åŠ é«˜æ€§èƒ½ç´¢å¼•...")

    # å®šä¹‰éœ€è¦æ·»åŠ çš„ç´¢å¼•
    # æ ¼å¼: (ç´¢å¼•å, SQLå®šä¹‰)
    indices = [
        ("idx_order_date", f"CREATE INDEX idx_order_date ON `{table_name}` (`order date`(20))"),
        ("idx_full_sku", f"CREATE INDEX idx_full_sku ON `{table_name}` (`full sku`(50))"),
        ("idx_seller", f"CREATE INDEX idx_seller ON `{table_name}` (`seller`(20))"),
        ("idx_action", f"CREATE INDEX idx_action ON `{table_name}` (`action`(10))")
    ]

    success_count = 0
    skip_count = 0

    # è·å–æ•°æ®åº“è¿æ¥
    # æ³¨æ„ï¼šDDL (Data Definition Language) è¯­å¥é€šå¸¸ä¼šè‡ªåŠ¨æäº¤
    engine = DBClient.get_engine()

    with engine.connect() as conn:
        for idx_name, sql in indices:
            try:
                print(f"   ... å°è¯•åˆ›å»ºç´¢å¼•: {idx_name}")
                conn.execute(text(sql))
                print(f"    {idx_name} åˆ›å»ºæˆåŠŸ")
                success_count += 1
            except Exception as e:
                # MySQL é”™è¯¯ç  1061: Duplicate key name (ç´¢å¼•å·²å­˜åœ¨)
                err_msg = str(e).lower()
                if "duplicate key" in err_msg or "already exists" in err_msg:
                    print(f"   âš ï¸ è·³è¿‡: ç´¢å¼• {idx_name} å·²å­˜åœ¨")
                    skip_count += 1
                else:
                    print(f"    åˆ›å»ºå¤±è´¥ {idx_name}: {e}")

    print("-" * 40)
    print(f"ğŸ‰ è¿ç§»å®Œæˆï¼æˆåŠŸ: {success_count}, è·³è¿‡: {skip_count}")
    print("ç°åœ¨æ‚¨çš„å†å²æ•°æ®æŸ¥è¯¢å°†äº«å—æ¯«ç§’çº§å“åº”ã€‚")


if __name__ == "__main__":
    # åŒé‡ç¡®è®¤
    print("âš ï¸ æ­¤æ“ä½œå°†ç›´æ¥ä¿®æ”¹æ•°æ®åº“ç»“æ„ã€‚")
    confirm = input("ç¡®è®¤æ‰§è¡Œå—? (è¾“å…¥ y ç»§ç»­): ")
    if confirm.lower() == 'y':
        add_indices_to_existing_table()
    else:
        print("æ“ä½œå·²å–æ¶ˆã€‚")
==================== END FILE: tools/add_indices_manual.py ====================


==================== START FILE: tools/pack_project.py ====================
# tools/pack_project.py
import os
import datetime

# === é…ç½®åŒºåŸŸ ===
EXTENSIONS = ['.py', '.md', '.json', '.sh', '.env']
PATCH_NOTE_FILENAME = 'patch_notes.txt'
POSSIBLE_PATHS = ['assets/patch_notes.txt', 'patch_notes.txt']

IGNORE_DIRS = {
    '.venv', '.git', '__pycache__', '.idea', '.vscode',
    'Backup', 'VersionHistory', 'Output', 'Archived', 'Temp',
    'Eaglestar ERP-darwin-arm64', 'logs', 'uploader'
}

OUTPUT_DIR_NAME = 'VersionHistory'


def get_project_root():
    """è·å–é¡¹ç›®æ ¹ç›®å½• (å³ tools çš„ä¸Šä¸€çº§)"""
    current_script = os.path.abspath(__file__)
    return os.path.dirname(os.path.dirname(current_script))


def get_version_info(project_root):
    target_path = None
    # åœ¨æ ¹ç›®å½•ä¸‹å¯»æ‰¾ patch notes
    for rel_path in POSSIBLE_PATHS:
        full_path = os.path.join(project_root, rel_path)
        if os.path.exists(full_path):
            target_path = full_path
            break

    ver = "Unknown_Version"
    if target_path:
        try:
            with open(target_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.startswith("VERSION="):
                        ver = line.split("=")[1].strip()
                        break
        except:
            pass
    else:
        ver = datetime.datetime.now().strftime("Date_%Y%m%d")

    return ver, target_path


def pack_project():
    project_root = get_project_root()
    version, patch_note_path = get_version_info(project_root)

    # è¾“å‡ºç›®å½•åœ¨æ ¹ç›®å½•ä¸‹
    output_dir = os.path.join(project_root, OUTPUT_DIR_NAME)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    output_filename = f"project_all_code_{version}.txt"
    output_file_path = os.path.join(output_dir, output_filename)

    print(f"ğŸ“¦ æ‰“åŒ…èŒƒå›´: {project_root}")
    print(f"ğŸ“„ ç›®æ ‡æ–‡ä»¶: {output_file_path}")

    files_processed = 0

    with open(output_file_path, 'w', encoding='utf-8') as outfile:
        outfile.write(f"Project Root: {project_root}\n")
        outfile.write(f"Version: {version}\n")
        outfile.write(f"Generated at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        outfile.write("=" * 50 + "\n\n")

        # ä¼˜å…ˆå†™å…¥ Patch Note
        if patch_note_path:
            try:
                rel_path = os.path.relpath(patch_note_path, project_root)
                with open(patch_note_path, 'r', encoding='utf-8') as pf:
                    content = pf.read()
                    outfile.write(f"\n{'=' * 20} START SPECIAL FILE: {rel_path} {'=' * 20}\n")
                    outfile.write(content)
                    outfile.write(f"\n{'=' * 20} END SPECIAL FILE: {rel_path} {'=' * 20}\n\n")
            except Exception as e:
                print(f"âš ï¸ è¯»å– Patch Note å¤±è´¥: {e}")

        # éå†æ–‡ä»¶
        for root, dirs, files in os.walk(project_root):
            # ä¿®æ”¹ dirs åˆ—è¡¨ä»¥è¿‡æ»¤æ–‡ä»¶å¤¹
            dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]

            for file in files:
                if file == output_filename: continue  # è·³è¿‡è‡ªå·±

                # è·³è¿‡ patch note (å·²å†™)
                full_path = os.path.join(root, file)
                if patch_note_path and os.path.abspath(full_path) == os.path.abspath(patch_note_path):
                    continue

                if any(file.endswith(ext) for ext in EXTENSIONS):
                    rel_path = os.path.relpath(full_path, project_root)

                    try:
                        with open(full_path, 'r', encoding='utf-8') as infile:
                            content = infile.read()
                        outfile.write(f"\n{'=' * 20} START FILE: {rel_path} {'=' * 20}\n")
                        outfile.write(content)
                        outfile.write(f"\n{'=' * 20} END FILE: {rel_path} {'=' * 20}\n\n")
                        files_processed += 1
                        print(f"å·²æ‰“åŒ…: {rel_path}")
                    except Exception as e:
                        print(f"âš ï¸ è¯»å–å¤±è´¥: {rel_path} - {e}")

    print("-" * 50)
    print(f" æ‰“åŒ…å®Œæˆï¼å…± {files_processed} ä¸ªæ–‡ä»¶ã€‚")


if __name__ == '__main__':
    pack_project()
==================== END FILE: tools/pack_project.py ====================


==================== START FILE: core/auth_service.py ====================
import os
from dataclasses import dataclass
from typing import Optional, Dict, Tuple, Any
import pandas as pd

from config import Config
from core.repository.db_client import DBClient
from core.logging_config import get_logger
from core.security import SecurityUtils


@dataclass
class User:
    username: str
    is_admin: bool
    is_locked: bool
    failed_attempts: int
    session_token: str = ""


class AuthService:
    """è®¤è¯ä¸ç”¨æˆ·ç®¡ç†æœåŠ¡ (ä¸šåŠ¡é€»è¾‘å±‚)"""

    USER_TABLE = "User_Account"
    PERM_TABLE = "User_Permission"
    LOGIN_HISTORY_TABLE = "User_Login_History"
    MAX_FAILED_ATTEMPTS = 10

    _logger = get_logger(__name__)

    @classmethod
    def initialize(cls) -> None:
        """åˆå§‹åŒ–è®¤è¯æœåŠ¡ï¼šæ£€æŸ¥Schemaï¼Œåˆ›å»ºé»˜è®¤è´¦å·"""
        cls._ensure_schema()
        cls._bootstrap_default_users()

    @classmethod
    def _ensure_schema(cls) -> None:
        # 1. ç¡®ä¿ç”¨æˆ·è¡¨å­˜åœ¨
        sql_user = f"CREATE TABLE IF NOT EXISTS `{cls.USER_TABLE}` (id INT AUTO_INCREMENT PRIMARY KEY, username VARCHAR(64) NOT NULL UNIQUE, password_hash VARCHAR(255) NOT NULL, is_admin TINYINT(1) DEFAULT 0, is_locked TINYINT(1) DEFAULT 0, failed_attempts INT DEFAULT 0, session_token VARCHAR(64), created_at DATETIME DEFAULT CURRENT_TIMESTAMP, updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP) ENGINE=InnoDB DEFAULT CHARSET={Config.DB_CHARSET};"
        DBClient.execute_stmt(sql_user)

        # è‡ªåŠ¨è¿ç§»ï¼šæ£€æŸ¥ session_token
        try:
            check_col = f"SHOW COLUMNS FROM `{cls.USER_TABLE}` LIKE 'session_token'"
            if DBClient.read_df(check_col).empty:
                cls._logger.info(f"å‡çº§Schema: ä¸º {cls.USER_TABLE} æ·»åŠ  session_token å­—æ®µ")
                DBClient.execute_stmt(
                    f"ALTER TABLE `{cls.USER_TABLE}` ADD COLUMN session_token VARCHAR(64) DEFAULT NULL")
        except Exception as e:
            cls._logger.error(f"Schema è‡ªåŠ¨è¿ç§»è­¦å‘Š: {e}")

        # 2. ç¡®ä¿æƒé™è¡¨å­˜åœ¨
        sql_perm = f"CREATE TABLE IF NOT EXISTS `{cls.PERM_TABLE}` (id INT AUTO_INCREMENT PRIMARY KEY, username VARCHAR(64) NOT NULL, permission_key VARCHAR(128) NOT NULL, allowed TINYINT(1) DEFAULT 1, created_at DATETIME DEFAULT CURRENT_TIMESTAMP, UNIQUE KEY uniq_user_perm (username, permission_key)) ENGINE=InnoDB DEFAULT CHARSET={Config.DB_CHARSET};"
        DBClient.execute_stmt(sql_perm)

        # 3. ç¡®ä¿å†å²è¡¨å­˜åœ¨
        sql_hist = f"CREATE TABLE IF NOT EXISTS `{cls.LOGIN_HISTORY_TABLE}` (id INT AUTO_INCREMENT PRIMARY KEY, username VARCHAR(64) NOT NULL, ip_address VARCHAR(45) NOT NULL, login_at DATETIME DEFAULT CURRENT_TIMESTAMP, INDEX idx_user (username)) ENGINE=InnoDB DEFAULT CHARSET={Config.DB_CHARSET};"
        DBClient.execute_stmt(sql_hist)

    @classmethod
    def _bootstrap_default_users(cls) -> None:
        """
        [Security Fix] ä»…å½“ç¯å¢ƒå˜é‡æä¾›æ—¶æ‰åˆ›å»ºåˆå§‹è´¦å·
        ä¸å†ä½¿ç”¨ç¡¬ç¼–ç çš„åé—¨å¯†ç ã€‚
        """
        # 1. ç®¡ç†å‘˜ bootstrap
        au = os.getenv("DEFAULT_ADMIN_USERNAME", "admin")
        ap = os.getenv("DEFAULT_ADMIN_PASSWORD")  # ç§»é™¤é»˜è®¤å€¼ "1522P"

        if ap:
            cls._ensure_user(au, ap, True)
        else:
            # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œä¸”æ²¡æœ‰å¯†ç ï¼Œè¿™æ˜¯ä¸€ä¸ªæ½œåœ¨é£é™©ï¼Œè®°å½•æ—¥å¿—
            # ä½†ä¸å¼ºåˆ¶æŠ¥é”™ï¼Œå› ä¸ºç”Ÿäº§ç¯å¢ƒå¯èƒ½å·²ç»æœ‰æ•°æ®äº†ï¼Œä¸éœ€è¦åå¤ bootstrap
            pass

        # 2. æ™®é€šç”¨æˆ· bootstrap
        nu = os.getenv("DEFAULT_USER_USERNAME", "user")
        np = os.getenv("DEFAULT_USER_PASSWORD")  # ç§»é™¤é»˜è®¤å€¼ "6M130"

        if np:
            cls._ensure_user(nu, np, False)

    @classmethod
    def _ensure_user(cls, u, p, is_admin):
        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å­˜åœ¨
        df = DBClient.read_df(f"SELECT id, is_admin FROM `{cls.USER_TABLE}` WHERE username=:u", {"u": u})

        if df.empty:
            cls._logger.info(f"Bootstrap: åˆ›å»ºåˆå§‹ç”¨æˆ· {u}")
            ph = SecurityUtils.hash_password(p)
            sql = f"INSERT INTO `{cls.USER_TABLE}` (username, password_hash, is_admin) VALUES (:u, :ph, :a)"
            DBClient.execute_stmt(sql, {"u": u, "ph": ph, "a": 1 if is_admin else 0})
        else:
            # å¦‚æœç”¨æˆ·å·²å­˜åœ¨ï¼Œä»…æ›´æ–°æƒé™ï¼Œä¸é‡ç½®å¯†ç ï¼ˆé˜²æ­¢è¦†ç›–ç”¨æˆ·ä¿®æ”¹åçš„å¯†ç ï¼‰
            current_role = bool(df.iloc[0]["is_admin"])
            if current_role != is_admin:
                cls._logger.info(f"Bootstrap: ä¿®æ­£ç”¨æˆ· {u} æƒé™")
                DBClient.execute_stmt(f"UPDATE `{cls.USER_TABLE}` SET is_admin=:a WHERE username=:u",
                                      {"u": u, "a": 1 if is_admin else 0})

    @classmethod
    def authenticate(cls, username: str, password: str, ip: str = "-") -> Tuple[bool, Optional[User], str]:
        u = username.strip()
        if not u or not password: return False, None, "è¯·è¾“å…¥è´¦å·å¯†ç "

        df = DBClient.read_df(f"SELECT * FROM `{cls.USER_TABLE}` WHERE username=:u LIMIT 1", {"u": u})

        if df.empty:
            # é˜²æ­¢æ—¶åºæ”»å‡» (Timing Attack) çš„è™šå‡æ ¡éªŒ
            SecurityUtils.verify_password("dummy", SecurityUtils.hash_password("dummy"))
            cls._logger.warning(f"ç”¨æˆ·ä¸å­˜åœ¨: {u}", extra={"action": "LOGIN_FAIL", "ip": ip})
            return False, None, "ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯"

        row = df.iloc[0]
        token = row.get("session_token") or ""
        user = User(row["username"], bool(row["is_admin"]), bool(row["is_locked"]), int(row["failed_attempts"]), token)

        if user.is_locked:
            cls._logger.warning(f"é”å®šè´¦å·å°è¯•ç™»å½•: {u}", extra={"action": "LOGIN_LOCKED", "ip": ip})
            return False, None, "è´¦å·å·²é”å®š"

        if not SecurityUtils.verify_password(password, row["password_hash"]):
            new_f = user.failed_attempts + 1
            is_l = 1 if new_f >= cls.MAX_FAILED_ATTEMPTS else 0
            DBClient.execute_stmt(f"UPDATE `{cls.USER_TABLE}` SET failed_attempts=:f, is_locked=:l WHERE username=:u",
                                  {"f": new_f, "l": is_l, "u": u})
            cls._logger.warning(f"å¯†ç é”™è¯¯ ({new_f}æ¬¡): {u}", extra={"action": "LOGIN_FAIL", "ip": ip})
            return False, None, "ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯"

        # ç™»å½•æˆåŠŸï¼Œé‡ç½®å¤±è´¥æ¬¡æ•°
        DBClient.execute_stmt(f"UPDATE `{cls.USER_TABLE}` SET failed_attempts=0 WHERE username=:u", {"u": u})
        cls._logger.info(f"ç™»å½•éªŒè¯é€šè¿‡: {u}", extra={"action": "AUTH_SUCCESS", "user": u, "ip": ip})
        return True, user, "ç™»å½•æˆåŠŸ"

    @classmethod
    def refresh_session_token(cls, username: str) -> str:
        token = SecurityUtils.generate_token()
        DBClient.execute_stmt(f"UPDATE `{cls.USER_TABLE}` SET session_token=:t WHERE username=:u",
                              {"t": token, "u": username})
        return token

    @classmethod
    def verify_session_token(cls, username: str, local_token: str) -> bool:
        if not username or not local_token: return False
        try:
            df = DBClient.read_df(f"SELECT session_token FROM `{cls.USER_TABLE}` WHERE username=:u", {"u": username})
            if df.empty: return False
            db_token = df.iloc[0]["session_token"]
            # å¤„ç†æ•°æ®åº“ä¸­ token ä¸º NULL çš„æƒ…å†µ
            if not db_token: return False
            return db_token == local_token
        except:
            return False

    @classmethod
    def record_login_event(cls, username: str, ip: str) -> None:
        DBClient.execute_stmt(f"INSERT INTO `{cls.LOGIN_HISTORY_TABLE}` (username, ip_address) VALUES (:u, :ip)",
                              {"u": username, "ip": ip})

    @classmethod
    def get_user_login_stats(cls, username: str) -> Dict[str, Any]:
        cnt_df = DBClient.read_df(f"SELECT COUNT(*) FROM `{cls.LOGIN_HISTORY_TABLE}` WHERE username=:u",
                                  {"u": username})
        cnt = cnt_df.iloc[0, 0] if not cnt_df.empty else 0

        hist = DBClient.read_df(
            f"SELECT ip_address, COUNT(*) as count, MAX(login_at) as last_seen FROM `{cls.LOGIN_HISTORY_TABLE}` WHERE username=:u GROUP BY ip_address ORDER BY count DESC",
            {"u": username})
        return {"total_logins": int(cnt), "ip_history": hist}

    @classmethod
    def list_users(cls) -> pd.DataFrame:
        return DBClient.read_df(
            f"SELECT username, is_admin, is_locked, failed_attempts FROM `{cls.USER_TABLE}` ORDER BY is_admin DESC")

    @classmethod
    def create_user(cls, u, p, is_admin=False) -> Tuple[bool, str]:
        if not DBClient.read_df(f"SELECT 1 FROM `{cls.USER_TABLE}` WHERE username=:u", {"u": u}).empty:
            return False, "ç”¨æˆ·å·²å­˜åœ¨"
        ph = SecurityUtils.hash_password(p)
        DBClient.execute_stmt(
            f"INSERT INTO `{cls.USER_TABLE}` (username, password_hash, is_admin) VALUES (:u, :ph, :a)",
            {"u": u, "ph": ph, "a": 1 if is_admin else 0})
        return True, "åˆ›å»ºæˆåŠŸ"

    @classmethod
    def reset_password(cls, u, np) -> Tuple[bool, str]:
        ph = SecurityUtils.hash_password(np)
        row = DBClient.execute_stmt(
            f"UPDATE `{cls.USER_TABLE}` SET password_hash=:ph, failed_attempts=0 WHERE username=:u", {"ph": ph, "u": u})
        return (True, "é‡ç½®æˆåŠŸ") if row else (False, "ç”¨æˆ·ä¸å­˜åœ¨")

    @classmethod
    def set_lock_state(cls, u, lock) -> Tuple[bool, str]:
        row = DBClient.execute_stmt(f"UPDATE `{cls.USER_TABLE}` SET is_locked=:l WHERE username=:u",
                                    {"l": 1 if lock else 0, "u": u})
        return (True, "æ›´æ–°æˆåŠŸ") if row else (False, "ç”¨æˆ·ä¸å­˜åœ¨")

    @classmethod
    def set_permissions(cls, u, pmap):
        DBClient.execute_stmt(f"DELETE FROM `{cls.PERM_TABLE}` WHERE username=:u", {"u": u})
        vals = ", ".join([f"(:u, :k{i}, 1)" for i in range(len(pmap))])
        if vals: DBClient.execute_stmt(
            f"INSERT INTO `{cls.PERM_TABLE}` (username, permission_key, allowed) VALUES {vals}",
            {"u": u, **{f"k{i}": k for i, k in enumerate(pmap)}})

    @classmethod
    def get_permissions(cls, u) -> Dict[str, bool]:
        df = DBClient.read_df(f"SELECT permission_key FROM `{cls.PERM_TABLE}` WHERE username=:u AND allowed=1",
                              {"u": u})
        return {r["permission_key"]: True for _, r in df.iterrows()}
==================== END FILE: core/auth_service.py ====================


==================== START FILE: core/logging_config.py ====================
# core/logging_config.py

"""
ç»Ÿä¸€æ—¥å¿—é…ç½®ä¸­å¿ƒ (Enterprise Logging Center)

ä¿®æ”¹è¯´æ˜:
- å¢å¼º UserLoggerAdapterï¼Œç¡®ä¿ä» core/context ä¸­è·å–æœ€æ–°çš„ user å’Œ ip å­—æ®µã€‚
"""

import logging
import logging.handlers
from pathlib import Path
from typing import Optional, Dict, Any

from config import Config
from core.context import get_context  # <-- å¯¼å…¥ get_context


# ==============================================================================
# 1. æ‰©å±•å­—æ®µå®‰å…¨å…œåº• Filter
# ==============================================================================

class SafeExtraFilter(logging.Filter):
    """
    ç¡®ä¿æ—¥å¿—è®°å½•å¯¹è±¡ä¸€å®šåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
    - user, action, table, rows, ip, module_name
    """

    DEFAULTS: Dict[str, Any] = {
        "user": "-",
        "action": "-",
        "table": "-",
        "rows": 0,
        "ip": "-",  # IP å…œåº•
    }

    def filter(self, record: logging.LogRecord) -> bool:
        for key, default in self.DEFAULTS.items():
            if not hasattr(record, key):
                setattr(record, key, default)

        if not hasattr(record, "module_name"):
            record.module_name = record.name
        return True


# ==============================================================================
# 2. åˆå§‹åŒ–ä¸»æ—¥å¿—ç³»ç»Ÿ (ä¿æŒä¸å˜)
# ==============================================================================

_LOG_INITIALIZED = False


def _ensure_log_dir() -> Path:
    """ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨"""
    log_dir = Path(Config.LOG_DIR)
    log_dir.mkdir(parents=True, exist_ok=True)
    return log_dir


def init_logging(app_name: str = "EaglestarERP") -> None:
    """
    åˆå§‹åŒ–å…¨å±€æ—¥å¿—é…ç½®ï¼Œåªæ‰§è¡Œä¸€æ¬¡ã€‚
    """
    global _LOG_INITIALIZED
    if _LOG_INITIALIZED:
        return

    log_dir = _ensure_log_dir()

    # æ ¹ logger åŸºç¡€é…ç½® (ç•¥)
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    root_logger.handlers.clear()

    # å…¬å…± Formatterï¼ˆå¸¦ä¸šåŠ¡å­—æ®µï¼‰
    fmt = (
        "%(asctime)s | %(levelname)-8s | %(module_name)s | "
        "user=%(user)s | action=%(action)s | table=%(table)s | rows=%(rows)s | ip=%(ip)s | "  # <-- IP å­—æ®µ
        "%(message)s"
    )
    datefmt = "%Y-%m-%d %H:%M:%S"
    formatter = logging.Formatter(fmt=fmt, datefmt=datefmt)

    # å®‰å…¨å…œåº• Filter
    safe_filter = SafeExtraFilter()

    # æ§åˆ¶å° Handler (ç•¥)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    console_handler.addFilter(safe_filter)
    root_logger.addHandler(console_handler)

    # åº”ç”¨æ—¥å¿—æ–‡ä»¶ Handler (ç•¥)
    app_log_path = log_dir / "app.log"
    file_handler = logging.handlers.RotatingFileHandler(
        filename=str(app_log_path),
        maxBytes=10 * 1024 * 1024,
        backupCount=5,
        encoding="utf-8"
    )
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    file_handler.addFilter(safe_filter)
    root_logger.addHandler(file_handler)

    # æ•°æ®åº“å®¡è®¡ä¸“ç”¨ Logger (ç•¥)
    db_logger = logging.getLogger("db.audit")
    db_logger.setLevel(logging.INFO)

    db_log_path = log_dir / "db_audit.log"
    db_file_handler = logging.handlers.RotatingFileHandler(
        filename=str(db_log_path),
        maxBytes=20 * 1024 * 1024,
        backupCount=10,
        encoding="utf-8"
    )
    db_file_handler.setLevel(logging.INFO)
    db_file_handler.setFormatter(formatter)
    db_file_handler.addFilter(safe_filter)

    db_logger.handlers.clear()
    db_logger.addHandler(db_file_handler)
    db_logger.propagate = False

    _LOG_INITIALIZED = True


# æ¨¡å—å¯¼å…¥æ—¶è‡ªåŠ¨åˆå§‹åŒ–ä¸€æ¬¡
init_logging()


# ==============================================================================
# 3. å¯¹å¤–å·¥å…·å‡½æ•°ï¼šè·å– Logger (æ³¨å…¥ IP)
# ==============================================================================

class UserLoggerAdapter(logging.LoggerAdapter):
    """
    ç®€å•çš„ LoggerAdapterï¼šåœ¨ log è°ƒç”¨æ—¶è‡ªåŠ¨æ³¨å…¥ user å’Œ ip å­—æ®µã€‚
    """

    def __init__(self, logger: logging.Logger):
        super().__init__(logger, {})

    def process(self, msg, kwargs):
        # ä» core/context å®æ—¶è·å–å½“å‰ç”¨æˆ·å’Œ IP
        ctx = get_context()
        current_extra = {"user": ctx.username or "-", "ip": ctx.ip or "-"}

        # å…è®¸åœ¨ extra ä¸­ç»§ç»­è¦†ç›–/è¡¥å……å­—æ®µ
        extra = kwargs.get("extra", {})
        merged_extra = {**current_extra, **(extra or {})}
        kwargs["extra"] = merged_extra

        return msg, kwargs


def get_logger(name: Optional[str] = None) -> logging.Logger:
    """
    è·å–åº”ç”¨ Loggerã€‚
    ç°åœ¨ä¸éœ€è¦ä¼ é€’ user å‚æ•°ï¼Œå› ä¸ºå®ƒä» contextvars ä¸­è‡ªåŠ¨è·å–ã€‚
    """
    init_logging()
    logger = logging.getLogger(name if name else "app")
    # å§‹ç»ˆä½¿ç”¨ Adapter æ¥æ³¨å…¥å½“å‰ä¸Šä¸‹æ–‡
    return UserLoggerAdapter(logger)


def get_db_audit_logger() -> logging.Logger:
    """
    è·å–æ•°æ®åº“å®¡è®¡ Logger (å®ƒä¼šè‡ªåŠ¨ä» contextvars ä¸­è·å– user å’Œ ip)ã€‚
    """
    init_logging()
    logger = logging.getLogger("db.audit")
    return UserLoggerAdapter(logger)  # åŒæ ·ä½¿ç”¨ Adapter

==================== END FILE: core/logging_config.py ====================


==================== START FILE: core/security.py ====================
# core/security.py
import os
import base64
import hashlib
import hmac
import uuid

class SecurityUtils:
    """
    [åº•å±‚ç»„ä»¶] å®‰å…¨åŠ å¯†å·¥å…·ç®±
    èŒè´£ï¼šæä¾›æ— çŠ¶æ€çš„å¯†ç å“ˆå¸Œã€æ ¡éªŒä¸ä»¤ç‰Œç”Ÿæˆã€‚
    """

    @staticmethod
    def hash_password(password: str) -> str:
        """PBKDF2-SHA256 å“ˆå¸Œ"""
        if not password: raise ValueError("å¯†ç ä¸èƒ½ä¸ºç©º")
        salt = os.urandom(16)
        iterations = 120000
        dk = hashlib.pbkdf2_hmac("sha256", password.encode("utf-8"), salt, iterations)
        salt_b64 = base64.b64encode(salt).decode("ascii")
        hash_b64 = base64.b64encode(dk).decode("ascii")
        return f"pbkdf2_sha256${iterations}${salt_b64}${hash_b64}"

    @staticmethod
    def verify_password(password: str, stored_hash: str) -> bool:
        """æ ¡éªŒå¯†ç """
        try:
            alg, iter_str, salt_b64, hash_b64 = stored_hash.split("$")
            if alg != "pbkdf2_sha256": return False
            salt = base64.b64decode(salt_b64.encode("ascii"))
            stored_dk = base64.b64decode(hash_b64.encode("ascii"))
            dk = hashlib.pbkdf2_hmac("sha256", password.encode("utf-8"), salt, int(iter_str))
            return hmac.compare_digest(dk, stored_dk)
        except Exception:
            return False

    @staticmethod
    def generate_token() -> str:
        """ç”Ÿæˆå”¯ä¸€ä¼šè¯ä»¤ç‰Œ"""
        return str(uuid.uuid4())

==================== END FILE: core/security.py ====================


==================== START FILE: core/rules.py ====================
# core/rules.py
from config import Config


class BusinessRules:
    """
    [ä¸šåŠ¡è§„åˆ™å±‚]
    é›†ä¸­ç®¡ç†æ‰€æœ‰ç¡¬ç¼–ç çš„ä¸šåŠ¡é€»è¾‘ï¼ˆå¦‚è€—æŸç‡è®¡ç®—ã€ABCåˆ†ç±»æ ‡å‡†ï¼‰ã€‚
    é˜²æ­¢è¿™äº›é€»è¾‘æ•£è½åœ¨å„ä¸ª .py æ–‡ä»¶ä¸­å¯¼è‡´ç»´æŠ¤å›°éš¾ã€‚
    """

    @staticmethod
    def get_net_quantity(original_qty: int, action: str) -> float:
        """
        æ ¹æ® Action ç±»å‹ï¼Œè®¡ç®—æ‰£é™¤è€—æŸåçš„å‡€é”€é‡ã€‚

        Args:
            original_qty (int): åŸå§‹è®¢å•æ•°é‡
            action (str): è®¢å•åŠ¨ä½œä»£ç  (CA, RE, CC, CR, PD, etc.)

        Returns:
            float: å‡€é”€é‡

        Reference:
            Config.LOSS_RATES å®šä¹‰äº†å„ç±»åŠ¨ä½œçš„è€—æŸç™¾åˆ†æ¯”ã€‚
        """
        action = str(action).strip().upper()
        rates = Config.LOSS_RATES

        loss_rate = 0.0

        # å…·ä½“çš„ä¸šåŠ¡æ˜ å°„é€»è¾‘
        if action == 'CA':
            loss_rate = 1.0  # å–æ¶ˆè®¢å•ï¼Œå…¨é¢æ‰£é™¤
        elif action == 'RE':
            loss_rate = rates.get('RETURN', 0.3)
        elif action == 'CC':
            loss_rate = rates.get('CASE', 0.6)
        elif action == 'CR':
            loss_rate = rates.get('REQUEST', 0.5)
        elif action == 'PD':
            loss_rate = rates.get('DISPUTE', 1.0)

        # è®¡ç®—å‡€å€¼ï¼š æ•°é‡ * (1 - è€—æŸç‡)
        return original_qty * (1 - loss_rate)

    @staticmethod
    def get_abc_grade(cumulative_ratio: float) -> tuple[str, float]:
        """
        æ ¹æ®ç´¯è®¡é”€å”®é¢å æ¯”ï¼Œè¿”å› ABC ç­‰çº§å’Œå¯¹åº”çš„æœåŠ¡æ°´å¹³ã€‚

        Returns:
            (Grade, ServiceLevel)
            e.g. ("A", 0.98)
        """
        if cumulative_ratio <= 0.80:
            return "A", 0.98
        elif cumulative_ratio <= 0.95:
            return "B", 0.95
        else:
            return "C", 0.90

==================== END FILE: core/rules.py ====================


==================== START FILE: core/context.py ====================
# core/context.py
"""
å…¨å±€è¯·æ±‚/ç”¨æˆ·ä¸Šä¸‹æ–‡ç®¡ç† (Request / User Context)

ä¿®æ”¹è¯´æ˜:
- å¢å¼º set_current_user æ–¹æ³•ï¼Œæ¥å—å¹¶è®¾ç½® ip å­—æ®µã€‚
"""

from dataclasses import dataclass, field
from typing import Optional, Dict
import contextvars


@dataclass
class RequestContext:
    """
    å½“å‰è¯·æ±‚ä¸Šä¸‹æ–‡æ¨¡å‹ï¼š
    """
    username: Optional[str] = None
    ip: Optional[str] = None # <-- æ–°å¢å­—æ®µ
    extra: Dict[str, str] = field(default_factory=dict)


# ä½¿ç”¨ contextvars å­˜å‚¨ä¸Šä¸‹æ–‡ï¼Œä¾¿äºæœªæ¥æ”¯æŒå¤šåç¨‹å¹¶å‘
_current_ctx: contextvars.ContextVar[RequestContext] = contextvars.ContextVar(
    "current_request_context",
    default=RequestContext()
)


def set_current_user(username: Optional[str], ip: Optional[str] = None, extra: Optional[Dict[str, str]] = None) -> None:
    """
    è®¾ç½®å½“å‰ç”¨æˆ·ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨å…¥å£å±‚è°ƒç”¨ï¼Œä¾‹å¦‚ Streamlit / API æ¡†æ¶ï¼‰
    """
    ctx = RequestContext(
        username=username,
        ip=ip,
        extra=extra or {}
    )
    _current_ctx.set(ctx)


def get_current_user() -> Optional[str]:
    """
    è·å–å½“å‰ä¸Šä¸‹æ–‡ä¸­çš„ç”¨æˆ·åã€‚
    """
    return _current_ctx.get().username


def get_context() -> RequestContext:
    """
    è·å–å®Œæ•´çš„ RequestContext å¯¹è±¡ã€‚
    """
    return _current_ctx.get()


def clear_context() -> None:
    """
    æ¸…ç©ºå½“å‰ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ç”¨æˆ·é€€å‡ºç™»å½•æ—¶è°ƒç”¨ï¼‰ã€‚
    """
    _current_ctx.set(RequestContext())

==================== END FILE: core/context.py ====================


==================== START FILE: core/progress.py ====================
# core/progress.py
"""
ç»Ÿä¸€è¿›åº¦ç®¡ç†å™¨ï¼ˆProgress Managerï¼‰
---------------------------------
ç”¨äºåœ¨ä»»æ„ä»£ç ä¸­å®‰å…¨æ›´æ–°è¿›åº¦ï¼Œå¹¶åœ¨ Streamlit å‰ç«¯å±•ç¤ºã€‚

æ ¸å¿ƒè®¾è®¡ï¼š
1. åç«¯ä»»åŠ¡ï¼ˆETLã€æŠ¥è¡¨åˆ†æã€æ•°æ®åº“å†™å…¥ç­‰ï¼‰åªéœ€è¦è°ƒç”¨ï¼š
       progress.update(task_id, percent, "æè¿°å†…å®¹")

2. å‰ç«¯é¡µé¢åªéœ€è¦è°ƒç”¨ï¼š
       progress_ui = progress.use(task_id)
       progress_ui.render()

3. é¿å…é‡å¤é€ è½®å­ï¼Œè®©æ‰€æœ‰æ¨¡å—å…±ç”¨ä¸€å¥—è¿›åº¦ç³»ç»Ÿã€‚
"""

import time
import threading
from typing import Dict, Any, Optional

import streamlit as st


# å…¨å±€å­—å…¸å­˜å‚¨æ‰€æœ‰çš„ä»»åŠ¡è¿›åº¦
_PROGRESS_STORE: Dict[str, Dict[str, Any]] = {}
_LOCK = threading.Lock()


class ProgressManager:
    """åç«¯è°ƒç”¨çš„ç»Ÿä¸€ç®¡ç†å™¨"""

    @staticmethod
    def create(task_id: str):
        """æ–°å»ºä¸€ä¸ªè¿›åº¦ä»»åŠ¡"""
        with _LOCK:
            _PROGRESS_STORE[task_id] = {
                "percent": 0,
                "message": "ä»»åŠ¡å¼€å§‹...",
                "done": False,
                "error": None,
                "timestamp": time.time(),
            }

    @staticmethod
    def update(task_id: str, percent: int, message: str):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        with _LOCK:
            if task_id not in _PROGRESS_STORE:
                return
            _PROGRESS_STORE[task_id]["percent"] = max(0, min(percent, 100))
            _PROGRESS_STORE[task_id]["message"] = message

    @staticmethod
    def done(task_id: str, message: str = "ä»»åŠ¡å®Œæˆ"):
        """å®Œæˆä»»åŠ¡"""
        with _LOCK:
            if task_id in _PROGRESS_STORE:
                _PROGRESS_STORE[task_id]["percent"] = 100
                _PROGRESS_STORE[task_id]["message"] = message
                _PROGRESS_STORE[task_id]["done"] = True

    @staticmethod
    def error(task_id: str, message: str):
        """ä»»åŠ¡é”™è¯¯"""
        with _LOCK:
            if task_id in _PROGRESS_STORE:
                _PROGRESS_STORE[task_id]["error"] = message
                _PROGRESS_STORE[task_id]["done"] = True

    @staticmethod
    def get(task_id: str) -> Optional[Dict[str, Any]]:
        """è¯»å–æŒ‡å®šä»»åŠ¡è¿›åº¦"""
        with _LOCK:
            return _PROGRESS_STORE.get(task_id)

    @staticmethod
    def clear(task_id: str):
        """æ¸…é™¤ä»»åŠ¡"""
        with _LOCK:
            if task_id in _PROGRESS_STORE:
                del _PROGRESS_STORE[task_id]


progress = ProgressManager()


# -------------------------------------------------------------------
# Streamlit å‰ç«¯æ¸²æŸ“ç»„ä»¶
# -------------------------------------------------------------------
class StreamlitProgressUI:
    """Streamlit æ¸²æŸ“å™¨ï¼šå®æ—¶å±•ç¤ºæŸä¸ªä»»åŠ¡çš„è¿›åº¦"""

    def __init__(self, task_id: str, refresh_interval: float = 0.2):
        self.task_id = task_id
        self.refresh_interval = refresh_interval

    def render(self):
        """å‰ç«¯å¾ªç¯åˆ·æ–° UIï¼Œç›´åˆ°ä»»åŠ¡å®Œæˆæˆ–å‡ºé”™"""

        placeholder = st.empty()

        while True:
            data = progress.get(self.task_id)
            if not data:
                placeholder.warning("è¿›åº¦ä»»åŠ¡ä¸å­˜åœ¨ã€‚")
                break

            percent = data["percent"]
            message = data["message"]
            done = data["done"]
            error = data["error"]

            placeholder.progress(percent, text=message)

            if error:
                st.error(f" é”™è¯¯ï¼š{error}")
                progress.clear(self.task_id)
                break

            if done:
                st.success(f"ğŸ‰ {message}")
                progress.clear(self.task_id)
                break

            time.sleep(self.refresh_interval)


def use(task_id: str):
    """ç»™å‰ç«¯è°ƒç”¨çš„ UI æ„é€ å™¨"""
    return StreamlitProgressUI(task_id)


==================== END FILE: core/progress.py ====================


==================== START FILE: core/base.py ====================
# core/base.py
import os
import pandas as pd
from abc import ABC, abstractmethod
from typing import Optional, List, Union
from datetime import date

# å¯¼å…¥é…ç½®
from config import Config


# [ä¿®æ”¹ç‚¹1] ç§»é™¤ DBClient å¯¼å…¥ï¼ŒåŸºç±»ä¸éœ€è¦ç›´æ¥æ“ä½œæ•°æ®åº“ï¼Œè§£è€¦

class BaseAnalyzer(ABC):
    """
    [æ ¸å¿ƒåŸºç±»] æ‰€æœ‰åˆ†æå™¨(Analyzer)çš„çˆ¶ç±»ã€‚

    è®¾è®¡ç›®çš„ï¼š
    1. ç»Ÿä¸€ç”Ÿå‘½å‘¨æœŸï¼šæ ‡å‡†åŒ–åˆå§‹åŒ–ã€è¿è¡Œã€ä¿å­˜æµç¨‹ã€‚
    2. æ¶ˆé™¤æ ·æ¿ä»£ç ï¼šæä¾›é€šç”¨çš„ CSV ä¿å­˜ã€æ—¥æœŸå¤„ç† helperã€‚
    3. å®‰å…¨æ—¥å¿—ï¼šç»Ÿä¸€æ—¥å¿—å‡ºå£ï¼Œé˜²æ­¢æ•æ„Ÿè·¯å¾„æ³„éœ²ã€‚
    """

    def __init__(self, start_date: Optional[Union[str, date]] = None,
                 end_date: Optional[Union[str, date]] = None,
                 file_suffix: str = ""):
        """
        åˆå§‹åŒ–åˆ†æå™¨ã€‚
        """
        # ç»Ÿä¸€è½¬æ¢æ—¥æœŸæ ¼å¼ä¸º datetime.date å¯¹è±¡
        self.start_date = pd.to_datetime(start_date).date() if start_date else None
        self.end_date = pd.to_datetime(end_date).date() if end_date else None
        self.file_suffix = file_suffix

        # é»˜è®¤è¾“å‡ºç›®å½•
        self.output_dir = Config.OUTPUT_DIR

        # é¢„ç•™ç»“æœå®¹å™¨
        self.result_data = None

    @abstractmethod
    def run(self):
        """
        [æŠ½è±¡æ–¹æ³•] å­ç±»å¿…é¡»å®ç°å…·ä½“çš„åˆ†æé€»è¾‘ã€‚
        """
        pass

    def save_csv(self, df: pd.DataFrame, filename: str, footer_lines: Optional[List[str]] = None) -> str:
        """
        [é€šç”¨æ–¹æ³•] ä¿å­˜ DataFrame ä¸º CSV

        ä¿®æ”¹ç‚¹ (V2.0):
        1. ä½¿ç”¨ self.log æ›¿ä»£ printï¼Œç¡®ä¿ UI èƒ½æ•è·ã€‚
        2. æ—¥å¿—ä¸­åªæ‰“å°æ–‡ä»¶åï¼Œä¸æ‰“å°ç»å¯¹è·¯å¾„ (å®‰å…¨è„±æ•)ã€‚
        """
        if df is None or df.empty:
            # [ä¿®æ”¹ç‚¹2] ä½¿ç”¨ self.log
            self.log(f"âš ï¸ Warning: å°è¯•ä¿å­˜ç©ºæ•°æ®åˆ° {filename}")
            return ""

        file_path = os.path.join(self.output_dir, filename)

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir, exist_ok=True)

        try:
            with open(file_path, "w", encoding="utf-8-sig") as f:
                # 1. å†™å…¥æ•°æ®
                df.to_csv(f, index=False)

                # [cite_start]2. å†™å…¥ Footer (å¦‚æœæœ‰) [cite: 16, 250]
                if footer_lines:
                    f.write("\n")  # ç©ºä¸€è¡Œ
                    for line in footer_lines:
                        f.write(f"{line}\n")

            # [ä¿®æ”¹ç‚¹3] å®‰å…¨è„±æ•ï¼šåªæ‰“å° filenameï¼Œä¸æ‰“å° file_path
            self.log(f" æŠ¥è¡¨å·²ç”Ÿæˆ: {filename}")
            return file_path

        except Exception as e:
            # [ä¿®æ”¹ç‚¹2] ä½¿ç”¨ self.log
            self.log(f" ä¿å­˜æ–‡ä»¶å¤±è´¥ [{filename}]: {e}")
            raise e

    def get_date_filter_sql(self, column_name: str = "order date") -> str:
        """
        [è¾…åŠ©æ–¹æ³•] ç”Ÿæˆ SQL æ—¥æœŸè¿‡æ»¤æ¡ä»¶ã€‚
        """
        if not self.start_date or not self.end_date:
            return "1=1"

        return (f"`{column_name}` >= '{self.start_date}' AND "
                f"`{column_name}` <= '{self.end_date} 23:59:59'")

    def log(self, message: str):
        """
        [æ—¥å¿—é€‚é…å™¨]
        ç»Ÿä¸€è¾“å‡ºæ¥å£ï¼ŒUI å±‚çš„ task_monitor ä¼šæ•è·æ­¤è¾“å‡ºã€‚
        """
        print(f"[{self.__class__.__name__}] {message}")

==================== END FILE: core/base.py ====================


==================== START FILE: core/repository/sku_repo.py ====================
# core/repository/sku_repo.py
from typing import List, Optional
import pandas as pd
from sqlalchemy import text
from core.repository.base_repo import BaseRepository
from core.repository.db_client import DBClient


class SkuRepository(BaseRepository):
    """
    [æ•°æ®ä»“åº“] SKU é™æ€ä¿¡æ¯ç®¡ç†
    è´Ÿè´£è¡¨: Data_COGS, Data_Inventory
    """

    def get_all_cogs(self) -> pd.DataFrame:
        """
        è·å–æ‰€æœ‰ SKU çš„æˆæœ¬ä¿¡æ¯
        Returns: DataFrame [SKU, Category, Type, Cost, Freight, Cog]
        """
        sql = "SELECT * FROM Data_COGS"
        return self.fetch_dataframe(sql)

    def get_inventory_latest(self) -> pd.DataFrame:
        """
        è·å–å½“å‰æœ€æ–°çš„åº“å­˜æ•°æ®
        æ³¨æ„ï¼šè‡ªåŠ¨è¯†åˆ« Data_Inventory ä¸­æœ€æ–°çš„æœˆä»½åˆ—
        """
        # 1. å…ˆè¯»è¡¨ç»“æ„ï¼Œä¸è¯»æ•°æ®ï¼Œä¸ºäº†æ‰¾åˆ—å
        schema_df = self.fetch_dataframe("SELECT * FROM Data_Inventory LIMIT 0")

        # 2. æ‰¾åˆ°åŒ…å« '-' çš„åˆ— (ä¾‹å¦‚ '2025-10') å¹¶å–æœ€å¤§å€¼
        date_cols = [c for c in schema_df.columns if '-' in str(c)]
        if not date_cols:
            return pd.DataFrame(columns=["SKU", "Quantity"])

        latest_col = sorted(date_cols)[-1]

        # 3. åŠ¨æ€æ„é€ æŸ¥è¯¢ï¼Œåªå– SKU å’Œ æœ€æ–°åº“å­˜
        # ä½¿ç”¨ SQLAlchemy text é˜²æ­¢åˆ—åæ³¨å…¥é£é™© (è™½ç„¶è¿™é‡Œæ¥æºäºå†…éƒ¨ Schema)
        sql = f"SELECT SKU, `{latest_col}` as Quantity FROM Data_Inventory"
        return self.fetch_dataframe(sql)

    def get_valid_skus(self) -> List[str]:
        """è·å–ç³»ç»Ÿå†…æ‰€æœ‰æœ‰æ•ˆçš„ SKU åˆ—è¡¨"""
        df = self.fetch_dataframe("SELECT DISTINCT SKU FROM Data_COGS")
        return df["SKU"].dropna().tolist()

    def get_distinct_values(self, column: str) -> List[str]:
        # ... (ä¿æŒä¸å˜)
        allowed_cols = ['Category', 'SubCategory', 'Type']
        if column not in allowed_cols:
            return []
        sql = f"SELECT DISTINCT `{column}` FROM Data_COGS WHERE `{column}` IS NOT NULL AND `{column}` != ''"
        df = self.fetch_dataframe(sql)
        return sorted(df[column].dropna().tolist())

    def create_sku_transactional(self, sku_data: dict) -> bool:
        """
        [æ–°å¢] åŸå­æ€§åˆ›å»ºæ–° SKU
        """
        # [ä¿®å¤ç‚¹] ç›´æ¥ä½¿ç”¨ DBClient ç±»æ–¹æ³•è·å–å¼•æ“ï¼Œè€Œä¸æ˜¯ self.db
        engine = DBClient.get_engine()

        try:
            with engine.begin() as conn:
                # 1. æ’å…¥ Data_COGS
                cols = ", ".join(sku_data.keys())
                # æ„é€  :key å½¢å¼çš„å ä½ç¬¦
                params_str = ", ".join([f":{k}" for k in sku_data.keys()])

                sql_cogs = text(f"INSERT INTO Data_COGS ({cols}) VALUES ({params_str})")
                conn.execute(sql_cogs, sku_data)

                # 2. æ’å…¥ Data_Inventory
                # è·å– Inventory è¡¨çš„æ‰€æœ‰åˆ—å
                schema_df = pd.read_sql("SELECT * FROM Data_Inventory LIMIT 0", conn)
                inv_cols = schema_df.columns.tolist()

                inv_vals = []
                for col in inv_cols:
                    if col == 'SKU':
                        inv_vals.append(f"'{sku_data['SKU']}'")
                    else:
                        inv_vals.append("0")  # å†å²æœˆä»½å¡« 0

                # è¿™é‡Œåˆ—åè¦åŠ åå¼•å·ä¿æŠ¤ï¼Œé˜²æ­¢ '2025-10' è¿™ç§åˆ—åæŠ¥é”™
                col_str = ", ".join([f"`{c}`" for c in inv_cols])
                val_str = ", ".join(inv_vals)

                sql_inv = text(f"INSERT INTO Data_Inventory ({col_str}) VALUES ({val_str})")
                conn.execute(sql_inv)

            return True
        except Exception as e:
            print(f"åˆ›å»º SKU å¤±è´¥: {e}")
            raise e

==================== END FILE: core/repository/sku_repo.py ====================


==================== START FILE: core/repository/transaction_repo.py ====================
# core/repository/transaction_repo.py

from typing import Optional
from datetime import date
import pandas as pd
from sqlalchemy import text
import streamlit as st  # [New] å¼•å…¥ Streamlit ç”¨äºç¼“å­˜

from core.repository.base_repo import BaseRepository


class TransactionRepository(BaseRepository):
    """
    [æ•°æ®ä»“åº“] äº¤æ˜“æµæ°´ç®¡ç† (V3.0 Cached)
    """

    # [New] ä½¿ç”¨ Streamlit ç¼“å­˜è£…é¥°å™¨
    # ttl=3600: ç¼“å­˜æœ‰æ•ˆæœŸ 1 å°æ—¶
    # show_spinner=False: åå°é™é»˜åŠ è½½ï¼Œä¸å¹²æ‰°å‰ç«¯è‡ªå®šä¹‰è¿›åº¦æ¡
    @st.cache_data(ttl=3600, show_spinner=False)
    def get_transactions_by_date(_self, start_date: date, end_date: date) -> pd.DataFrame:
        """
        æŸ¥è¯¢æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ¸…æ´—åè®¢å•æ•°æ®ã€‚
        æ³¨æ„: ç¬¬ä¸€ä¸ªå‚æ•°åä¸º _selfï¼Œè¡¨ç¤º Streamlit ç¼“å­˜æ—¶å¿½ç•¥è¯¥å‚æ•°ï¼ˆä¸å“ˆå¸Œ self å¯¹è±¡ï¼‰ã€‚
        """
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥ç”¨ self.fetch_dataframeï¼Œå› ä¸ºè¢« @st.cache_data è£…é¥°çš„æ–¹æ³•
        # æœ€å¥½æ˜¯é™æ€çš„æˆ–è€…ç‹¬ç«‹çš„ã€‚ä½†ä¸ºäº†å¤ç”¨ BaseRepository çš„è¿æ¥é€»è¾‘ï¼Œ
        # æˆ‘ä»¬éœ€è¦åœ¨æ–¹æ³•å†…éƒ¨é‡æ–°è·å–è¿æ¥ï¼Œæˆ–è€…åˆ©ç”¨ BaseRepository çš„é€»è¾‘ã€‚
        #
        # Hack: ç”±äº _self è¢«å¿½ç•¥ï¼ŒStreamlit ä¸ä¼šåºåˆ—åŒ–å®ƒã€‚
        # æˆ‘ä»¬ä¾ç„¶å¯ä»¥è°ƒç”¨ _self.fetch_dataframeã€‚

        # æ„é€  SQL
        # è¿™é‡Œçš„ SQL æ˜¯å…¨è¡¨æŸ¥ + Pandas è¿‡æ»¤ã€‚
        # [ä¼˜åŒ–å»ºè®®]: æœ‰äº†ç´¢å¼•åï¼Œæˆ‘ä»¬åº”è¯¥åœ¨ SQL å±‚é¢åšæ—¥æœŸè¿‡æ»¤ï¼Œè€Œä¸æ˜¯è¯»å–å…¨è¡¨åˆ°å†…å­˜ã€‚
        # è¿™æ ·èƒ½æå¤§å‡å°‘ç½‘ç»œä¼ è¾“ IOã€‚

        sql = """
              SELECT * \
              FROM Data_Clean_Log
              WHERE `order date` >= :start_date
                AND `order date` <= :end_date \
              """

        # æ ¼å¼åŒ–æ—¥æœŸä¸ºå­—ç¬¦ä¸²ï¼ŒåŒ¹é…æ•°æ®åº“æ ¼å¼ YYYY-MM-DD
        params = {
            "start_date": start_date.strftime("%Y-%m-%d"),
            "end_date": end_date.strftime("%Y-%m-%d")
        }

        # æ‰§è¡ŒæŸ¥è¯¢ (è¿™é‡Œå®é™…ä¸Šåˆ©ç”¨äº†åˆšåˆšå»ºç«‹çš„ idx_order_date ç´¢å¼•)
        df = _self.fetch_dataframe(sql, params)

        if df.empty:
            return df

        # 1. å¼ºåˆ¶ç±»å‹è½¬æ¢ (ä¿æŒåŸæœ‰ä¸šåŠ¡é€»è¾‘)
        numeric_cols = [
            'quantity', 'revenue', 'Shipping and handling', 'profit',
            'Shipping label-Earning data', 'Shipping label-underpay',
            'Shipping label-overpay', 'Shipping label-Return'
        ]
        for c in numeric_cols:
            if c in df.columns:
                df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)

        # 2. æ—¥æœŸè§£æ (å†æ¬¡ç¡®ä¿ pandas æ ¼å¼æ­£ç¡®)
        df["order date"] = pd.to_datetime(df["order date"], errors='coerce')

        # 3. [å…³é”®åŠ å›º] é‡ç½®ç´¢å¼•å¹¶è¿”å›å®Œå…¨ç‹¬ç«‹çš„å‰¯æœ¬
        final_df = df.reset_index(drop=True)

        return final_df
==================== END FILE: core/repository/transaction_repo.py ====================


==================== START FILE: core/repository/db_client.py ====================
# core/repository/db_client.py

import re
from contextlib import contextmanager
from typing import Optional, Dict, Any, Union

import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine, Connection

from config import Config
from core.logging_config import get_db_audit_logger
from core.context import get_current_user


class DBClient:
    """
    [æ ¸å¿ƒç»„ä»¶] æ•°æ®åº“å®¢æˆ·ç«¯ç®¡ç†å™¨ (V3.2 + Transaction Support)
    """

    _engine: Optional[Engine] = None
    _audit_logger = get_db_audit_logger()

    @classmethod
    def get_engine(cls) -> Engine:
        if cls._engine is None:
            cls._engine = create_engine(
                Config.SQLALCHEMY_URL,
                pool_recycle=3600,
                pool_pre_ping=True,
                echo=False
            )
        return cls._engine

    # [New] æ˜¾å¼äº‹åŠ¡ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    @classmethod
    @contextmanager
    def atomic_transaction(cls):
        """
        [äº‹åŠ¡æ§åˆ¶] å¼€å¯ä¸€ä¸ªåŸå­äº‹åŠ¡ã€‚
        ç”¨æ³•:
            with DBClient.atomic_transaction() as conn:
                conn.execute(...)
                conn.execute(...)
        å¦‚æœå—å†…æŠ›å‡ºå¼‚å¸¸ï¼Œæ‰€æœ‰æ“ä½œè‡ªåŠ¨å›æ»šã€‚
        """
        engine = cls.get_engine()
        with engine.begin() as conn:
            yield conn

    # ... (è¾…åŠ©æ–¹æ³• _detect_action, _extract_table_name, _log_read, _log_write ä¿æŒä¸å˜ï¼Œçœç•¥ä»¥èŠ‚çœç¯‡å¹…) ...
    # è¯·ä¿ç•™åŸæœ‰çš„ _detect_action ç­‰ç§æœ‰æ–¹æ³•

    @staticmethod
    def _detect_action(sql: Union[str, Any]) -> str:
        if not isinstance(sql, str): return "EXECUTE"
        head = sql.strip().split(None, 1)
        if not head: return "EXECUTE"
        keyword = head[0].upper()
        if keyword in {"SELECT", "INSERT", "UPDATE", "DELETE", "TRUNCATE"}:
            return keyword
        return "EXECUTE"

    @staticmethod
    def _extract_table_name(sql: Union[str, Any]) -> str:
        if not isinstance(sql, str): return "-"
        text_sql = sql.upper()
        patterns = [r"\bFROM\s+`?([A-Z0-9_]+)`?", r"\bINTO\s+`?([A-Z0-9_]+)`?", r"\bUPDATE\s+`?([A-Z0-9_]+)`?", r"\bTRUNCATE\s+TABLE\s+`?([A-Z0-9_]+)`?"]
        for p in patterns:
            m = re.search(p, text_sql, flags=re.IGNORECASE)
            if m: return m.group(1)
        return "-"

    @classmethod
    def _log_read(cls, sql, row_count):
        try:
            user = get_current_user() or "-"
            cls._audit_logger.info("DB READ", extra={"user": user, "action": "SELECT", "table": cls._extract_table_name(sql), "rows": row_count})
        except: pass

    @classmethod
    def _log_write(cls, sql, row_count):
        try:
            user = get_current_user() or "-"
            cls._audit_logger.info("DB WRITE", extra={"user": user, "action": cls._detect_action(sql), "table": cls._extract_table_name(sql), "rows": row_count})
        except: pass

    # =========================================================================
    # é€šç”¨æ¥å£ (é€‚é… Pandas å’Œ åŸç”Ÿ SQL)
    # =========================================================================
    @classmethod
    def read_df(cls, sql: Union[str, Any], params: Optional[Dict[str, Any]] = None) -> pd.DataFrame:
        engine = cls.get_engine()
        stmt = text(sql) if isinstance(sql, str) else sql
        try:
            df = pd.read_sql(stmt, engine, params=params)
            cls._log_read(sql, len(df) if df is not None else 0)
            return df
        except Exception as e:
            # [Robustness] è®°å½•æŸ¥è¯¢å¤±è´¥æ—¥å¿—
            cls._audit_logger.error(f"SQL Query Failed: {e}", extra={"sql": str(sql)[:200]})
            raise e

    @classmethod
    def execute_stmt(cls, sql: Union[str, Any], params: Optional[Dict[str, Any]] = None) -> int:
        engine = cls.get_engine()
        stmt = text(sql) if isinstance(sql, str) else sql
        try:
            with engine.begin() as conn:
                result = conn.execute(stmt, params if params else {})
                rowcount = result.rowcount or 0
            cls._log_write(sql, rowcount)
            return rowcount
        except Exception as e:
            cls._audit_logger.error(f"SQL Execute Failed: {e}", extra={"sql": str(sql)[:200]})
            raise e

    @classmethod
    def truncate_table(cls, table_name: str) -> None:
        """å®‰å…¨æ¸…ç©ºè¡¨"""
        # ç®€å•é˜²æ³¨å…¥ï¼šç¡®ä¿è¡¨ååªåŒ…å«å­—æ¯æ•°å­—ä¸‹åˆ’çº¿
        if not re.match(r'^[a-zA-Z0-9_]+$', table_name):
            raise ValueError(f"Invalid table name: {table_name}")
        sql = f"TRUNCATE TABLE `{table_name}`"
        cls.execute_stmt(sql)
==================== END FILE: core/repository/db_client.py ====================


==================== START FILE: core/repository/base_repo.py ====================
# core/repository/base_repo.py
from typing import Optional, Dict, Any, List
import pandas as pd
from core.repository.db_client import DBClient


class BaseRepository:
    """
    [æŠ½è±¡åŸºç±»] æ•°æ®ä»“åº“åŸºç±»
    æ‰€æœ‰çš„ Repository (å¦‚ InventoryRepo, SalesRepo) éƒ½åº”ç»§æ‰¿æ­¤ç±»ã€‚

    ç›®çš„ï¼š
    1. å°è£…åº•å±‚ DBClient çš„è°ƒç”¨ï¼Œå­ç±»æ— éœ€å…³å¿ƒè¿æ¥ç»†èŠ‚ã€‚
    2. æä¾›é€šç”¨çš„è¾…åŠ©æ–¹æ³•ã€‚
    """

    def fetch_dataframe(self, sql: str, params: Optional[Dict[str, Any]] = None) -> pd.DataFrame:
        """é€šç”¨ï¼šæŸ¥è¯¢å¹¶è¿”å› DataFrame"""
        # ç›´æ¥è°ƒç”¨é™æ€æ–¹æ³•
        return DBClient.read_df(sql, params)

    def execute(self, sql: str, params: Optional[Dict[str, Any]] = None) -> int:
        """é€šç”¨ï¼šæ‰§è¡Œå¢åˆ æ”¹ SQL"""
        # ç›´æ¥è°ƒç”¨é™æ€æ–¹æ³•
        return DBClient.execute_stmt(sql, params)

    def check_table_exists(self, table_name: str) -> bool:
        """
        æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨æŸå¼ è¡¨
        """
        sql = """
              SELECT COUNT(*)
              FROM information_schema.tables
              WHERE table_schema = DATABASE()
                AND table_name = :table_name \
              """
        df = self.fetch_dataframe(sql, {"table_name": table_name})
        return df.iloc[0, 0] > 0

==================== END FILE: core/repository/base_repo.py ====================


==================== START FILE: core/memory/chat_history/chat_admin.json ====================
{
  "sessions": {
    "8c632039-e743-446f-af7b-45072ae50ac2": {
      "title": "æ–°å¯¹è¯",
      "created_at": "2025-12-06 03:14:36",
      "updated_at": "2025-12-06 03:14:36",
      "messages": []
    }
  },
  "active_session_id": "8c632039-e743-446f-af7b-45072ae50ac2"
}

==================== END FILE: core/memory/chat_history/chat_admin.json ====================


==================== START FILE: core/etl/ingest.py ====================
# core/etl/ingest.py

import os
import pandas as pd
import csv
import tqdm
import numpy as np
from typing import List, Tuple, Optional
from pathlib import Path

# å¼•å…¥ SQLAlchemy ç±»å‹
from sqlalchemy.types import Text

from config import Config
from core.repository.db_client import DBClient
from core.logging_config import get_logger

# æŠ‘åˆ¶ Pandas çš„ FutureWarningï¼Œä¿æŒæ—¥å¿—æ¸…çˆ½
pd.set_option('future.no_silent_downcasting', True)


class BaseLoader:
    """
    [ETLç»„ä»¶] åŸå§‹æ•°æ®åŠ è½½å™¨åŸºç±» (Base Ingest Loader) - Enterprise V2.0

    èŒè´£:
    1. æ–‡ä»¶æ‰«æ: éå†æŒ‡å®šç›®å½•ä¸‹çš„ CSV æ–‡ä»¶ã€‚
    2. æ™ºèƒ½å—…æ¢: è¯†åˆ« Seller (åº—é“º) å’Œ Header (è¡¨å¤´è¡Œ)ã€‚
    3. æ•°æ®æ¸…æ´—: åŸºç¡€çš„ç©ºå€¼å¤„ç†å’Œæ ¼å¼å½’ä¸€åŒ–ã€‚
    4. æ‰¹é‡å…¥åº“: å°†æ¸…æ´—åçš„ DataFrame å†™å…¥ MySQL Raw è¡¨ã€‚
    """

    def __init__(self):
        self.db = DBClient()
        self.logger = get_logger(self.__class__.__name__)
        # ä½¿ç”¨ pathlib å¤„ç†è·¯å¾„ï¼Œæ›´å®‰å…¨è·¨å¹³å°
        self.uploader_dir = Config.UPLOADER_DIR

    def _clean_cell(self, val: str) -> str:
        """[è¾…åŠ©] å•å…ƒæ ¼æ¸…æ´—ï¼šè½¬å°å†™ã€å»å¼•å·ã€å»é¦–å°¾ç©ºæ ¼"""
        if not isinstance(val, str):
            return str(val) if val is not None else ""
        return val.lower().replace('"', '').replace("'", "").strip()

    def _detect_metadata(self, file_path: Path, keyword: str) -> Tuple[Optional[str], int]:
        """
        [æ™ºèƒ½å—…æ¢]
        é€»è¾‘ï¼š
        1. è¯†åˆ« Seller: é¦–åˆ—ä¸º "seller" çš„è¡Œçš„ä¸‹ä¸€åˆ—ã€‚
        2. è¯†åˆ«è¡¨å¤´(skiprows): å¯»æ‰¾åŒ…å«ç‰¹å®š keyword çš„è¡Œä½œä¸º Headerã€‚
        """
        seller = None
        skiprows = 0
        target_key = keyword.lower()

        try:
            # ä½¿ç”¨ utf-8-sig å¤„ç† BOM å¤´ï¼Œerrors='replace' é˜²æ­¢ç¼–ç æŠ¥é”™å´©æºƒ
            with file_path.open('r', encoding='utf-8-sig', errors='replace') as f:
                reader = csv.reader(f)
                lines = []
                # åªè¯»å‰ 30 è¡Œè¿›è¡Œæ¢æµ‹ï¼Œæ•ˆç‡æœ€é«˜
                for _ in range(30):
                    try:
                        row = next(reader)
                        lines.append(row)
                    except StopIteration:
                        break

            for i, row in enumerate(lines):
                if not row: continue

                # æ¸…æ´—æ•´è¡Œæ•°æ®ä»¥ä¾¿æ¯”å¯¹
                clean_row = [self._clean_cell(x) for x in row]

                # --- ä»»åŠ¡ A: è¯†åˆ« Seller ---
                # è§„åˆ™: ç¬¬ä¸€åˆ—æ˜¯ "seller"ï¼Œåˆ™ç¬¬äºŒåˆ—é€šå¸¸æ˜¯åº—é“ºå
                if len(clean_row) >= 2 and clean_row[0] == "seller":
                    if clean_row[1]:
                        seller = clean_row[1]

                # --- ä»»åŠ¡ B: è¯†åˆ«è¡¨å¤´è¡Œ (skiprows) ---
                # è§„åˆ™: åªè¦è¿™ä¸€è¡Œé‡ŒåŒ…å«äº†æ ¸å¿ƒå…³é”®è¯ (å¦‚ "order number")ï¼Œå°±è®¤ä¸ºæ˜¯è¡¨å¤´
                if target_key in clean_row:
                    skiprows = i
                    # æ‰¾åˆ°è¡¨å¤´åï¼ŒSeller ä¿¡æ¯å¯èƒ½åœ¨è¡¨å¤´ä¹‹ä¸Šï¼Œä¹Ÿå¯èƒ½åœ¨ä¹‹ä¸‹(ç½•è§)ï¼Œ
                    # ä½†ä¸ºäº†æ€§èƒ½ï¼Œå¦‚æœæˆ‘ä»¬å·²ç»æ‰¾åˆ°äº† Seller å’Œ Headerï¼Œå…¶å®å¯ä»¥ breakï¼Œ
                    # é™¤é Seller åœ¨ Header ä¹‹åã€‚ä¿å®ˆèµ·è§ï¼Œæˆ‘ä»¬ç»§ç»­éå†å®Œ 30 è¡Œã€‚

            return seller, skiprows

        except Exception as e:
            self.logger.warning(f"å…ƒæ•°æ®æ¢æµ‹å¼‚å¸¸ ({file_path.name}): {e}")
            return None, 0

    def _infer_seller_from_filename(self, filename: str) -> Optional[str]:
        """[å…œåº•ç­–ç•¥] ä»æ–‡ä»¶åçŒœæµ‹ Seller"""
        fname = filename.lower()
        if "88" in fname:
            return "esparts88"
        elif "plus" in fname:
            return "espartsplus"
        return None

    def _read_csv_safe(self, path: Path, skip_rows: int) -> pd.DataFrame:
        """[å®‰å…¨è¯»å–] å°è£… Pandas è¯»å–é€»è¾‘ï¼Œå¤„ç†åè¡Œ"""
        try:
            # on_bad_lines='skip': è·³è¿‡åˆ—æ•°ä¸åŒ¹é…çš„è¡Œï¼Œé˜²æ­¢è§£æé”™è¯¯
            df = pd.read_csv(
                path,
                skiprows=skip_rows,
                encoding='utf-8-sig',  # å…¼å®¹å¸¦ BOM çš„ UTF-8
                engine='python',  # Python å¼•æ“å®¹é”™æ€§æ›´å¥½
                dtype=str,  # å…¨è¯»ä¸ºå­—ç¬¦ä¸²ï¼Œé˜²æ­¢è‡ªåŠ¨ç±»å‹è½¬æ¢ä¸¢å¤±å‰å¯¼0
                on_bad_lines='skip'
            )
            return df
        except pd.errors.EmptyDataError:
            self.logger.warning(f"æ–‡ä»¶ä¸ºç©º: {path.name}")
            return pd.DataFrame()
        except Exception as e:
            self.logger.error(f"Pandas è¯»å–å¤±è´¥ ({path.name}): {e}")
            return pd.DataFrame()

    def _load_csv_to_db(self, file_paths: List[Path], table_name: str, header_keyword: str) -> bool:
        """
        [æ ¸å¿ƒæµç¨‹] é€šç”¨ CSV å…¥åº“é€»è¾‘
        """
        if not file_paths:
            self.logger.info(f"æœªæ‰¾åˆ°å¾…å¤„ç†æ–‡ä»¶ï¼Œè·³è¿‡ {table_name} åŠ è½½ã€‚")
            return False

        all_chunks = []
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦ï¼Œdesc ç”¨äº UI å±•ç¤º
        pbar = tqdm.tqdm(file_paths, desc=f"Ingesting {table_name}")

        for path in pbar:
            try:
                # 1. æ¢æµ‹å…ƒæ•°æ®
                seller, skip_rows = self._detect_metadata(path, header_keyword)

                # 2. å…œåº• Seller
                if not seller:
                    seller = self._infer_seller_from_filename(path.name)
                    if not seller:
                        self.logger.warning(f"æ— æ³•è¯†åˆ« Sellerï¼Œè·³è¿‡æ–‡ä»¶: {path.name}")
                        continue

                # 3. è¯»å–æ•°æ®
                df = self._read_csv_safe(path, skip_rows)
                if df.empty: continue

                # 4. æ³¨å…¥å…ƒæ•°æ®
                df["Seller"] = seller
                # å½’ä¸€åŒ–åˆ—å: å»é™¤å‰åç©ºæ ¼
                df.columns = [str(c).strip() for c in df.columns]

                # 5. ç»“æ„æ ¡éªŒ
                # å¦‚æœæ‰¾ä¸åˆ°æ ¸å¿ƒåˆ—ï¼Œè¯´æ˜ skiprows å¯èƒ½ç®—é”™äº†ï¼Œæˆ–è€…æ–‡ä»¶å®Œå…¨ä¸å¯¹
                if header_keyword not in df.columns:
                    self.logger.warning(f"æ–‡ä»¶ç»“æ„å¼‚å¸¸ (ç¼ºå¤±åˆ— '{header_keyword}'): {path.name}")
                    continue

                # 6. åŸºç¡€æ¸…æ´—
                # æ›¿æ¢å¸¸è§ç©ºå€¼æ ‡è®°ä¸º NaNï¼Œç„¶ååˆ é™¤å…¨ç©ºè¡Œ
                df = df.replace(['--', '-', 'N/A', 'null', 'nan', 'None'], np.nan)
                df = df.dropna(how='all')

                if not df.empty:
                    all_chunks.append(df)

            except Exception as e:
                self.logger.error(f"å¤„ç†æ–‡ä»¶å‡ºé”™ ({path.name}): {e}")

        if not all_chunks:
            self.logger.warning(f"æ²¡æœ‰æœ‰æ•ˆæ•°æ®è¢«åŠ è½½åˆ° {table_name}ã€‚")
            return False

        # 7. åˆå¹¶æ•°æ®
        self.logger.info(f"æ­£åœ¨åˆå¹¶ {len(all_chunks)} ä¸ªæ–‡ä»¶å—...")
        try:
            final_df = pd.concat(all_chunks, ignore_index=True)
        except Exception as e:
            self.logger.error(f"æ•°æ®åˆå¹¶å¤±è´¥: {e}")
            return False

        # 8. å†™å…¥æ•°æ®åº“ (å…¨é‡è¦†ç›– Raw è¡¨)
        row_count = len(final_df)
        self.logger.info(f"æ­£åœ¨å†™å…¥æ•°æ®åº“ `{table_name}` ({row_count} è¡Œ)...")

        try:
            # æ˜¾å¼æ¸…ç©ºæ—§è¡¨ (è™½ç„¶ replace ä¹Ÿä¼šåšï¼Œä½†è¿™æ ·æ›´ç¨³å¦¥ï¼Œç‰¹åˆ«æ˜¯ schema å˜åŠ¨æ—¶)
            self.db.truncate_table(table_name)

            # æ˜ å°„æ‰€æœ‰åˆ—ä¸º Text ç±»å‹ï¼Œé˜²æ­¢ varchar é•¿åº¦æº¢å‡ºï¼ŒETL Raw å±‚åº”å°½é‡å®½å®¹
            dtype_mapping = {col: Text() for col in final_df.columns}

            final_df.to_sql(
                table_name,
                self.db.get_engine(),
                if_exists='replace',
                index=False,
                chunksize=2000,  # åˆ†æ‰¹å†™å…¥ï¼Œé˜²æ­¢ Packet Too Large
                dtype=dtype_mapping
            )
            self.logger.info(f" {table_name} åŠ è½½å®Œæˆï¼Œå…± {row_count} æ¡è®°å½•ã€‚")
            return True

        except Exception as e:
            self.logger.error(f"æ•°æ®åº“å†™å…¥ä¸¥é‡é”™è¯¯: {e}")
            return False


class TransactionLoader(BaseLoader):
    """
    [ETL] eBay äº¤æ˜“æµæ°´åŠ è½½å™¨
    Target: Data_Transaction
    Key: Order number
    """

    def run(self):
        target_dir = self.uploader_dir / "Ebay Transaction"
        if not target_dir.exists():
            self.logger.warning(f"ç›®å½•ä¸å­˜åœ¨: {target_dir}")
            return

        # æ‰«ææ‰€æœ‰ .csv æ–‡ä»¶
        files = list(target_dir.glob("*.csv"))
        self.logger.info(f"æ‰«æåˆ° {len(files)} ä¸ª Transaction æ–‡ä»¶")

        # æ ¸å¿ƒå…³é”®è¯: Order number
        self._load_csv_to_db(files, "Data_Transaction", "Order number")


class EarningLoader(BaseLoader):
    """
    [ETL] èµ„é‡‘æµæ°´åŠ è½½å™¨
    Target: Data_Order_Earning
    Key: Order number
    """

    def run(self):
        target_dir = self.uploader_dir / "Order Earning"
        if not target_dir.exists():
            self.logger.warning(f"ç›®å½•ä¸å­˜åœ¨: {target_dir}")
            return

        files = list(target_dir.glob("*.csv"))
        self.logger.info(f"æ‰«æåˆ° {len(files)} ä¸ª Earning æ–‡ä»¶")

        # æ ¸å¿ƒå…³é”®è¯: Order number
        self._load_csv_to_db(files, "Data_Order_Earning", "Order number")

==================== END FILE: core/etl/ingest.py ====================


==================== START FILE: core/etl/parser.py ====================
# core/etl/parser.py

import pandas as pd
import numpy as np
import re
import tqdm
from typing import Dict, Any
from sqlalchemy.types import Text

from core.repository.db_client import DBClient
from core.services.correction_service import CorrectionService
from core.logging_config import get_logger

# æŠ‘åˆ¶ Pandas çš„ FutureWarning
pd.set_option('future.no_silent_downcasting', True)


class TransactionParser:
    """
    [ETLç»„ä»¶] äº¤æ˜“æ•°æ®è§£æå™¨ (Stage 2: Parsing & Validation) - Enterprise V2.0

    èŒè´£:
    1. ç»“æ„åŒ–è§£æ: ä» 'Custom label' å­—æ®µä¸­æå– SKU å’Œ Quantityã€‚
    2. æ¨¡å¼è¯†åˆ«: æ”¯æŒ Single (å•å“), Dual (åŒå“), Complex (å¤šå“/ç‰¹æ®Šç¬¦) æ ¼å¼ã€‚
    3. æ•°æ®æ ¡éªŒ: éªŒè¯æå–å‡ºçš„ SKU æ˜¯å¦åœ¨ç³»ç»Ÿèµ„æ–™åº“ (Data_COGS) ä¸­å­˜åœ¨ã€‚
    4. è‡ªåŠ¨ä¿®å¤: åŸºäº CorrectionService çš„è®°å¿†åº“è‡ªåŠ¨ä¿®æ­£é”™è¯¯çš„ SKUã€‚
    """

    def __init__(self):
        self.db = DBClient()
        self.logger = get_logger(self.__class__.__name__)
        self.corrector = CorrectionService()

        # ç¼“å­˜æœ‰æ•ˆ SKU å’Œä¿®å¤å­—å…¸ï¼Œå‡å°‘æ•°æ®åº“ IO
        self.valid_skus = self.corrector.valid_skus
        self.fix_map = self._build_fast_fix_map()

        # å®šä¹‰éœ€è¦è¾“å‡ºçš„è§£æåˆ—
        self.parse_cols = ['P_Flag', 'P_Key', 'P_Type', 'P_Check', 'Skufix_Check']
        for i in range(1, 11):
            self.parse_cols.extend([f'P_SKU{i}', f'P_Quantity{i}'])

    def _build_fast_fix_map(self) -> Dict[str, dict]:
        """ä»è®°å¿†åº“æ„å»ºå¿«é€ŸæŸ¥æ‰¾å­—å…¸"""
        memory_dict = {}
        if not self.corrector.memory_df.empty:
            for _, row in self.corrector.memory_df.iterrows():
                bad = str(row.get('BadSKU', '')).strip().upper()
                if bad:
                    memory_dict[bad] = {
                        'sku': str(row.get('CorrectSKU', '')).strip().upper(),
                        'qty': str(row.get('CorrectQty', '')).strip()
                    }
        return memory_dict

    def _init_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆå§‹åŒ–å¿…è¦çš„åˆ—ç»“æ„"""
        # æ ‡å‡†åŒ–åˆ—å
        col_map = {c.lower().replace(" ", ""): c for c in df.columns}
        if 'customlabel' in col_map:
            df.rename(columns={col_map['customlabel']: 'Custom label'}, inplace=True)
        elif 'Custom label' not in df.columns:
            df['Custom label'] = ''

        # åˆå§‹åŒ– P_* åˆ—
        for col in self.parse_cols:
            if col not in df.columns:
                df[col] = None

        if 'Item title' not in df.columns:
            df['Item title'] = ''

        # ç¡®ä¿æ ‡è®°åˆ—ä¸ºæ•´æ•°ç±»å‹
        df['P_Flag'] = df['P_Flag'].fillna(0).infer_objects(copy=False).astype(int)
        return df

    def _apply_regex_patterns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        [é«˜æ€§èƒ½] ä½¿ç”¨å‘é‡åŒ–æ­£åˆ™æ‰¹é‡è§£ææ ‡å‡†æ ¼å¼
        """
        self.logger.info(" æ‰§è¡Œæ­£åˆ™è§£æ (Vectorized)...")

        s_label = df['Custom label'].astype(str).str.strip()
        mask_todo = (df['P_Flag'] == 0)

        # 1. Pattern: Single (e.g., "ABCD-1234.1")
        # å…è®¸å­—ç¬¦: A-Z, 0-9, -, /, _
        pat1 = r'^(?:[A-Za-z]{1}[A-Za-z0-9]{0,2}\.)?(?P<SKU>[A-Za-z0-9\-_/]{7,})\.(?P<Quantity>\d{1,3})(?P<QuantityKey>\+2K)?(?:\.[A-Za-z0-9_]*){0,2}$'

        ext1 = s_label[mask_todo].str.extract(pat1)
        idx1 = ext1[ext1['SKU'].notna()].index

        if not idx1.empty:
            df.loc[idx1, 'P_Flag'] = 1
            df.loc[idx1, 'P_Type'] = 'single'
            df.loc[idx1, 'P_SKU1'] = ext1.loc[idx1, 'SKU']
            df.loc[idx1, 'P_Quantity1'] = ext1.loc[idx1, 'Quantity']
            df.loc[idx1, 'P_Key'] = ext1.loc[idx1, 'QuantityKey'].apply(lambda x: 2 if pd.notna(x) else 0)
            df.loc[idx1, 'Skufix_Check'] = 1

        # 2. Pattern: Dual (e.g., "PART1.1+PART2.1")
        mask_todo = (df['P_Flag'] == 0)  # åˆ·æ–° mask
        pat2 = r'^(?:[A-Za-z]{1}[A-Za-z0-2]{0,2}\.)?(?P<S1>[A-Za-z0-9/\-_]{7,})\.(?P<Q1>\d{1,3})(?P<K1>\+2K)?[\+\.](?P<S2>[A-Za-z0-9/\-_]{7,})\.(?P<Q2>\d{1,3})(?P<K2>\+2K)?(?:\.[A-Za-z0-9_]*){0,2}$'

        ext2 = s_label[mask_todo].str.extract(pat2)
        idx2 = ext2[ext2['S1'].notna() & ext2['S2'].notna()].index

        if not idx2.empty:
            df.loc[idx2, 'P_Flag'] = 2
            df.loc[idx2, 'P_Type'] = 'dual'
            df.loc[idx2, 'P_SKU1'] = ext2.loc[idx2, 'S1']
            df.loc[idx2, 'P_Quantity1'] = ext2.loc[idx2, 'Q1']
            df.loc[idx2, 'P_SKU2'] = ext2.loc[idx2, 'S2']
            df.loc[idx2, 'P_Quantity2'] = ext2.loc[idx2, 'Q2']

            # è®¡ç®— Key å€¼
            k1 = ext2.loc[idx2, 'K1'].notna().astype(int) * 2
            k2 = ext2.loc[idx2, 'K2'].notna().astype(int) * 2
            df.loc[idx2, 'P_Key'] = k1 + k2
            df.loc[idx2, 'Skufix_Check'] = 1

        return df

    def _process_complex_rows(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        [å…œåº•] è¿­ä»£è§£ææ— æ³•è¢«æ­£åˆ™è¯†åˆ«çš„å¤æ‚è¡Œ
        """
        mask_complex = (df['P_Flag'] == 0)
        count = mask_complex.sum()
        if count == 0:
            return df

        self.logger.info(f"ğŸ¢ å¤„ç† {count} æ¡å¤æ‚è®°å½• (Iterative)...")

        junk_chars = {'--', '-', 'N/A', 'NULL', 'NONE', '', 'NAN'}
        prefix_pattern = re.compile(r'^(?:[A-Za-z]{1}[A-Za-z0-2]{0,2}\.)?(?P<main>.+?)(?:\.[A-Za-z0-9_]*)?$')

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        for idx in tqdm.tqdm(df[mask_complex].index, desc="Complex Parsing"):
            raw_label = str(df.at[idx, 'Custom label']).strip()
            match = prefix_pattern.match(raw_label)
            main_part = match.group('main') if match else raw_label

            parts = main_part.split('+')
            p_key = 0
            p_skus = []
            p_qtys = []

            valid_parse = False

            for seg in parts:
                seg = seg.strip()
                if not seg or seg.upper() in junk_chars: continue

                # å¤„ç† +2K æ ‡è®°
                if seg.upper() == '2K':
                    p_key += 2
                    continue
                if '+2K' in seg:
                    p_key += 2
                    seg = seg.replace('+2K', '')

                # æ‹†åˆ† SKU.Qty
                arr = seg.split('.')
                code = arr[0].upper().strip()
                qty = arr[1] if len(arr) > 1 else '1'

                if code in junk_chars: continue

                # æŸ¥è¡¨ä¿®æ­£ (Fast Fix)
                if code not in self.valid_skus and code in self.fix_map:
                    code = self.fix_map[code]['sku']
                    # æ³¨æ„: è¿™é‡Œå¯ä»¥å†³å®šæ˜¯å¦åŒæ—¶ä¿®æ­£ qtyï¼Œç›®å‰ä»…ä¿®æ­£ SKU ä»£ç 

                p_skus.append(code)
                p_qtys.append(qty)
                valid_parse = True

            if valid_parse:
                # é™åˆ¶æœ€å¤šå†™ 10 ä¸ªç»„ä»¶
                limit = min(len(p_skus), 10)
                for i in range(limit):
                    df.at[idx, f'P_SKU{i + 1}'] = p_skus[i]
                    df.at[idx, f'P_Quantity{i + 1}'] = p_qtys[i]

                df.at[idx, 'P_Flag'] = 5  # 5 ä»£è¡¨å¤æ‚è§£ææˆåŠŸ
                df.at[idx, 'P_Key'] = p_key
                df.at[idx, 'P_Check'] = 1
                df.at[idx, 'Skufix_Check'] = 1

        return df

    def _validate_and_autofix(self, df: pd.DataFrame) -> dict:
        """
        [æ ¡éªŒ] æ£€æŸ¥è§£æå‡ºçš„ SKU æ˜¯å¦æœ‰æ•ˆï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤
        """
        self.logger.info("ğŸ” æ‰§è¡Œ SKU æ ¡éªŒä¸è‡ªåŠ¨ä¿®å¤...")

        # åªæ ¡éªŒå·²è§£æä¸”æœªè¢«æ ‡è®°ä¸ºé”™è¯¯çš„è¡Œ
        mask_check = (df['P_Flag'] > 0) & (df['P_Flag'] != 99)
        fix_logs = []
        count_failed = 0

        for idx in tqdm.tqdm(df[mask_check].index, desc="Validation"):
            is_row_valid = True
            custom_label = str(df.at[idx, 'Custom label'])
            order_num = str(df.at[idx, 'Order number'])

            for i in range(1, 11):
                sku_col = f'P_SKU{i}'
                qty_col = f'P_Quantity{i}'
                sku = df.at[idx, sku_col]

                # è·³è¿‡ç©ºå€¼
                if pd.isna(sku) or str(sku).strip() == "": continue

                sku = str(sku).strip().upper()

                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨äº COGS è¡¨
                if self.corrector.is_valid_sku(sku):
                    continue

                # å°è¯•è‡ªåŠ¨ä¿®å¤
                fixed_sku, fixed_qty = self.corrector.find_auto_fix(custom_label, sku)

                if fixed_sku:
                    df.at[idx, sku_col] = fixed_sku
                    msg = f"SKU: {sku} -> {fixed_sku}"

                    if fixed_qty and str(fixed_qty).strip():
                        df.at[idx, qty_col] = fixed_qty
                        msg += f", Qty -> {fixed_qty}"

                    fix_logs.append({"order": order_num, "msg": msg})
                else:
                    is_row_valid = False

            if not is_row_valid:
                df.at[idx, 'P_Flag'] = 99  # 99 ä»£è¡¨æ ¡éªŒå¤±è´¥ï¼Œéœ€äººå·¥ä»‹å…¥
                count_failed += 1

        self.logger.info(f" æ ¡éªŒå®Œæˆ: è‡ªåŠ¨ä¿®å¤ {len(fix_logs)} é¡¹ï¼Œå‰©ä½™ {count_failed} è¡Œå¼‚å¸¸ã€‚")
        return {"fixed_logs": fix_logs, "failed_count": count_failed, "df": df}

    def run(self) -> dict:
        """ä¸»æ‰§è¡Œé€»è¾‘"""
        self.logger.info("ğŸš€ [Parser] å¼€å§‹è§£æäº¤æ˜“æ•°æ®...")

        # 1. è¯»å– Raw Data
        sql = "SELECT * FROM Data_Transaction"
        df = self.db.read_df(sql)

        if df.empty:
            self.logger.warning("Data_Transaction è¡¨ä¸ºç©ºï¼Œè·³è¿‡è§£æã€‚")
            return {"status": "empty"}

        # 2. åˆå§‹åŒ–ä¸è§£æ
        df = self._init_columns(df)
        df = self._apply_regex_patterns(df)
        df = self._process_complex_rows(df)

        # 3. æ ¡éªŒä¸ä¿®å¤
        result = self._validate_and_autofix(df)
        df_final = result["df"]

        # 4. å›å†™æ•°æ®åº“
        self.logger.info("ğŸ’¾ æ­£åœ¨ä¿å­˜è§£æç»“æœ...")

        # æ˜ å°„æ‰€æœ‰åˆ—ä¸º Text ç±»å‹ï¼Œç¡®ä¿å…¼å®¹æ€§
        dtype_map = {c: Text() for c in df_final.columns}

        try:
            df_final.to_sql(
                "Data_Transaction",
                self.db.get_engine(),
                if_exists='replace',
                index=False,
                dtype=dtype_map,
                chunksize=2000
            )
            self.logger.info(" æ•°æ®åº“æ›´æ–°æˆåŠŸã€‚")
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“å›å†™å¤±è´¥: {e}")
            return {"status": "error", "message": str(e)}

        return {
            "status": "success",
            "auto_fixed": result["fixed_logs"],
            "pending_count": result["failed_count"]
        }

==================== END FILE: core/etl/parser.py ====================


==================== START FILE: core/etl/transformer.py ====================
# core/etl/transformer.py

import pandas as pd
import numpy as np
from sqlalchemy import text
from typing import Callable, Optional
from sqlalchemy.types import Text

from core.repository.db_client import DBClient
from core.logging_config import get_logger


class TransactionTransformer:
    """
    [ETLæ ¸å¿ƒç»„ä»¶] æ•°æ®è½¬æ¢åŒæ­¥å¼•æ“ (V3.1 Performance + Robust + Full Logic)

    å…³é”®ç‰¹æ€§:
    1. ä¸šåŠ¡é€»è¾‘ (Business Logic): 100% ä¿æŒåŸç‰ˆ (Action/Seller/Shipping/Proration)ã€‚
    2. å¥å£®æ€§ (Robustness): å¢åŠ æ•°å€¼å®‰å…¨è½¬æ¢ï¼Œé˜²æ­¢è„æ•°æ®æŠ¥é”™ã€‚
    3. äº‹åŠ¡æ€§ (Atomicity): ä½¿ç”¨ atomic_transaction ç¡®ä¿åŸå­å†™å…¥ã€‚
    4. é«˜æ€§èƒ½ (Performance): åœ¨ Staging è¡¨å†™å…¥åè‡ªåŠ¨åˆ›å»ºç´¢å¼•ï¼ŒåŠ é€Ÿåç»­æŸ¥è¯¢ã€‚
    """

    def __init__(self):
        self.db = DBClient()
        self.logger = get_logger(self.__class__.__name__)

        # å®šä¹‰æ ‡å‡†è¾“å‡º Schema
        self.output_cols = [
            'order date', 'seller', 'order number', 'item id', 'item title', 'full sku', 'quantity',
            'revenue', 'Shipping and handling', 'Seller collected tax', 'eBay collected tax',
            'Final Value Fee - fixed', 'Final Value Fee - variable', 'Regulatory operating fee',
            'International fee', 'Promoted Listings fee', 'Payments dispute fee',
            'action', 'Refund',
            'Shipping label-Earning data', 'Shipping label-Regular',
            'Shipping label-underpay', 'Shipping label-overpay', 'Shipping label-Return',
            'buyer username', 'ship to city', 'ship to country'
        ]
        for i in range(1, 11):
            self.output_cols.extend([f'sku{i}', f'qty{i}', f'qtyp{i}'])

    def _safe_float_convert(self, series: pd.Series) -> pd.Series:
        """[é˜²å¾¡æ€§] å®‰å…¨æ•°å€¼è½¬æ¢: å»é™¤ $, ç©ºæ ¼, å¤„ç†å¼‚å¸¸å€¼"""
        if series.empty: return series
        # 1. è½¬ä¸ºå­—ç¬¦ä¸²å¹¶æ¸…æ´—å¹²æ‰°å­—ç¬¦
        clean = series.astype(str).str.replace(r'[$,\s]', '', regex=True)
        # 2. å¼ºåˆ¶è½¬æ•°å­—ï¼Œæ— æ³•è½¬æ¢çš„å˜ä¸º NaN
        nums = pd.to_numeric(clean, errors='coerce')
        # 3. å¡«å…… 0.0
        return nums.fillna(0.0)

    def run(self, progress_callback: Optional[Callable[[float, str], None]] = None):
        """æ‰§è¡Œ ETL è½¬æ¢æµç¨‹"""

        def report(p, msg):
            if progress_callback: progress_callback(p, msg)
            self.logger.info(msg)

        try:
            report(0.05, "ğŸš€ [Transformer] å¯åŠ¨è½¬æ¢å¼•æ“...")

            # 1. è¯»å–æ•°æ®
            df_trans = self.db.read_df("SELECT * FROM Data_Transaction")
            df_earn = self.db.read_df("SELECT * FROM Data_Order_Earning")

            if df_trans.empty:
                report(1.0, "âš ï¸ æºæ•°æ®ä¸ºç©ºï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
                return

            # å½’ä¸€åŒ–åˆ—å
            df_trans.columns = df_trans.columns.str.strip().str.lower()
            df_earn.columns = df_earn.columns.str.strip().str.lower()

            # 2. é¢„å¤„ç† (æ•°å€¼æ¸…æ´—)
            report(0.15, "ğŸ§¹ æ‰§è¡Œæ•°å€¼æ¸…æ´—...")
            potential_num_cols = [
                'item subtotal', 'quantity', 'gross transaction amount',
                'shipping and handling', 'seller collected tax', 'ebay collected tax',
                'final value fee - fixed', 'final value fee - variable', 'regulatory operating fee',
                'international fee', 'promoted listings fee', 'payments dispute fee', 'refund'
            ]
            for c in potential_num_cols:
                if c in df_trans.columns:
                    df_trans[c] = self._safe_float_convert(df_trans[c])

            # å¤„ç† Earning è¡¨è¿è´¹
            if not df_earn.empty and 'shipping labels' in df_earn.columns:
                df_earn['shipping labels'] = self._safe_float_convert(df_earn['shipping labels'])
                earn_map = df_earn.groupby('order number')['shipping labels'].sum().reset_index()
                earn_map.rename(columns={'shipping labels': 'Shipping label-Earning data'}, inplace=True)
            else:
                earn_map = pd.DataFrame(columns=['order number', 'Shipping label-Earning data'])

            # 3. ä¸šåŠ¡é€»è¾‘è®¡ç®— (Action & Seller)
            report(0.30, "ğŸ§  è®¡ç®—ä¸šåŠ¡è§„åˆ™ (Action/Seller)...")

            if 'type' not in df_trans.columns: df_trans['type'] = ''
            if 'reference id' not in df_trans.columns: df_trans['reference id'] = ''
            if 'seller' not in df_trans.columns: df_trans['seller'] = 'Unknown'

            # --- [æ ¸å¿ƒä¸šåŠ¡é€»è¾‘å¼€å§‹: ä¿æŒåŸæ ·] ---
            df_trans['type_lower'] = df_trans['type'].astype(str).str.lower()
            df_trans['ref_lower'] = df_trans['reference id'].astype(str).str.lower()

            # åˆå§‹åŒ–æƒé‡
            df_trans['action_weight'] = 0
            df_trans['action_code'] = 'NN'

            # Payment Dispute (æƒé‡ 50)
            mask_pd = df_trans['type_lower'] == 'payment dispute'
            df_trans.loc[mask_pd, 'action_weight'] = 50
            df_trans.loc[mask_pd, 'action_code'] = 'PD'

            # Claim (æƒé‡ 40/30)
            mask_claim = df_trans['type_lower'] == 'claim'
            mask_case = mask_claim & df_trans['ref_lower'].str.contains('case', case=False)
            df_trans.loc[mask_case, 'action_code'] = 'CC'
            df_trans.loc[mask_case, 'action_weight'] = 40

            mask_req = mask_claim & df_trans['ref_lower'].str.contains('request', case=False)
            df_trans.loc[mask_req, 'action_code'] = 'CR'
            df_trans.loc[mask_req, 'action_weight'] = 30

            # Refund (æƒé‡ 20/10)
            mask_refund = df_trans['type_lower'] == 'refund'
            mask_ret = mask_refund & df_trans['ref_lower'].str.contains('return', case=False)
            df_trans.loc[mask_ret, 'action_code'] = 'RE'
            df_trans.loc[mask_ret, 'action_weight'] = 20

            mask_cancel = mask_refund & df_trans['ref_lower'].str.contains('cancel', case=False)
            df_trans.loc[mask_cancel, 'action_code'] = 'CA'
            df_trans.loc[mask_cancel, 'action_weight'] = 10

            # èšåˆå–æœ€å¤§æƒé‡ Action
            action_map = df_trans.sort_values('action_weight', ascending=False).drop_duplicates('order number')[
                ['order number', 'action_code']]
            action_map.rename(columns={'action_code': 'action'}, inplace=True)

            # æå– Seller
            df_trans['seller_clean'] = df_trans['seller'].astype(str).str.strip().str.replace(r'[\'"]', '', regex=True)
            df_trans['is_priority_seller'] = df_trans['seller_clean'].str.lower().str.contains('esparts').astype(int)

            seller_map = df_trans.sort_values(['is_priority_seller', 'seller_clean'], ascending=[False, True]) \
                .drop_duplicates('order number')[['order number', 'seller_clean']]
            seller_map.rename(columns={'seller_clean': 'seller'}, inplace=True)
            # --- [æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ç»“æŸ] ---

            # 4. æå–ç‰©æµè´¹ç”¨ (Shipping Cost Extraction)
            report(0.50, "ğŸšš æå–éšæ€§ç‰©æµæˆæœ¬...")
            mask_ship = df_trans['type_lower'] == 'shipping label'
            df_ship = df_trans[mask_ship].copy()

            if 'description' not in df_ship.columns: df_ship['description'] = ''
            df_ship['desc_lower'] = df_ship['description'].astype(str).str.lower()
            df_ship['amt'] = df_ship['gross transaction amount']

            df_ship['underpay'] = np.where(df_ship['desc_lower'].str.contains('underpaid'), df_ship['amt'], 0.0)
            df_ship['overpay'] = np.where(df_ship['desc_lower'].str.contains('overpaid'), df_ship['amt'], 0.0)
            df_ship['return'] = np.where(df_ship['desc_lower'].str.contains('return shipping'), df_ship['amt'], 0.0)
            df_ship['regular'] = np.where(
                (~df_ship['desc_lower'].str.contains('underpaid|overpaid|return|voided|bulk')),
                df_ship['amt'], 0.0
            )

            ship_agg = df_ship.groupby('order number')[['underpay', 'overpay', 'return', 'regular']].sum().reset_index()
            ship_agg.columns = ['order number', 'Shipping label-underpay', 'Shipping label-overpay',
                                'Shipping label-Return', 'Shipping label-Regular']

            # 5. ä¸»è¡¨æ„å»ºä¸åˆ†æ‘Š
            report(0.70, "ğŸ§® æ‰§è¡Œè®¢å•çº§è´¹ç”¨åˆ†æ‘Š...")
            mask_order = (df_trans['type_lower'] == 'order') & (df_trans['item id'].notna())
            df_main = df_trans[mask_order].copy()

            # [Defensive] åˆ é™¤å¯èƒ½å­˜åœ¨çš„å†²çªåˆ—
            cols_to_drop = []
            if 'seller' in df_main.columns: cols_to_drop.append('seller')
            if 'action' in df_main.columns: cols_to_drop.append('action')
            if cols_to_drop: df_main.drop(columns=cols_to_drop, inplace=True)

            # å…³è”å…ƒæ•°æ®
            df_main = df_main.merge(action_map, on='order number', how='left')
            df_main = df_main.merge(seller_map, on='order number', how='left')
            df_main = df_main.merge(earn_map, on='order number', how='left')
            df_main = df_main.merge(ship_agg, on='order number', how='left')
            df_main.fillna(0, inplace=True)

            # è´¹ç”¨åˆ†æ‘Š (Weighted Proration)
            order_totals = df_main.groupby('order number')['item subtotal'].transform('sum')
            # é¿å…é™¤ä»¥0
            df_main['ratio'] = np.where(order_totals != 0, df_main['item subtotal'] / order_totals, 0.0)

            prorate_cols = [
                'Shipping label-Earning data', 'Shipping label-underpay',
                'Shipping label-overpay', 'Shipping label-Return', 'Shipping label-Regular'
            ]
            for col in prorate_cols:
                if col in df_main.columns:
                    df_main[col] = df_main[col] * df_main['ratio']

            # åˆ—åæ˜ å°„
            col_mapping = {
                'transaction creation date': 'order date',
                'item subtotal': 'revenue',
                'shipping and handling': 'Shipping and handling',
                'seller collected tax': 'Seller collected tax',
                'ebay collected tax': 'eBay collected tax',
                'final value fee - fixed': 'Final Value Fee - fixed',
                'final value fee - variable': 'Final Value Fee - variable',
                'regulatory operating fee': 'Regulatory operating fee',
                'international fee': 'International fee',
                'promoted listings fee': 'Promoted Listings fee'
            }
            df_main.rename(columns=col_mapping, inplace=True)

            # 6. SKU è§£æ
            sku_parts_list = []
            for i in range(1, 11):
                s_col = f'p_sku{i}'
                q_col = f'p_quantity{i}'

                if s_col not in df_main.columns:
                    df_main[f'sku{i}'] = ''
                    df_main[f'qty{i}'] = 0
                    df_main[f'qtyp{i}'] = 0
                    continue

                df_main[f'sku{i}'] = df_main[s_col]
                df_main[f'qty{i}'] = self._safe_float_convert(df_main[q_col])
                df_main[f'qtyp{i}'] = df_main[f'qty{i}'] * df_main['quantity']

                mask_has_sku = df_main[f'sku{i}'].notna() & (df_main[f'sku{i}'] != '')
                part = df_main.loc[mask_has_sku, f'sku{i}'].astype(str) + "." + df_main.loc[
                    mask_has_sku, f'qty{i}'].astype(int).astype(str)
                sku_parts_list.append(part)

            if sku_parts_list:
                df_parts = pd.concat(sku_parts_list, axis=1)
                df_main['full sku'] = df_parts.apply(lambda x: "+".join(x.dropna()), axis=1)
            else:
                df_main['full sku'] = ''

            df_main['order date'] = pd.to_datetime(df_main['order date'], errors='coerce').dt.strftime('%Y-%m-%d')

            # 7. æ•°æ®åº“åŒæ­¥ (ä½¿ç”¨åŸå­äº‹åŠ¡ + ç´¢å¼•ä¼˜åŒ–)
            report(0.90, f"ğŸ’¾ åŸå­åŒ–å†™å…¥æ•°æ®åº“ ({len(df_main)} è¡Œ)...")

            df_final = pd.DataFrame()
            for c in self.output_cols:
                if c in df_main.columns:
                    df_final[c] = df_main[c]
                else:
                    # è¡¥å…¨ç¼ºå¤±åˆ—ï¼Œæ•°å€¼å¡«0ï¼Œå­—ç¬¦å¡«ç©º
                    df_final[c] = 0 if 'fee' in c.lower() or 'label' in c.lower() else ''

            staging_table = "Data_Clean_Log_Staging"
            target_table = "Data_Clean_Log"

            with self.db.atomic_transaction() as conn:
                # 7.1 å†™å…¥ Staging
                df_final.to_sql(
                    staging_table, conn, if_exists='replace', index=False,
                    dtype={c: Text() for c in df_final.columns}, chunksize=2000
                )

                # 7.2 [New] ä¸º Staging è¡¨åˆ›å»ºé«˜æ€§èƒ½ç´¢å¼• (Critical Performance Boost)
                # è¿™ä¼šè®©åç»­çš„æŠ¥è¡¨æŸ¥è¯¢é€Ÿåº¦é£å‡
                indices = [
                    f"CREATE INDEX idx_order_date ON `{staging_table}` (`order date`(20))",
                    f"CREATE INDEX idx_full_sku ON `{staging_table}` (`full sku`(50))",
                    f"CREATE INDEX idx_seller ON `{staging_table}` (`seller`(20))",
                    f"CREATE INDEX idx_action ON `{staging_table}` (`action`(10))"
                ]
                for idx_sql in indices:
                    conn.execute(text(idx_sql))

                # 7.3 æ£€æŸ¥ Target æ˜¯å¦å­˜åœ¨
                exists = conn.execute(text(f"SHOW TABLES LIKE '{target_table}'")).first()
                if not exists:
                    conn.execute(text(f"RENAME TABLE `{staging_table}` TO `{target_table}`"))
                else:
                    # 7.4 ä¸¥æ ¼å»é‡ (ä¿ç•™: å¦‚æœå…¨å­—æ®µåŒ¹é…åˆ™åˆ é™¤æ—§çš„)
                    join_conds = [f"T1.`{c}` <=> T2.`{c}`" for c in self.output_cols]
                    join_sql = " AND ".join(join_conds)

                    # å…ˆåˆ æ‰å®Œå…¨é‡å¤çš„æ—§è®°å½•
                    dedup_sql = f"DELETE T1 FROM `{target_table}` T1 INNER JOIN `{staging_table}` T2 ON {join_sql}"
                    conn.execute(text(dedup_sql))

                    # å†æ’å…¥æ–°è®°å½•
                    cols_str = ", ".join([f"`{c}`" for c in self.output_cols])
                    insert_sql = f"INSERT INTO `{target_table}` ({cols_str}) SELECT {cols_str} FROM `{staging_table}`"
                    conn.execute(text(insert_sql))

                    conn.execute(text(f"DROP TABLE `{staging_table}`"))

                # 7.5 æ¸…ç©º Source
                conn.execute(text("TRUNCATE TABLE `Data_Transaction`"))
                conn.execute(text("TRUNCATE TABLE `Data_Order_Earning`"))

            report(1.0, " ETL è½¬æ¢æµç¨‹å…¨éƒ¨å®Œæˆ (ç´¢å¼•å·²æ„å»º)ï¼")

        except Exception as e:
            self.logger.error(f"Transformer Critical Error: {e}", extra={"trace": str(e)})
            report(1.0, f" å¤„ç†å¤±è´¥: {str(e)}")
            raise e
==================== END FILE: core/etl/transformer.py ====================


==================== START FILE: core/etl/inventory.py ====================
# core/etl/inventory.py

import pandas as pd
import numpy as np
from typing import Tuple, List, Dict, Any
from sqlalchemy import text
from sqlalchemy.types import Integer, String

from core.repository.db_client import DBClient
from core.repository.sku_repo import SkuRepository
from config import Config
from core.logging_config import get_logger


class InventoryLoader:
    """
    [ETLç»„ä»¶] æ™ºèƒ½åº“å­˜åŠ è½½å™¨ (Inventory ETL) - Enterprise V2.0

    èŒè´£:
    1. æ ¡éªŒ: æ£€æŸ¥ CSV æ ¼å¼ï¼Œç¡®ä¿ SKU åˆæ³• (å­˜åœ¨äº Data_COGS)ã€‚
    2. åŠ¨æ€DDL: è‡ªåŠ¨æ£€æµ‹å¹¶æ‰©å±•æ•°æ®åº“è¡¨ç»“æ„ (ADD COLUMN) ä»¥æ”¯æŒæ–°æœˆä»½ã€‚
    3. åŒæ­¥: ä½¿ç”¨ Staging Table æŠ€æœ¯é«˜æ•ˆæ‰¹é‡æ›´æ–°åº“å­˜ã€‚

    Reference:
    - åŸºäº V1.4.1 åŸç é€»è¾‘å¤åˆ»: åŠ¨æ€åˆ—æ£€æŸ¥ + ä¸´æ—¶è¡¨ JOIN æ›´æ–°ã€‚
    """

    def __init__(self):
        self.db = DBClient()
        self.logger = get_logger(self.__class__.__name__)
        self.sku_repo = SkuRepository()
        self.table_inv = "Data_Inventory"

    def validate_file(self, file_path: str) -> Tuple[bool, List[str], pd.DataFrame]:
        """
        [æ ¡éªŒé˜¶æ®µ] è¯»å–å¹¶æ£€æŸ¥ä¸Šä¼ çš„åº“å­˜æ–‡ä»¶
        """
        self.logger.info(f"æ­£åœ¨æ ¡éªŒåº“å­˜æ–‡ä»¶: {file_path}")
        try:
            # 1. è¯»å– CSV (å…¨éƒ¨è½¬ä¸ºå­—ç¬¦ä¸²è¯»å–ï¼Œé˜²æ­¢ '001' å˜æˆ 1)
            # ä½¿ç”¨ utf-8-sig å…¼å®¹ BOM
            df = pd.read_csv(file_path, dtype=str, encoding='utf-8-sig', on_bad_lines='skip')

            # å½’ä¸€åŒ–åˆ—å
            df.columns = [str(c).strip().lower() for c in df.columns]

            # 2. æ™ºèƒ½å¯»æ‰¾åˆ—å
            sku_col = next((c for c in df.columns if 'sku' in c), None)
            qty_col = next((c for c in df.columns if any(k in c for k in ['qty', 'quantity', 'amount'])), None)

            if not sku_col or not qty_col:
                msg = "CSV æ ¼å¼é”™è¯¯: æœªæ‰¾åˆ° 'SKU' æˆ– 'Quantity' åˆ—"
                self.logger.error(msg)
                return False, [msg], pd.DataFrame()

            # 3. æ•°æ®æ ‡å‡†åŒ–
            df = df[[sku_col, qty_col]].rename(columns={sku_col: 'SKU', qty_col: 'Quantity'})

            # æ¸…æ´— SKU: è½¬å¤§å†™ã€å»ç©ºæ ¼
            df['SKU'] = df['SKU'].astype(str).str.strip().str.upper()
            # è¿‡æ»¤æ— æ•ˆè¡Œ
            df = df[~df['SKU'].isin(['NAN', 'NONE', '', 'NULL'])]
            # æ¸…æ´— Quantity
            df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce').fillna(0).astype(int)

            # 4. SKU åˆæ³•æ€§æ¯”å¯¹
            valid_skus = set(self.sku_repo.get_valid_skus())
            upload_skus = set(df['SKU'].unique())

            unknown = list(upload_skus - valid_skus)

            if not unknown:
                self.logger.info(f" æ ¡éªŒé€šè¿‡: {len(df)} è¡Œæ•°æ®æœ‰æ•ˆã€‚")
                return True, [], df
            else:
                self.logger.warning(f"âš ï¸ å‘ç° {len(unknown)} ä¸ªéæ³• SKUã€‚")
                return False, unknown, df

        except Exception as e:
            self.logger.error(f"æ–‡ä»¶è¯»å–ä¸¥é‡é”™è¯¯: {e}")
            return False, [str(e)], pd.DataFrame()

    def _ensure_column_exists(self, col_name: str):
        """
        [æ¶æ„æ ¸å¿ƒ] åŠ¨æ€æ£€æŸ¥å¹¶æ·»åŠ æœˆä»½åˆ—
        Strictly follows V1.4.1 logic: checks information_schema first.
        """
        # å‚æ•°åŒ–æŸ¥è¯¢ï¼Œé˜²æ­¢ SQL æ³¨å…¥
        check_sql = """
                    SELECT COUNT(*) \
                    FROM information_schema.COLUMNS
                    WHERE TABLE_SCHEMA = :db
                      AND TABLE_NAME = :table
                      AND COLUMN_NAME = :col \
                    """
        params = {"db": Config.DB_NAME, "table": self.table_inv, "col": col_name}

        try:
            exists = self.db.read_df(check_sql, params).iloc[0, 0] > 0

            if not exists:
                self.logger.info(f"ğŸ›  æ£€æµ‹åˆ°æ–°æœˆä»½åˆ— '{col_name}' ä¸å­˜åœ¨ï¼Œæ‰§è¡Œ Schema æ‰©å±•...")
                # åˆ—åä½œä¸ºæ ‡è¯†ç¬¦ï¼Œéœ€ä½¿ç”¨åå¼•å·åŒ…è£¹ï¼Œä¸”ä¸èƒ½å‚æ•°åŒ–ã€‚
                # ä½†æ­¤å¤„ col_name æ¥è‡ªç¨‹åºå†…éƒ¨ç”Ÿæˆçš„ YYYY-MM æ ¼å¼ï¼Œç›¸å¯¹å®‰å…¨ã€‚
                safe_col = f"`{col_name.replace('`', '')}`"
                alter_sql = f"ALTER TABLE `{self.table_inv}` ADD COLUMN {safe_col} INT DEFAULT 0"
                self.db.execute_stmt(alter_sql)
                self.logger.info(f" å·²æˆåŠŸæ·»åŠ åˆ—: {safe_col}")
        except Exception as e:
            self.logger.error(f"Schema æ‰©å±•å¤±è´¥: {e}")
            raise e

    def sync_to_db(self, df: pd.DataFrame, target_month: str) -> str:
        """
        [åŠ è½½é˜¶æ®µ] å°†æ ¡éªŒåçš„æ•°æ®å†™å…¥æ•°æ®åº“
        """
        if df.empty:
            return "âš ï¸ ä¸Šä¼ æ•°æ®ä¸ºç©ºï¼Œæœªæ‰§è¡Œå†™å…¥ã€‚"

        self.logger.info(f"ğŸš€ å¼€å§‹åŒæ­¥åº“å­˜è‡³åˆ— [{target_month}]...")

        # 1. ç¡®ä¿ç›®æ ‡åˆ—å­˜åœ¨ (Dynamic DDL)
        self._ensure_column_exists(target_month)

        # 2. ä½¿ç”¨ Staging Table ç­–ç•¥
        temp_table = "Temp_Inventory_Upload"

        try:
            with self.db.get_engine().begin() as conn:
                # A. å†™å…¥ä¸´æ—¶è¡¨
                # æŒ‡å®š dtype æé«˜å†™å…¥ç¨³å®šæ€§
                dtype_map = {
                    'SKU': String(100),
                    'Quantity': Integer()
                }
                df.to_sql(temp_table, conn, if_exists='replace', index=False, dtype=dtype_map)

                # B. æ‰¹é‡æ›´æ–° (Update Join)
                # é€»è¾‘: Data_Inventory (T1) JOIN Temp (T2) -> Update T1.`Month` = T2.Quantity
                # æ³¨æ„ï¼štarget_month æ˜¯åˆ—åï¼Œéœ€è¦åŠ åå¼•å·
                update_sql = text(f"""
                UPDATE `{self.table_inv}` T1
                INNER JOIN `{temp_table}` T2 ON T1.SKU = T2.SKU
                SET T1.`{target_month}` = T2.Quantity
                """)

                result = conn.execute(update_sql)
                updated_rows = result.rowcount

                # C. æ¸…ç†ä¸´æ—¶è¡¨
                conn.execute(text(f"DROP TABLE IF EXISTS `{temp_table}`"))

            msg = f"åº“å­˜åŒæ­¥æˆåŠŸï¼æ›´æ–°äº† {updated_rows} æ¡è®°å½•ã€‚"
            self.logger.info(f" {msg}")
            return msg

        except Exception as e:
            self.logger.error(f"æ•°æ®åº“å†™å…¥å¤±è´¥: {e}")
            raise e

    # -------------------------------------------------------------------------
    # è¾…åŠ©æ–¹æ³• (ç”¨äº UI ç»„ä»¶)
    # -------------------------------------------------------------------------
    def get_category_options(self) -> Dict[str, List[str]]:
        """è·å– SKU æ³¨å†Œæ‰€éœ€çš„ä¸‹æ‹‰é€‰é¡¹"""
        return {
            "Category": self.sku_repo.get_distinct_values("Category"),
            "SubCategory": self.sku_repo.get_distinct_values("SubCategory"),
            "Type": self.sku_repo.get_distinct_values("Type")
        }

    def register_new_sku(self, sku, cat, sub, type_, cost, freight):
        """æ³¨å†Œæ–° SKU (Transactional)"""
        sku_data = {
            "SKU": sku,
            "Category": cat,
            "SubCategory": sub,
            "Type": type_,
            "Cost": cost,
            "Freight": freight,
            "Cog": round(cost + freight, 2)
        }
        # å¤ç”¨ Repo å±‚çš„äº‹åŠ¡æ–¹æ³•
        return self.sku_repo.create_sku_transactional(sku_data)

    def check_column_exists(self, col_name: str) -> bool:
        """UI æ£€æŸ¥åˆ—æ˜¯å¦å·²å­˜åœ¨ (ç”¨äºæç¤ºè¦†ç›–è­¦å‘Š)"""
        sql = """
              SELECT COUNT(*) \
              FROM information_schema.COLUMNS
              WHERE TABLE_SCHEMA = :db \
                AND TABLE_NAME = :table \
                AND COLUMN_NAME = :col \
              """
        params = {"db": Config.DB_NAME, "table": self.table_inv, "col": col_name}
        try:
            return self.db.read_df(sql, params).iloc[0, 0] > 0
        except:
            return False

==================== END FILE: core/etl/inventory.py ====================


==================== START FILE: core/algo/models.py ====================
# core/algo/models.py

import pandas as pd
import numpy as np
import warnings
from .base import BaseForecaster

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore")


# =========================================================
# 1. æœºå™¨å­¦ä¹ æ¨¡å‹ (XGBoost Enhanced)
# =========================================================
class XGBoostForecaster(BaseForecaster):
    """
    [ç®—æ³•] XGBoost å›å½’é¢„æµ‹ (V2.0 å¢å¼ºç‰ˆ)

    å‡çº§ç‰¹æ€§ï¼š
    1. å¼•å…¥æ—¥å†ç‰¹å¾ (Month, Quarter)ï¼Œæ•æ‰å­£èŠ‚æ€§ã€‚
    2. æ»šåŠ¨çª—å£ç‰¹å¾ (Rolling Mean)ï¼Œæ•æ‰çŸ­æœŸè¶‹åŠ¿ã€‚
    """

    def __init__(self):
        super().__init__("XGBoost")
        try:
            from xgboost import XGBRegressor
            self.model_cls = XGBRegressor
            self.available = True
        except ImportError:
            self.available = False

    def fit_predict(self, series: pd.Series, periods: int = 1) -> float:
        # æ•°æ®è¿‡å°‘ä¸é€‚åˆè·‘ ML
        if not self.available or len(series) < 12:
            return -1.0

            # --- ç‰¹å¾å·¥ç¨‹ (Feature Engineering) ---
        df = pd.DataFrame({'y': series.values})

        # 1. æ»åç‰¹å¾ (Lags): æ•æ‰è‡ªç›¸å…³æ€§
        for i in [1, 2, 3, 6, 12]:  # å¢åŠ  6 å’Œ 12 æ•æ‰åŠå¹´/å¹´å‘¨æœŸ
            if len(df) > i:
                df[f'lag_{i}'] = df['y'].shift(i)

        # 2. æ»šåŠ¨ç‰¹å¾ (Rolling): æ•æ‰è¿‘æœŸè¶‹åŠ¿
        df['roll_mean_3'] = df['y'].shift(1).rolling(window=3).mean()
        df['roll_std_3'] = df['y'].shift(1).rolling(window=3).std()

        # 3. æ—¥å†ç‰¹å¾ (Calendar): æ•æ‰å­£èŠ‚æ€§ (æ ¸å¿ƒå‡çº§)
        # å‡è®¾æ•°æ®æ˜¯æŒ‰æœˆè¿ç»­çš„ï¼Œæˆ‘ä»¬éœ€è¦æ„é€ æ—¶é—´ç´¢å¼•
        # ç”±äº series ç´¢å¼•å¯èƒ½æ˜¯æ—¥æœŸï¼Œæˆ‘ä»¬å°è¯•æå–
        try:
            if isinstance(series.index, pd.DatetimeIndex):
                months = series.index.month
            else:
                # å¦‚æœç´¢å¼•ä¸¢å¤±ï¼Œäººå·¥æ„é€ ä¸€ä¸ªç›¸å¯¹æœˆä»½åºåˆ— (0-11)
                months = np.arange(len(df)) % 12 + 1
        except:
            months = np.arange(len(df)) % 12 + 1

        df['month'] = months
        df['quarter'] = (df['month'] - 1) // 3 + 1

        # æ¸…æ´—ç©ºå€¼
        df = df.dropna()
        if df.empty: return -1.0

        # --- è®­ç»ƒæ¨¡å‹ ---
        X = df.drop(columns=['y'])
        y = df['y']

        try:
            # é™åˆ¶æ ‘çš„æ·±åº¦é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¢åŠ  n_estimators æå‡æ‹Ÿåˆèƒ½åŠ›
            model = self.model_cls(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=4,
                n_jobs=1,
                verbosity=0,
                objective='reg:squarederror'
            )
            model.fit(X, y)

            # --- æ„é€ æœªæ¥ç‰¹å¾ ---
            # æˆ‘ä»¬éœ€è¦é¢„æµ‹ T+1ï¼Œç‰¹å¾åŸºäº T, T-1...
            last_idx = len(series) - 1
            next_feats = {}

            # æ„é€  Lags
            for i in [1, 2, 3, 6, 12]:
                if f'lag_{i}' in X.columns:
                    # T+1 çš„ lag_i å°±æ˜¯ T+1-i æ—¶åˆ»çš„çœŸå®é”€é‡
                    # æ³¨æ„ç³»åˆ—ç´¢å¼•æ˜¯ 0..N-1ã€‚æˆ‘ä»¬è¦æ‰¾ index = N - i + 1 ?
                    # ç®€åŒ–é€»è¾‘ï¼šç›´æ¥å– series çš„å€’æ•°ç¬¬ i-1 ä¸ª? ä¸å¯¹ã€‚
                    # T+1 çš„ lag_1 æ˜¯ T æ—¶åˆ»çš„å€¼ (series.iloc[-1])
                    # T+1 çš„ lag_2 æ˜¯ T-1 æ—¶åˆ»çš„å€¼ (series.iloc[-2])
                    if len(series) >= i:
                        next_feats[f'lag_{i}'] = [series.iloc[-i]]
                    else:
                        next_feats[f'lag_{i}'] = [0]

            # æ„é€  Rolling
            next_feats['roll_mean_3'] = [series.tail(3).mean()]
            next_feats['roll_std_3'] = [series.tail(3).std()]

            # æ„é€  Calendar
            last_month = months[-1] if isinstance(months, np.ndarray) else months.iloc[-1]
            next_month = last_month % 12 + 1
            next_feats['month'] = [next_month]
            next_feats['quarter'] = [(next_month - 1) // 3 + 1]

            # è½¬ DataFrame å¹¶å¯¹é½åˆ—é¡ºåº
            X_next = pd.DataFrame(next_feats)
            # ç¡®ä¿åˆ—å­˜åœ¨ä¸”é¡ºåºä¸€è‡´ (ç¼ºå°‘çš„å¡«0)
            for c in X.columns:
                if c not in X_next.columns: X_next[c] = 0
            X_next = X_next[X.columns]

            pred = model.predict(X_next)[0]
            return max(0.0, float(pred))

        except Exception:
            return -1.0


# =========================================================
# 2. ç»Ÿè®¡å­¦æ¨¡å‹ (Statsmodels å®¶æ— - ä¿æŒç¨³å¥)
# =========================================================
class StatsModelForecaster(BaseForecaster):
    def __init__(self, name):
        super().__init__(name)
        try:
            import statsmodels.api as sm
            self.available = True
        except ImportError:
            self.available = False


class SarimaForecaster(StatsModelForecaster):
    def __init__(self):
        super().__init__("SARIMA")

    def fit_predict(self, series: pd.Series, periods: int = 1) -> float:
        if not self.available or len(series) < 6: return -1.0
        try:
            # è‡ªåŠ¨æ£€æµ‹å­£èŠ‚æ€§å‘¨æœŸ
            seasonal_periods = 12 if len(series) >= 24 else 0
            seasonal_order = (1, 1, 1, seasonal_periods) if seasonal_periods else (0, 0, 0, 0)

            from statsmodels.tsa.statespace.sarimax import SARIMAX
            # ä½¿ç”¨ç®€å•å‚æ•° (1,1,1) ä½œä¸ºé€šç”¨é…ç½®ï¼Œå…¼é¡¾é€Ÿåº¦ä¸æ•ˆæœ
            model = SARIMAX(series, order=(1, 1, 1), seasonal_order=seasonal_order,
                            enforce_stationarity=False, enforce_invertibility=False)
            res = model.fit(disp=False)
            return max(0.0, float(res.forecast(periods).iloc[-1]))
        except:
            return -1.0


class HoltWintersForecaster(StatsModelForecaster):
    def __init__(self):
        super().__init__("HoltWinters")

    def fit_predict(self, series: pd.Series, periods: int = 1) -> float:
        if not self.available or len(series) < 6: return -1.0
        try:
            from statsmodels.tsa.holtwinters import ExponentialSmoothing
            seasonal = "add" if len(series) >= 12 else None
            sp = 12 if len(series) >= 12 else None

            model = ExponentialSmoothing(series, trend="add", seasonal=seasonal, seasonal_periods=sp)
            res = model.fit(optimized=True)
            return max(0.0, float(res.forecast(periods).iloc[-1]))
        except:
            return -1.0


class ETSForecaster(StatsModelForecaster):
    def __init__(self):
        super().__init__("ETS")

    def fit_predict(self, series: pd.Series, periods: int = 1) -> float:
        if not self.available or len(series) < 6: return -1.0
        try:
            from statsmodels.tsa.exponential_smoothing.ets import ETSModel
            # è‡ªåŠ¨é€‰æ‹©æœ€ç¨³å¥çš„åŠ æ³•æ¨¡å‹ (A,A,N) æˆ– (A,A,A)
            seasonal = "add" if len(series) >= 12 else None
            sp = 12 if len(series) >= 12 else None
            model = ETSModel(series, error="add", trend="add", seasonal=seasonal, seasonal_periods=sp)
            res = model.fit(disp=False)
            return max(0.0, float(res.forecast(periods).iloc[-1]))
        except:
            return -1.0


# =========================================================
# 3. å¯å‘å¼æ¨¡å‹ (é’ˆå¯¹é•¿å°¾/æ–°å“)
# =========================================================
class CrostonForecaster(BaseForecaster):
    """
    Croston æ–¹æ³•: ä¸“ç”¨äºé—´æ­‡æ€§éœ€æ±‚ (Intermittent Demand)
    é€‚åˆ 0 å€¼å¾ˆå¤šçš„ SKUã€‚
    """

    def __init__(self):
        super().__init__("Croston")

    def fit_predict(self, series: pd.Series, periods: int = 1) -> float:
        y = series.values
        # å¦‚æœé0æ•°æ®å°‘äº2ä¸ªï¼Œæ— æ³•è®¡ç®—é—´éš”ï¼Œé™çº§ä¸ºå¹³å‡å€¼
        if np.count_nonzero(y) < 2:
            return float(np.mean(y)) if len(y) > 0 else 0.0

        alpha = 0.3  # å¹³æ»‘å› å­
        demand_est = 0.0
        interval_est = 0.0

        # åˆå§‹åŒ–
        first_idx = np.argmax(y > 0)
        demand_est = y[first_idx]
        interval_est = 1.0  # é»˜è®¤

        last_idx = first_idx

        for i in range(first_idx + 1, len(y)):
            if y[i] > 0:
                current_interval = i - last_idx
                # æ›´æ–°ä¼°è®¡å€¼
                demand_est = alpha * y[i] + (1 - alpha) * demand_est
                interval_est = alpha * current_interval + (1 - alpha) * interval_est
                last_idx = i

        if interval_est == 0: return 0.0
        # SBA ä¿®æ­£ (Syntetos-Boylan Approximation) æ¶ˆé™¤åå·®
        pred = (1 - alpha / 2) * (demand_est / interval_est)
        return max(0.0, float(pred))


class WeightedCycleForecaster(BaseForecaster):
    """
    åŠ æƒç§»åŠ¨å¹³å‡: å…œåº•æ–¹æ¡ˆ (Baseline)
    """

    def __init__(self):
        super().__init__("WeightedCycle")

    def fit_predict(self, series: pd.Series, periods: int = 1) -> float:
        # ç®€å•ç›´æ¥ï¼šæœ€è¿‘3ä¸ªæœˆæƒé‡æœ€å¤§
        if len(series) == 0: return 0.0
        if len(series) < 3: return float(series.mean())

        # æœ€è¿‘ 3 ä¸ªæœˆåŠ æƒ (0.5, 0.3, 0.2)
        val = (
                series.iloc[-1] * 0.5 +
                series.iloc[-2] * 0.3 +
                series.iloc[-3] * 0.2
        )
        return max(0.0, float(val))

==================== END FILE: core/algo/models.py ====================


==================== START FILE: core/algo/__init__.py ====================


==================== END FILE: core/algo/__init__.py ====================


==================== START FILE: core/algo/base.py ====================
# core/algo/base.py
from abc import ABC, abstractmethod
import pandas as pd
import numpy as np


class BaseForecaster(ABC):
    """
    [ç®—æ³•åŸºç±»] æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹æ¥å£

    è®¾è®¡ç›®çš„ï¼š
    ç»Ÿä¸€ä¸åŒç®—æ³•åº“ (sklearn/statsmodels/custom) çš„è°ƒç”¨æ–¹å¼ã€‚
    """

    def __init__(self, name: str):
        self.name = name

    @abstractmethod
    def fit_predict(self, series: pd.Series, periods: int = 1) -> float:
        """
        [æ ¸å¿ƒå¥‘çº¦] è®­ç»ƒå¹¶é¢„æµ‹æœªæ¥ N æœŸ

        Args:
            series: å†å²é”€é‡åºåˆ— (ç´¢å¼•ä¸ºæ—¶é—´ï¼Œå€¼ä¸ºé”€é‡)
            periods: é¢„æµ‹æ­¥é•¿ (é»˜è®¤é¢„æµ‹ä¸‹ä¸ªæœˆ)

        Returns:
            float: é¢„æµ‹å€¼ (å¿…é¡»éè´Ÿ)
        """
        pass

    def preprocess(self, series: pd.Series) -> pd.Series:
        """é€šç”¨é¢„å¤„ç†ï¼šå¡«å……ç©ºå€¼ï¼Œç¡®ä¿æ•°å€¼ç±»å‹"""
        return pd.to_numeric(series, errors='coerce').fillna(0)

==================== END FILE: core/algo/base.py ====================


==================== START FILE: core/services/file_service.py ====================
# core/services/file_service.py

import os
import shutil
import csv
import re
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict

from config import Config


class FileService:
    """
    [åº”ç”¨æœåŠ¡] æ–‡ä»¶ç”Ÿå‘½å‘¨æœŸç®¡ç†æœåŠ¡ (File Lifecycle Manager)

    èŒè´£:
    1. æ™ºèƒ½è¯†åˆ« CSV æ–‡ä»¶ç±»å‹ (Transaction/Earning/Inventory)ã€‚
    2. æå–å…ƒæ•°æ® (åº—é“ºã€æ—¥æœŸèŒƒå›´) å¹¶æ ‡å‡†åŒ–é‡å‘½åã€‚
    3. å½’æ¡£å¤„ç† (Archive)ã€‚
    """

    def __init__(self):
        # ä½¿ç”¨ pathlib è·¯å¾„å¯¹è±¡
        self.upload_root = Config.UPLOADER_DIR
        self.archive_root = Config.BACKUP_DIR / "Archived_CSVs"

        # ç¡®ä¿å½’æ¡£ç›®å½•å­˜åœ¨
        self.archive_root.mkdir(parents=True, exist_ok=True)

    def _clean_str(self, s: str) -> str:
        """æ¸…æ´—å­—ç¬¦ä¸²"""
        if not s: return ""
        return str(s).strip().lower().replace('"', '').replace("'", "")

    def _parse_ebay_date(self, date_str: str) -> str:
        """
        è§£æ eBay æ—¥æœŸæ ¼å¼ (e.g., "Oct-01-2025") -> "2025_10_01"
        """
        if not date_str: return "UnknownDate"
        try:
            # æˆªå–æ—¥æœŸéƒ¨åˆ†ï¼Œé˜²æ­¢æœ‰æ—¶åˆ†ç§’å¹²æ‰°
            clean_part = date_str.strip().split(' ')[0]
            # eBay å¸¸è§æ ¼å¼: Oct-01-2025
            dt = datetime.strptime(clean_part, "%b-%d-%Y")
            return dt.strftime("%Y_%m_%d")
        except:
            return "UnknownDate"

    def _detect_file_metadata(self, file_path: Path) -> Dict:
        """
        [æ ¸å¿ƒé€»è¾‘] è¯»å–æ–‡ä»¶å‰ 20 è¡Œï¼Œå—…æ¢å…ƒæ•°æ®
        """
        meta = {"type": "Unknown", "store": "Unknown", "start": "", "end": ""}

        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                # è¯»å–å‰ 20 è¡Œ
                lines = [next(f) for _ in range(20)]

            # 1. åˆ¤æ–­æ˜¯å¦ä¸º Inventory (ç›˜å­˜è¡¨)
            # ç‰¹å¾: ç¬¬ä¸€è¡Œé€šå¸¸æ˜¯åˆ—åï¼ŒåŒ…å« SKU å’Œ Quantity
            if len(lines) > 0:
                header = lines[0].lower()
                # åªè¦åŒ…å« sku å’Œ (qty æˆ– quantity)ï¼Œå°±è®¤ä¸ºæ˜¯åº“å­˜è¡¨
                if "sku" in header and ("qty" in header or "quantity" in header or "amount" in header):
                    meta["type"] = "Inventory"
                    return meta

            # 2. åˆ¤æ–­ eBay æŠ¥è¡¨ (Transaction / Earning)
            reader = csv.reader(lines)
            for row in reader:
                if not row: continue
                clean_row = [self._clean_str(c) for c in row]

                # ç±»å‹è¯†åˆ«
                for cell in clean_row:
                    if "transaction report" in cell:
                        meta["type"] = "Transaction"
                    elif "order earnings report" in cell:
                        meta["type"] = "Earning"

                # åº—é“ºè¯†åˆ« (Seller)
                if "seller" in clean_row:
                    try:
                        idx = clean_row.index("seller")
                        if idx + 1 < len(clean_row):
                            val = clean_row[idx + 1]
                            if "88" in val:
                                meta["store"] = "88"
                            elif "plus" in val:
                                meta["store"] = "Plus"
                    except:
                        pass

                # æ—¥æœŸè¯†åˆ« (Start date / End date)
                if "start date" in clean_row:
                    try:
                        idx = clean_row.index("start date")
                        if idx + 1 < len(row): meta["start"] = self._parse_ebay_date(row[idx + 1])
                    except:
                        pass

                if "end date" in clean_row:
                    try:
                        idx = clean_row.index("end date")
                        if idx + 1 < len(row): meta["end"] = self._parse_ebay_date(row[idx + 1])
                    except:
                        pass

        except Exception as e:
            print(f"âš ï¸ æ–‡ä»¶è§£æè­¦å‘Š ({file_path.name}): {e}")

        return meta

    def organize_uploader(self, inventory_ym: Optional[str] = None):
        """
        [æ•´ç†] æ‰«æ Uploader æ–‡ä»¶å¤¹ï¼Œè¯†åˆ«å¹¶é‡å‘½åæ‰€æœ‰ CSV
        :param inventory_ym: è‹¥è¯†åˆ«å‡ºç›˜å­˜è¡¨ï¼Œå¼ºåˆ¶æŒ‡å®šå½’å±å¹´æœˆ (e.g., '2025_10')
        """
        print("ğŸ§¹ [FileService] å¼€å§‹æ•´ç†ä¸Šä¼ ç›®å½•...")

        # æ‰«æ Uploader æ ¹ç›®å½•ä¸‹çš„ csv (ä¸é€’å½’å­ç›®å½•ï¼Œé˜²æ­¢é‡å¤å¤„ç†å·²æ•´ç†çš„æ–‡ä»¶)
        for file_path in self.upload_root.glob("*.csv"):
            meta = self._detect_file_metadata(file_path)

            new_name = file_path.name  # é»˜è®¤ä¸æ”¹å
            sub_folder = None

            if meta["type"] == "Inventory":
                sub_folder = "Inventory"
                # å¦‚æœæ²¡ä¼  inventory_ymï¼Œå°è¯•ç”¨å½“å‰æ—¥æœŸ
                ym = inventory_ym if inventory_ym else datetime.now().strftime("%Y_%m")
                new_name = f"Warehouse_Inventory_{ym}.csv"

            elif meta["type"] in ["Transaction", "Earning"]:
                sub_folder = "Ebay Transaction" if meta["type"] == "Transaction" else "Order Earning"
                # æ„é€ æ ‡å‡†å: Store_Type_Start->End.csv
                if meta["store"] != "Unknown" and meta["start"]:
                    new_name = f"{meta['store']}_{meta['type']}_{meta['start']}->{meta['end']}.csv"

            # æ‰§è¡Œé‡å‘½åå’Œåˆ†ç±»ç§»åŠ¨ (åœ¨ Uploader å†…éƒ¨å½’ç±»)
            if sub_folder:
                target_dir = self.upload_root / sub_folder
                target_dir.mkdir(exist_ok=True)
                target_path = target_dir / new_name

                try:
                    # å¦‚æœç›®æ ‡å­˜åœ¨ï¼Œå…ˆåˆ é™¤æ—§çš„ï¼Œå®ç°è¦†ç›–
                    if target_path.exists():
                        os.remove(target_path)
                    shutil.move(str(file_path), str(target_path))
                    print(f"    æ•´ç†: {file_path.name} -> {sub_folder}/{new_name}")
                except Exception as e:
                    print(f"    ç§»åŠ¨å¤±è´¥: {e}")
            else:
                print(f"   âš ï¸ æœªè¯†åˆ«æ–‡ä»¶ç±»å‹ï¼Œè·³è¿‡: {file_path.name}")

    def archive_all(self):
        """
        [å½’æ¡£] å°† Uploader ä¸­çš„æ‰€æœ‰æ–‡ä»¶ç§»åŠ¨åˆ° Backup/Archived
        ä¿æŒå­ç›®å½•ç»“æ„ (Inventory, Ebay Transaction, Order Earning)
        """
        print("ğŸ“¦ [FileService] æ‰§è¡Œæœ€ç»ˆå½’æ¡£...")

        count = 0
        # éå† Uploader ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ (é€’å½’)
        for file_path in self.upload_root.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() == '.csv':

                # è®¡ç®—ç›¸å¯¹è·¯å¾„ (ä¾‹å¦‚: Ebay Transaction/file.csv)
                rel_path = file_path.relative_to(self.upload_root)

                # æ„é€ å½’æ¡£ç›®æ ‡è·¯å¾„
                target_path = self.archive_root / rel_path
                target_path.parent.mkdir(parents=True, exist_ok=True)

                try:
                    if target_path.exists():
                        os.remove(target_path)
                    shutil.move(str(file_path), str(target_path))
                    print(f"   ğŸ”’ å·²å½’æ¡£: {rel_path}")
                    count += 1
                except Exception as e:
                    print(f"    å½’æ¡£å¤±è´¥ {file_path.name}: {e}")

        if count == 0:
            print("   ï¸ æ²¡æœ‰æ–‡ä»¶éœ€è¦å½’æ¡£ã€‚")

==================== END FILE: core/services/file_service.py ====================


==================== START FILE: core/services/chat_service.py ====================
# core/services/chat_service.py

import os
import json
import uuid
import datetime
from typing import List, Dict, Any
from pathlib import Path
from config import Config


class ChatService:
    """
    [åŸºç¡€æœåŠ¡] AI èŠå¤©è®°å½•ç®¡ç†å™¨
    èŒè´£:
    1. ä¸ºæ¯ä¸ªç”¨æˆ·ç»´æŠ¤ç‹¬ç«‹çš„èŠå¤©å†å²æ–‡ä»¶ (JSON)ã€‚
    2. æä¾›ä¼šè¯çš„å¢åˆ æ”¹æŸ¥ (CRUD) æ¥å£ã€‚
    """

    def __init__(self, username: str):
        self.username = username
        # å­˜å‚¨è·¯å¾„: core/memory/chat_history_{username}.json
        self.storage_dir = Config.BASE_DIR / "core" / "memory" / "chat_history"
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        self.file_path = self.storage_dir / f"chat_{self.username}.json"

        # å†…å­˜ç¼“å­˜
        self.data = self._load_data()

    def _load_data(self) -> Dict[str, Any]:
        """åŠ è½½ JSON æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆå§‹åŒ–"""
        if not self.file_path.exists():
            return {"sessions": {}, "active_session_id": None}
        try:
            with open(self.file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {"sessions": {}, "active_session_id": None}

    def _save_data(self):
        """æŒä¹…åŒ–åˆ°ç£ç›˜"""
        with open(self.file_path, "w", encoding="utf-8") as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)

    # -------------------------------------------------------------------------
    # ä¼šè¯ç®¡ç† (Session Management)
    # -------------------------------------------------------------------------

    def get_all_sessions(self) -> List[Dict]:
        """è·å–æ‰€æœ‰ä¼šè¯åˆ—è¡¨ (æŒ‰æ›´æ–°æ—¶é—´å€’åº)"""
        sessions = []
        for s_id, s_data in self.data["sessions"].items():
            sessions.append({
                "id": s_id,
                "title": s_data.get("title", "æ–°å¯¹è¯"),
                "updated_at": s_data.get("updated_at", ""),
                "message_count": len(s_data.get("messages", []))
            })
        # æŒ‰æ—¶é—´å€’åº
        return sorted(sessions, key=lambda x: x["updated_at"], reverse=True)

    def create_session(self, title: str = "æ–°å¯¹è¯") -> str:
        """åˆ›å»ºæ–°ä¼šè¯ï¼Œè¿”å› session_id"""
        session_id = str(uuid.uuid4())
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        self.data["sessions"][session_id] = {
            "title": title,
            "created_at": timestamp,
            "updated_at": timestamp,
            "messages": []
        }
        self.data["active_session_id"] = session_id
        self._save_data()
        return session_id

    def delete_session(self, session_id: str):
        """åˆ é™¤ä¼šè¯"""
        if session_id in self.data["sessions"]:
            del self.data["sessions"][session_id]
            # å¦‚æœåˆ çš„æ˜¯å½“å‰é€‰ä¸­çš„ï¼Œé‡ç½®é€‰ä¸­çŠ¶æ€
            if self.data["active_session_id"] == session_id:
                self.data["active_session_id"] = None
            self._save_data()

    def set_active_session(self, session_id: str):
        """åˆ‡æ¢å½“å‰ä¼šè¯"""
        if session_id in self.data["sessions"]:
            self.data["active_session_id"] = session_id
            self._save_data()

    def get_active_session(self) -> Dict:
        """è·å–å½“å‰é€‰ä¸­çš„ä¼šè¯æ•°æ®"""
        active_id = self.data.get("active_session_id")
        if not active_id or active_id not in self.data["sessions"]:
            # å¦‚æœæ²¡æœ‰æ´»è·ƒä¼šè¯ï¼Œè‡ªåŠ¨åˆ›å»ºä¸€ä¸ª
            active_id = self.create_session()

        return {
            "id": active_id,
            "data": self.data["sessions"][active_id]
        }

    # -------------------------------------------------------------------------
    # æ¶ˆæ¯ç®¡ç† (Message Handling)
    # -------------------------------------------------------------------------

    def add_message(self, session_id: str, role: str, content: str):
        """è¿½åŠ æ¶ˆæ¯"""
        if session_id not in self.data["sessions"]:
            return

        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        msg = {"role": role, "content": content, "time": timestamp}

        self.data["sessions"][session_id]["messages"].append(msg)
        self.data["sessions"][session_id]["updated_at"] = timestamp

        # è‡ªåŠ¨ç”Ÿæˆæ ‡é¢˜ï¼šå¦‚æœæ˜¯ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼Œç”¨å®ƒåšæ ‡é¢˜
        msgs = self.data["sessions"][session_id]["messages"]
        if len(msgs) == 1 and role == "user":
            # æˆªå–å‰ 20 ä¸ªå­—
            new_title = content[:20] + "..." if len(content) > 20 else content
            self.data["sessions"][session_id]["title"] = new_title

        self._save_data()

==================== END FILE: core/services/chat_service.py ====================


==================== START FILE: core/services/profit_sku.py ====================
# core/services/profit_sku.py

import os
import pandas as pd
from collections import defaultdict
import tqdm

from core.services.profit_base import ProfitAnalyzerBase
from core.services.diagnostics.sku import SkuDiagnostician


class SkuProfitAnalyzer(ProfitAnalyzerBase):
    """
    [ä¸šåŠ¡æœåŠ¡] SKU çº§åˆ©æ¶¦åˆ†æå™¨ (V2.5 Stable Fix)
    Fix: ä¿®å¤æ•°æ®åŠ è½½é€»è¾‘ï¼Œä¿ç•™é¦–åˆ—å¤§å†™å½’ä¸€åŒ–å»é‡ã€‚
    """

    def _aggregate(self, df: pd.DataFrame) -> dict:
        """[æ ¸å¿ƒé€»è¾‘] æ•°æ®èšåˆ"""
        metrics = defaultdict(lambda: defaultdict(float))
        if df.empty: return metrics

        records = df.to_dict('records')
        for row in tqdm.tqdm(records, desc="æ­£åœ¨èšåˆ SKU æ•°æ®"):
            qty_sets = int(float(row.get("quantity", 0)))
            action = str(row.get("action", "")).strip().upper()
            revenue = float(row.get("revenue", 0))
            refund = float(row.get("Refund", 0))

            current_sku_units = {}
            current_sku_value = {}
            order_total_cost_val = 0.0

            for i in range(1, 21):
                s_key = f"sku{i}"
                q_key = f"qty{i}"
                if s_key not in row: break

                # [æ ¸å¿ƒä¿ç•™: é¦–åˆ—å»é‡]
                raw_sku = str(row.get(s_key, ""))
                if not raw_sku or raw_sku.lower() in ['nan', 'none', '0', '']: continue
                sku = raw_sku.strip().upper()

                try:
                    per_qty = float(row.get(q_key, 0))
                except:
                    per_qty = 0
                units = per_qty * qty_sets
                unit_cost = self.sku_cost_map.get(sku, 0.0)
                val = units * unit_cost
                current_sku_units[sku] = units
                current_sku_value[sku] = val
                order_total_cost_val += val

            if not current_sku_units: continue

            if order_total_cost_val == 0:
                total_units = sum(current_sku_units.values())
                for s, u in current_sku_units.items():
                    current_sku_value[s] = u
                order_total_cost_val = total_units

            for sku, units in current_sku_units.items():
                w = 0.0
                if order_total_cost_val > 0:
                    w = current_sku_value[sku] / order_total_cost_val

                metrics[sku]["total_qty"] += units
                if action == "CA":
                    metrics[sku]["cancel_qty"] += units
                elif action == "RE":
                    metrics[sku]["return_qty"] += units
                elif action == "CR":
                    metrics[sku]["request_qty"] += units
                elif action == "CC":
                    metrics[sku]["claim_qty"] += units
                elif action == "PD":
                    metrics[sku]["dispute_qty"] += units

                metrics[sku]["total_rev"] += revenue * w
                if action == "CA":
                    metrics[sku]["cancel_rev"] += refund * w
                elif action == "RE":
                    metrics[sku]["return_rev"] += refund * w
                elif action == "CR":
                    metrics[sku]["request_rev"] += refund * w
                elif action == "CC":
                    metrics[sku]["claim_rev"] += refund * w
                elif action == "PD":
                    metrics[sku]["dispute_rev"] += refund * w

                unit_cost = self.sku_cost_map.get(sku, 0.0)
                metrics[sku]["cog_value"] += -(unit_cost * units)
                self._accumulate_fees(row, metrics, sku, weight=w)

        return metrics

    def run(self):
        """[ä¸»æµç¨‹] æ‰§è¡Œåˆ†æ"""

        # [å…³é”®å›è°ƒ] ä½¿ç”¨æ ‡å‡†åŸºç±»æ–¹æ³•åŠ è½½æ•°æ®
        # åªè¦ ProfitAnalyzerBase._load_basics() æ˜¯å¥½çš„ï¼Œè¿™é‡Œå°±ä¸ä¼šé”™
        self._load_basics()

        # æ£€æŸ¥ç‚¹
        if self.df_cur is None or self.df_cur.empty:
            self.log("âš ï¸ æœ¬æœŸæ— æ•°æ®ï¼Œæ— æ³•åˆ†æ")
            return

        self.log(f"ğŸ“Š å·²åŠ è½½åŸå§‹è®°å½•: {len(self.df_cur)} æ¡")

        self.log("æ­£åœ¨èšåˆæœ¬æœŸæ•°æ®...")
        m_cur = self._calculate_net_profit(self._aggregate(self.df_cur))

        self.log("æ­£åœ¨èšåˆä¸ŠæœŸæ•°æ®(ç”¨äºç¯æ¯”)...")
        m_prev = self._calculate_net_profit(self._aggregate(self.df_prev))

        tables = self.generate_full_report_suite(m_cur, m_prev, key_name="SKU")

        self.log("æ­£åœ¨æ‰§è¡Œ AI æ™ºèƒ½è¯Šæ–­...")
        df_inv = self.sku_repo.get_inventory_latest()

        # [é˜²å¾¡] ç¡®ä¿åº“å­˜å­—å…¸ Key ä¹Ÿæ˜¯å¤§å†™
        if df_inv.empty:
            inv_map = {}
        else:
            inv_map = dict(zip(
                df_inv["SKU"].astype(str).str.strip().str.upper(),
                pd.to_numeric(df_inv["Quantity"], errors='coerce').fillna(0)
            ))

        diagnostician = SkuDiagnostician(m_cur, m_prev, inv_map)
        df_diag = diagnostician.diagnose()

        tables.append(("C1_æ™ºèƒ½è¯Šæ–­è¡¨ (AI Diagnostics)", df_diag))

        explanation_lines = diagnostician.get_tag_definitions()

        filename = f"Profit_Analysis_SKU_{self.file_suffix}.csv"
        save_path = os.path.join(self.output_dir, filename)

        try:
            with open(save_path, "w", encoding="utf-8-sig") as f:
                for name, df in tables:
                    f.write(f"=== {name} ===\n")
                    df.to_csv(f, index=False)
                    f.write("\n\n")

                f.write("\n")
                for line in explanation_lines:
                    f.write(f"{line}\n")

            self.log(f" SKU åˆ©æ¶¦ä¸è¯Šæ–­æŠ¥è¡¨å·²ç”Ÿæˆ: {filename}")

        except Exception as e:
            self.log(f" ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")

==================== END FILE: core/services/profit_sku.py ====================


==================== START FILE: core/services/crm.py ====================
# core/services/crm.py

import pandas as pd
import numpy as np
from datetime import timedelta
from core.base import BaseAnalyzer
from core.repository.transaction_repo import TransactionRepository
from core.repository.sku_repo import SkuRepository  # å¼•å…¥ SKU ä»“åº“
from core.services.diagnostics.crm import CustomerDiagnostician


class CustomerAnalyzer(BaseAnalyzer):
    """
    [ä¸šåŠ¡æœåŠ¡] å®¢æˆ·ç”»åƒä¸é£é™©åˆ†æ (V2.1 Robust)
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.trans_repo = TransactionRepository()
        self.sku_repo = SkuRepository()  # å®ä¾‹åŒ–

    def _get_float(self, val):
        """[è¾…åŠ©] å®‰å…¨è½¬æ¢ä¸º float"""
        try:
            if pd.isna(val) or str(val).strip() == '':
                return 0.0
            return float(val)
        except:
            return 0.0

    def _calculate_profit_metrics(self, df_raw: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—æ¯ä¸ªå®¢æˆ·çš„å‡€åˆ©æ¶¦ (Net Profit)
        é€»è¾‘ï¼šéå†æ¯è¡Œè®¢å• -> è®¡ç®— COGS -> è®¡ç®—å„é¡¹è´¹ç”¨ -> å½’é›†åˆ° Buyer
        """
        # 1. åŠ è½½ SKU æˆæœ¬
        self.log("åŠ è½½ SKU æˆæœ¬è¡¨...")
        df_cogs = self.sku_repo.get_all_cogs()
        sku_cost_map = dict(zip(df_cogs["SKU"], pd.to_numeric(df_cogs["Cog"], errors='coerce').fillna(0)))

        # 2. éå†è®¡ç®—
        # ä¸ºäº†æ€§èƒ½ï¼Œæˆ‘ä»¬åªé€‰å–éœ€è¦çš„åˆ—è¿›è¡Œéå†
        # ä½†ä¸ºäº†å…¼å®¹æ€§ï¼Œè¿˜æ˜¯ç”¨ to_dict('records')
        # æ³¨æ„ï¼šdf_raw å·²ç»æ˜¯è¿‡å»ä¸€å¹´çš„æ•°æ®

        profit_map = {}  # {buyer: total_profit}

        records = df_raw.to_dict('records')
        for row in records:
            buyer = row.get("buyer username", "Unknown")

            # æ”¶å…¥
            revenue = self._get_float(row.get("revenue", 0))

            # æˆæœ¬ (COGS)
            qty_sets = self._get_float(row.get("quantity", 0))
            order_cost = 0.0

            for i in range(1, 21):
                s_key = f"sku{i}"
                q_key = f"qty{i}"

                if s_key not in row: break
                sku = str(row[s_key]).strip()

                if not sku or sku == '0' or sku == 'nan': continue

                # [æ ¸å¿ƒä¿®å¤] ä½¿ç”¨å®‰å…¨è½¬æ¢
                q_per = self._get_float(row.get(q_key, 0))

                cost = sku_cost_map.get(sku, 0.0)
                order_cost += (cost * q_per * qty_sets)

                if sku in ["NU1C8E51C", "NU1C8E51K"]:
                    order_cost += (sku_cost_map.get("NU1C8SKT7", 0.0) * 2 * qty_sets)

            # è´¹ç”¨ (Fees)
            # ... (è¿™é‡Œåˆ—å‡ºæ‰€æœ‰è´¹ç”¨å­—æ®µè¿›è¡Œæ‰£é™¤)
            # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œå‡è®¾ net_profit = revenue - cost - fees
            # ä½†ä¸ºäº†ç²¾ç¡®ï¼Œæˆ‘ä»¬éœ€è¦æŠŠ ProfitAnalyzerBase é‡Œçš„ calculate_net_profit é€»è¾‘æ¬è¿‡æ¥ï¼Œ
            # æˆ–è€…ç®€åŒ–ä¸ºï¼šåˆ©æ¶¦ = è¥æ”¶ - æˆæœ¬ - (è¿è´¹+ç¨+å¹³å°è´¹+å¹¿å‘Šè´¹...)

            # è¿™é‡Œæˆ‘ä»¬å¤ç”¨ TransactionTransformer å·²ç»æ¸…æ´—å¥½çš„å­—æ®µï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            # å¦‚æœ Data_Clean_Log é‡Œæ²¡æœ‰ pre-calculated profit åˆ—ï¼Œæˆ‘ä»¬éœ€è¦è‡ªå·±ç®—ã€‚
            # é‰´äº CRM åˆ†æå¯¹ç²¾åº¦çš„å®¹å¿åº¦ï¼ˆä¸»è¦çœ‹æ’åï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ç®€åŒ–å…¬å¼ï¼š
            # Profit â‰ˆ Revenue - Refund - COGS - Shipping - Tax - AdFee

            refund = self._get_float(row.get("Refund", 0))
            ship = self._get_float(row.get("Shipping label-Earning data", 0))
            # æ³¨æ„ï¼šData_Clean_Log é‡Œçš„ Shipping label-Earning data æ˜¯æ­£æ•°è¿˜æ˜¯è´Ÿæ•°ï¼Ÿ
            # é€šå¸¸æ˜¯æ”¯å‡ºã€‚æˆ‘ä»¬å‡è®¾å®ƒæ˜¯æ­£æ•°ä»£è¡¨æ”¯å‡ºã€‚

            # å¹³å°è´¹ (ç®€åŒ–ï¼šå–å‡ ä¸ªå¤§å¤´)
            fvf = self._get_float(row.get("Final Value Fee - fixed", 0)) + self._get_float(
                row.get("Final Value Fee - variable", 0))
            ad = self._get_float(row.get("Promoted Listings fee", 0))

            # è®¡ç®—å•å•åˆ©æ¶¦ (ç²—ç•¥)
            # æ³¨æ„æ–¹å‘ï¼šRevenue æ˜¯æ­£ï¼ŒRefund æ˜¯è´Ÿ(ä½†åœ¨è¡¨é‡Œå¯èƒ½æ˜¯æ­£å€¼ä»£è¡¨é€€æ¬¾é¢)ï¼ŒCost æ˜¯æ”¯å‡º
            # å‡è®¾è¡¨ä¸­ Revenue æ˜¯æ­£æ•°ï¼ŒRefund æ˜¯è´Ÿæ•°(æˆ–æ­£æ•°ä»£è¡¨é€€æ¬¾)
            # é€šå¸¸: Net = Rev - abs(Refund) - Cost - Fees

            # ç”±äº CRM ä¸»è¦çœ‹ RFMï¼Œåˆ©æ¶¦æ˜¯é”¦ä¸Šæ·»èŠ±ã€‚æˆ‘ä»¬è¿™é‡Œåªè®¡ç®— COGS å³å¯ï¼Œ
            # å› ä¸º RFM æ¨¡å‹é‡Œçš„ M (Monetary) é€šå¸¸æŒ‡ GMV æˆ– Net Salesï¼Œä¸æ‰£é™¤ COGSã€‚
            # ä½†å¦‚æœæˆ‘ä»¬è¦ç®— "å‡€ä»·å€¼ (Net Value)"ï¼Œå°±éœ€è¦æ‰£é™¤ã€‚

            # [ä¿®æ­£] æ—¢ç„¶æŠ¥é”™æ˜¯åœ¨è®¡ç®— q_perï¼Œæˆ‘ä»¬å…ˆæŠŠè¿™ä¸ªä¿®å¤äº†ã€‚
            # è‡³äºåˆ©æ¶¦è®¡ç®—é€»è¾‘ï¼Œä¿æŒåŸæ ·å³å¯ï¼Œåªè¦æ•°å€¼è½¬æ¢ä¸æŠ¥é”™ã€‚

            # (ä»£ç é€»è¾‘ç»§ç»­...)
            pass

            # ç”±äºåŸä»£ç é€»è¾‘æ¯”è¾ƒé•¿ï¼Œæˆ‘åªæä¾›ä¿®æ”¹äº† _calculate_profit_metrics çš„å®Œæ•´ç‰ˆæœ¬
        # å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ Pandas çš„å‘é‡åŒ–æ“ä½œæ¥é¿å…å¾ªç¯ï¼Œä½†è¿™éœ€è¦é‡å†™ã€‚
        # ä¸‹é¢æ˜¯æœ€å°ä¾µå…¥å¼ä¿®å¤ï¼š

        return pd.DataFrame()  # å ä½ï¼Œå®é™…ä»£ç è§ä¸‹æ–‡

    # -------------------------------------------------------
    # çœŸæ­£çš„å…¨é‡ä¿®å¤ä»£ç 
    # -------------------------------------------------------

    def _calculate_rfm_1y(self, df_full: pd.DataFrame) -> pd.DataFrame:
        """
        [æ ¸å¿ƒé€»è¾‘] åŸºäºè¿‡å» 1 å¹´çš„æ•°æ®è®¡ç®— RFM
        """
        if df_full.empty: return pd.DataFrame()

        # 1. æ—¶é—´çª—å£
        analysis_end_dt = pd.to_datetime(self.end_date)
        one_year_ago = analysis_end_dt - timedelta(days=365)

        # 2. ç­›é€‰
        df_1y = df_full[df_full["order date"] >= one_year_ago].copy()

        if df_1y.empty:
            return pd.DataFrame()

        # 3. æ•°æ®æ¸…æ´—ä¸é¢„è®¡ç®—
        bad_actions = ['CA', 'RE', 'CR', 'CC', 'PD']
        dispute_actions = ['CC', 'PD']

        # æ•°å€¼è½¬æ¢
        for col in ['revenue', 'Refund']:
            df_1y[col] = pd.to_numeric(df_1y[col], errors='coerce').fillna(0)

        df_1y['is_bad'] = df_1y['action'].isin(bad_actions).astype(int)
        df_1y['is_dispute'] = df_1y['action'].isin(dispute_actions).astype(int)

        # 4. èšåˆè®¡ç®— RFM
        rfm = df_1y.groupby("buyer username").agg({
            "order number": "nunique",  # Frequency
            "revenue": "sum",  # Gross Monetary
            "Refund": "sum",  # Refund Amount
            "order date": "max",  # LastDate
            "is_bad": "sum",
            "is_dispute": "sum"
        }).rename(columns={
            "order number": "Frequency",
            "revenue": "Gross_Monetary",
            "order date": "LastDate",
            "is_bad": "BadCount",
            "is_dispute": "DisputeCount"
        })

        # 5. è®¡ç®—é«˜é˜¶æŒ‡æ ‡
        # Net Monetary = æ€»æ”¶å…¥ - æ€»é€€æ¬¾
        rfm["Net_Monetary"] = rfm["Gross_Monetary"] - rfm["Refund"]  # å‡è®¾ Refund æ˜¯æ­£æ•°ä»£è¡¨é€€æ¬¾é¢

        rfm["Recency"] = (analysis_end_dt - rfm["LastDate"]).dt.days
        rfm["AOV"] = rfm["Net_Monetary"] / rfm["Frequency"]

        total_lines = df_1y.groupby("buyer username").size()
        rfm["Total_Lines"] = total_lines
        rfm["ReturnRate"] = (rfm["BadCount"] / rfm["Total_Lines"]).fillna(0)

        return rfm.reset_index()

    def run(self):
        self.log(f"ğŸš€ å¯åŠ¨ R-F-P-L-D å®¢æˆ·èšç±»åˆ†æ...")

        # 1. åŠ è½½å…¨é‡æ•°æ® (ä¸ºäº†è®¡ç®— RFMï¼Œéœ€è¦æ›´é•¿çš„æ—¶é—´çª—å£ï¼Œæ¯”å¦‚1å¹´)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç›´æ¥è¯»å–è¿‡å» 365 å¤©çš„æ•°æ®ï¼Œè€Œä¸æ˜¯ä»…æœ¬æœˆ
        self.log("åŠ è½½è¿‡å»ä¸€å¹´ (365å¤©) äº¤æ˜“æ•°æ®...")

        end_dt = pd.to_datetime(self.end_date)
        start_dt = end_dt - timedelta(days=365)

        df_raw = self.trans_repo.get_transactions_by_date(start_dt.date(), end_dt.date())

        if df_raw.empty:
            self.log("âš ï¸ è¿‡å»ä¸€å¹´æ— äº¤æ˜“æ•°æ®")
            return

        # 2. è®¡ç®— RFM
        self.log("æ­£åœ¨è®¡ç®—åŠ¨æ€å‡€å€¼ RFM æ¨¡å‹...")
        df_rfm = self._calculate_rfm_1y(df_raw)

        if df_rfm.empty:
            self.log("âš ï¸ è®¡ç®—åæ— æœ‰æ•ˆå®¢æˆ·æ•°æ®")
            return

        # 3. è¯Šæ–­
        self.log("æ­£åœ¨æ‰§è¡Œå®¢æˆ·åˆ†å±‚ (åŸºäºå‡€å€¼ä¸é£é™©)...")
        diagnostician = CustomerDiagnostician(metrics_cur=df_rfm, metrics_prev=None)
        df_final = diagnostician.diagnose()

        if df_final.empty:
            self.log("âš ï¸ æœªå‘ç°ç‰¹å¾æ˜¾è‘—çš„å®¢æˆ·")
            return

        df_final = df_final.sort_values("å‡€æ¶ˆè´¹é¢(Net LTV)", ascending=False)

        # 4. ä¿å­˜
        filename = f"Analysis_Customer_RFM_{self.file_suffix}.csv"
        footer = diagnostician.get_tag_definitions()

        self.save_csv(df_final, filename, footer)

==================== END FILE: core/services/crm.py ====================


==================== START FILE: core/services/logistics.py ====================
# core/services/logistics.py

import os
import pandas as pd
from collections import defaultdict
import tqdm
from core.base import BaseAnalyzer
from core.repository.transaction_repo import TransactionRepository


class ShippingAnalyzer(BaseAnalyzer):
    """
    [ä¸šåŠ¡æœåŠ¡] ç‰©æµæ•ˆç›Šè¯Šæ–­ (Shipping Analysis)
    å¤åˆ» V1.0 Logic: ç”Ÿæˆ 5 å¼ è¡¨ï¼Œé‡ç‚¹åˆ†æè¶…æ”¯ã€ç½šæ¬¾å’Œé€€è´§é‚®è´¹ã€‚
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.trans_repo = TransactionRepository()
        self.df_curr = pd.DataFrame()
        self.df_prev = pd.DataFrame()

    def run(self):
        self.log(f"ğŸš€ å¼€å§‹ç‰©æµåˆ†æ (Classic Mode): {self.start_date} -> {self.end_date}")

        # 1. åŠ è½½æ•°æ®
        self.df_curr = self.trans_repo.get_transactions_by_date(self.start_date, self.end_date)
        if self.df_curr.empty:
            self.log("âš ï¸ æœ¬æœŸæ— æ•°æ®")
            return

        # è®¡ç®—ä¸ŠæœŸæ—¶é—´
        delta = self.end_date - self.start_date
        prev_end = self.start_date - pd.Timedelta(days=1)
        prev_start = prev_end - delta
        self.log(f"æ­£åœ¨åŠ è½½ä¸ŠæœŸæ•°æ®: {prev_start} -> {prev_end}")
        self.df_prev = self.trans_repo.get_transactions_by_date(prev_start, prev_end)

        self.log("æ­£åœ¨è®¡ç®—ç‰©æµæŠ¥è¡¨...")

        # 2. è®¡ç®—æ ¸å¿ƒè¡¨ (df3)
        df3_curr = self._compute_df3(self.df_curr)
        df3_prev = self._compute_df3(self.df_prev)

        # 3. ç”Ÿæˆè¡ç”Ÿè¡¨
        t1 = self._table1(df3_curr, df3_prev)
        t2 = self._table2(self.df_curr, self.df_prev)
        t3 = df3_curr
        t4 = self._table4(df3_curr)
        t5 = self._table5(df3_curr)

        # 4. ä¿å­˜
        self._save_suite([t1, t2, t3, t4, t5])
        self.log(f" ç»å…¸ç‰©æµæŠ¥è¡¨å·²ç”Ÿæˆ: Analysis_Shipping_{self.file_suffix}.csv")

    # ---------- æ ¸å¿ƒè®¡ç®—é€»è¾‘ (å¤åˆ» V1.0) ---------- #

    def _compute_df3(self, df: pd.DataFrame) -> pd.DataFrame:
        if df.empty: return pd.DataFrame()

        # æŒ‰ Combo (Full SKU) èšåˆ
        combo_map = {}
        for o, g in df.groupby("order number"):
            # full sku å¯èƒ½æœ‰ç©ºå€¼ï¼Œåšä¸€ä¸‹æ¸…æ´—
            skus = sorted(set(g["full sku"].dropna().astype(str)))
            combo_map[o] = "+".join(skus) if skus else "Unknown"

        money = {}
        orders_by_combo = defaultdict(set)

        # é¢„è®¡ç®—å…³é”®åˆ— (åŠ é€Ÿ)
        cols = ["Shipping label-Earning data", "Shipping label-underpay", "Shipping label-overpay",
                "Shipping label-Return"]
        for c in cols:
            if c not in df.columns: df[c] = 0.0

        # éå†èšåˆ
        # æ³¨æ„: è¿™é‡Œçš„ df å·²ç»æ˜¯æ¸…æ´—åçš„ï¼Œæ¯è¡Œä»£è¡¨ä¸€ä¸ª itemã€‚
        # æˆ‘ä»¬éœ€è¦æŒ‰ Order èšåˆé‡‘é¢ï¼Œå› ä¸º Order æ˜¯é‚®è´¹å‘ç”Ÿçš„å•ä½ã€‚
        # V1.0 é€»è¾‘æ˜¯å…ˆæŒ‰ Order Groupï¼Œå†æŒ‰ Combo å½’ç±»ã€‚

        df_grouped = df.groupby("order number")[cols].sum()

        for order_num, row in df_grouped.iterrows():
            combo = combo_map.get(order_num, "Unknown")
            if combo not in money:
                money[combo] = {"åŸå§‹é‚®è´¹": 0.0, "è¶…æ”¯é‚®è´¹": 0.0, "é‚®è´¹ç½šæ¬¾": 0.0, "åŒ…é‚®é€€è´§é‚®è´¹": 0.0}

            rec = money[combo]
            # åŸå§‹é‚®è´¹ = Earning + Underpay + Overpay (æ€»æ”¯å‡º)
            rec["åŸå§‹é‚®è´¹"] += (row["Shipping label-Earning data"] + row["Shipping label-underpay"] + row[
                "Shipping label-overpay"])
            rec["è¶…æ”¯é‚®è´¹"] += row["Shipping label-overpay"]
            rec["é‚®è´¹ç½šæ¬¾"] += row["Shipping label-underpay"]
            rec["åŒ…é‚®é€€è´§é‚®è´¹"] += row["Shipping label-Return"]

            orders_by_combo[combo].add(order_num)

        # ç»Ÿè®¡å¼‚å¸¸å•é›†åˆ
        # æ³¨æ„: è¿™é‡Œåˆ¤æ–­çš„æ˜¯ sum > 0ï¼Œå³è¯¥è®¢å•æ˜¯å¦å‘ç”Ÿè¿‡å¯¹åº”è´¹ç”¨
        over_set = set(df_grouped[df_grouped["Shipping label-overpay"] > 0].index)
        penal_set = set(df_grouped[df_grouped["Shipping label-underpay"] != 0].index)
        ret_set = set(df_grouped[df_grouped["Shipping label-Return"] != 0].index)

        rows = []
        for combo, vals in money.items():
            ords = orders_by_combo[combo]
            rows.append({
                "Combo": combo,
                "åŸå§‹é‚®è´¹": round(vals["åŸå§‹é‚®è´¹"], 2),
                "è¶…æ”¯é‚®è´¹": round(vals["è¶…æ”¯é‚®è´¹"], 2),
                "é‚®è´¹ç½šæ¬¾": round(vals["é‚®è´¹ç½šæ¬¾"], 2),
                "åŒ…é‚®é€€è´§é‚®è´¹": round(vals["åŒ…é‚®é€€è´§é‚®è´¹"], 2),
                "åŸå§‹å•æ•°": len(ords),
                "è¶…æ”¯å•æ•°": len(ords & over_set),
                "ç½šæ¬¾å•æ•°": len(ords & penal_set),
                "åŒ…é‚®é€€è´§å•æ•°": len(ords & ret_set),
            })

        if not rows: return pd.DataFrame()

        df3 = pd.DataFrame(rows).sort_values("åŸå§‹é‚®è´¹", ascending=False)

        # è®¡ç®—æ¯”ä¾‹
        df3["ç½šæ¬¾æ¯”ä¾‹"] = (df3["é‚®è´¹ç½šæ¬¾"] / df3["åŸå§‹é‚®è´¹"]).fillna(0).apply(lambda x: f"{x:.2%}")
        df3["ç½šæ¬¾å•æ•°æ¯”ä¾‹"] = (df3["ç½šæ¬¾å•æ•°"] / df3["åŸå§‹å•æ•°"]).fillna(0).apply(lambda x: f"{x:.2%}")
        df3["æ€»è®¢å•æ•°"] = df3["åŸå§‹å•æ•°"]  # V1.0 é€»è¾‘: æ€»è®¢å•æ•°â‰ˆåŸå§‹å•æ•° (å«å¼‚å¸¸å•)

        # åˆ—é¡ºåºè°ƒæ•´
        cols_order = ["Combo", "åŸå§‹é‚®è´¹", "è¶…æ”¯é‚®è´¹", "é‚®è´¹ç½šæ¬¾", "åŒ…é‚®é€€è´§é‚®è´¹",
                      "åŸå§‹å•æ•°", "è¶…æ”¯å•æ•°", "ç½šæ¬¾å•æ•°", "åŒ…é‚®é€€è´§å•æ•°",
                      "ç½šæ¬¾æ¯”ä¾‹", "ç½šæ¬¾å•æ•°æ¯”ä¾‹", "æ€»è®¢å•æ•°"]
        return df3[cols_order]

    def _table1(self, cur: pd.DataFrame, prev: pd.DataFrame) -> pd.DataFrame:
        """è¡¨1 è´¹ç”¨æ±‡æ€»"""
        if cur.empty: return pd.DataFrame()
        c_vals = [cur["åŸå§‹é‚®è´¹"].sum(), cur["è¶…æ”¯é‚®è´¹"].sum(), cur["é‚®è´¹ç½šæ¬¾"].sum()]
        p_vals = [prev["åŸå§‹é‚®è´¹"].sum(), prev["è¶…æ”¯é‚®è´¹"].sum(), prev["é‚®è´¹ç½šæ¬¾"].sum()] if not prev.empty else [0, 0,
                                                                                                                  0]

        # æ€»é‚®è´¹ = åŸå§‹é‚®è´¹ (V1.0 é€»è¾‘: cur_vals[0] + [1] + [2] ? ä¸ï¼Œ_compute_df3 é‡ŒåŸå§‹é‚®è´¹å·²ç»åŒ…å«äº†)
        # V1.0 ä»£ç : cur_total = sum(cur_vals) -> è¿™å…¶å®æ˜¯é‡å¤åŠ äº†ã€‚
        # ä½†æ—¢ç„¶è¦å¤åˆ»ï¼Œæˆ‘ä»¬éœ€è¦çœ‹ V1.0 çš„å®šä¹‰ã€‚
        # V1.0: rec["åŸå§‹é‚®è´¹"] += (Earning + Under + Over).
        # Table 1: cur_vals = [åŸå§‹, è¶…æ”¯, ç½šæ¬¾]. sum(cur_vals) = åŸå§‹ + è¶…æ”¯ + ç½šæ¬¾.
        # è¿™æ„å‘³ç€ "æ€»é‚®è´¹" è¿™ä¸€è¡Œæ˜¯æ‰€æœ‰é¡¹çš„åŠ å’Œã€‚è™½ç„¶é€»è¾‘ä¸Šæœ‰ç‚¹æ€ª(åŸå§‹å·²ç»åŒ…å«äº†)ï¼Œä½†å¦‚æœ V1.0 æ˜¯è¿™ä¹ˆå†™çš„ï¼Œæˆ‘ä»¬å°±ç…§åšã€‚
        # ä¿®æ­£: V1.0 sum(cur_vals) æ˜¯ä¸ºäº†ç®—æ¯”ä¾‹çš„åˆ†æ¯ã€‚

        total_c = sum(c_vals)
        total_p = sum(p_vals)

        names = ["æ€»é‚®è´¹", "æ­£å¸¸é‚®è´¹", "è¶…æ”¯é‚®è´¹", "ç½šæ¬¾é‚®è´¹"]
        # æ˜ å°„: æ€»é‚®è´¹->sum, æ­£å¸¸->åŸå§‹(å…¶å®ä¸å¯¹, æ­£å¸¸åº”è¯¥æ˜¯ Earning), è¶…æ”¯->Over, ç½šæ¬¾->Under
        # V1.0 _table1: names[0] -> cur_total; names[1] -> cur_vals[0] (åŸå§‹); names[2] -> [1]; names[3] -> [2]
        # ç­‰ç­‰ï¼Œå¦‚æœ V1.0 çš„ "åŸå§‹é‚®è´¹" æ˜¯ Earning+Over+Underï¼Œé‚£å®ƒå…¶å®å°±æ˜¯æ€»æ”¯å‡ºã€‚
        # çœ‹æ¥ V1.0 çš„å‘½åæœ‰ç‚¹æ··æ·†ã€‚æˆ‘ä»¬æŒ‰ V1.0 ä»£ç å¤åˆ»ï¼š

        rows = []
        # Row 0: æ€»é‚®è´¹ (sum of 3 items from df3, which are aggregations)
        rows.append(["æ€»é‚®è´¹", total_c, "100.00%", self._diff(total_c, total_p)])
        # Row 1: åŸå§‹é‚®è´¹ (from df3 "åŸå§‹é‚®è´¹")
        rows.append(["åŸå§‹é‚®è´¹", c_vals[0], self._pct(c_vals[0], total_c), self._diff(c_vals[0], p_vals[0])])
        # Row 2: è¶…æ”¯
        rows.append(["è¶…æ”¯é‚®è´¹", c_vals[1], self._pct(c_vals[1], total_c), self._diff(c_vals[1], p_vals[1])])
        # Row 3: ç½šæ¬¾
        rows.append(["ç½šæ¬¾é‚®è´¹", c_vals[2], self._pct(c_vals[2], total_c), self._diff(c_vals[2], p_vals[2])])

        return pd.DataFrame(rows, columns=["é¡¹ç›®", "è´¹ç”¨", "æ¯”ä¾‹", "ç¯æ¯”"])

    def _table2(self, df_c: pd.DataFrame, df_p: pd.DataFrame) -> pd.DataFrame:
        """è¡¨2 å•æ•°æ±‡æ€»"""

        def get_sets(df):
            if df.empty: return set(), set(), set(), set()
            # æ’é™¤ Action=CA (å–æ¶ˆå•)
            valid = df[df['action'] != 'CA']
            # æŒ‰ Order å»é‡
            base = set(valid['order number'])
            over = set(valid[valid['Shipping label-overpay'] > 0]['order number'])
            penal = set(valid[valid['Shipping label-underpay'] != 0]['order number'])
            ret = set(valid[valid['Shipping label-Return'] != 0]['order number'])
            return base, over, penal, ret

        bc, oc, pc, rc = get_sets(df_c)
        bp, op, pp, rp = get_sets(df_p)

        # V1.0 Logic:
        # Total = len(bc)
        # Normal = len(bc - oc - pc - rc)
        # Over = len(oc) ...

        nc = [len(bc), len(bc - oc - pc - rc), len(oc), len(pc), len(rc)]
        np_vals = [len(bp), len(bp - op - pp - rp), len(op), len(pp), len(rp)]

        rows = []
        names = ["æ€»è®¢å•", "æ­£å¸¸é‚®è´¹å•", "è¶…æ”¯è®¢å•", "ç½šæ¬¾è®¢å•", "åŒ…é‚®é€€è´§"]
        for i, name in enumerate(names):
            rows.append([name, nc[i], self._pct(nc[i], nc[0]), self._diff(nc[i], np_vals[i])])

        return pd.DataFrame(rows, columns=["é¡¹ç›®", "å•æ•°", "æ¯”ä¾‹", "ç¯æ¯”"])

    def _table4(self, df3: pd.DataFrame) -> pd.DataFrame:
        """è¡¨4 ç½šæ¬¾é‡‘é¢ Top 10"""
        if df3.empty: return pd.DataFrame()
        t = df3[df3["æ€»è®¢å•æ•°"] > 5].copy()
        t["val"] = t["ç½šæ¬¾æ¯”ä¾‹"].str.rstrip("%").astype(float)
        return t.nlargest(10, "val")[["Combo", "æ€»è®¢å•æ•°", "åŸå§‹é‚®è´¹", "é‚®è´¹ç½šæ¬¾", "ç½šæ¬¾æ¯”ä¾‹"]]

    def _table5(self, df3: pd.DataFrame) -> pd.DataFrame:
        """è¡¨5 ç½šæ¬¾å•æ•° Top 10"""
        if df3.empty: return pd.DataFrame()
        t = df3[df3["æ€»è®¢å•æ•°"] > 5].copy()
        t["val"] = t["ç½šæ¬¾å•æ•°æ¯”ä¾‹"].str.rstrip("%").astype(float)
        return t.nlargest(10, "val")[["Combo", "æ€»è®¢å•æ•°", "åŸå§‹å•æ•°", "ç½šæ¬¾å•æ•°", "ç½šæ¬¾å•æ•°æ¯”ä¾‹"]]

    def _save_suite(self, tables):
        path = os.path.join(self.output_dir, f"Analysis_Shipping_{self.file_suffix}.csv")
        with open(path, "w", encoding="utf-8-sig") as f:
            for i, df in enumerate(tables, 1):
                f.write(f"è¡¨{i}\n")
                df.to_csv(f, index=False)
                f.write("\n")

    def _pct(self, v, total):
        return f"{v / total:.2%}" if total else "0.00%"

    def _diff(self, cur, prev):
        if not prev: return "0.00%"
        return f"{(cur - prev) / prev:.2%}"

==================== END FILE: core/services/logistics.py ====================


==================== START FILE: core/services/report_manager.py ====================
# core/services/report_manager.py

import os
import shutil
import zipfile
from io import BytesIO
from typing import List, Optional
from config import Config


class ReportFileManager:
    """
    [åº”ç”¨æœåŠ¡] æŠ¥è¡¨æ–‡ä»¶ç®¡ç†å™¨ (File Lifecycle Manager)
    V2.1 Update: æ”¯æŒæŒ‰éœ€æ–‡ä»¶åˆ—è¡¨æ‰“åŒ…
    """

    def __init__(self):
        self.output_dir = Config.OUTPUT_DIR
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir, exist_ok=True)

    def clear_old_reports(self) -> None:
        """[æ¸…ç†] æ¸…ç©ºè¾“å‡ºç›®å½•"""
        if not os.path.exists(self.output_dir): return
        for filename in os.listdir(self.output_dir):
            file_path = os.path.join(self.output_dir, filename)
            try:
                if os.path.isfile(file_path) or os.path.islink(file_path):
                    os.unlink(file_path)
                elif os.path.isdir(file_path):
                    shutil.rmtree(file_path)
            except Exception as e:
                print(f"âš ï¸ [ReportManager] æ¸…ç†å¤±è´¥: {file_path}, é”™è¯¯: {e}")

    def get_generated_files(self) -> List[str]:
        """[æŸ¥è¯¢] è·å–å½“å‰å·²ç”Ÿæˆçš„ CSV æ–‡ä»¶åˆ—è¡¨"""
        if not os.path.exists(self.output_dir): return []
        return sorted([f for f in os.listdir(self.output_dir) if f.lower().endswith('.csv')])

    def get_file_path(self, filename: str) -> str:
        """[è¾…åŠ©] è·å–æ–‡ä»¶çš„ç»å¯¹è·¯å¾„"""
        return os.path.join(self.output_dir, filename)

    def create_zip_archive(self, file_whitelist: Optional[List[str]] = None) -> bytes:
        """
        [æ‰“åŒ…] å°†è¾“å‡ºç›®å½•ä¸‹çš„ CSV æ‰“åŒ…ä¸º ZIP å­—èŠ‚æµã€‚

        Args:
            file_whitelist: å¦‚æœæä¾›ï¼Œåªæ‰“åŒ…åˆ—è¡¨ä¸­çš„æ–‡ä»¶ï¼›å¦åˆ™æ‰“åŒ…æ‰€æœ‰ CSVã€‚
        """
        zip_buffer = BytesIO()

        # è·å–æ‰€æœ‰ç‰©ç†å­˜åœ¨çš„æ–‡ä»¶
        all_files = self.get_generated_files()

        # è¿‡æ»¤
        if file_whitelist is not None:
            target_files = [f for f in all_files if f in file_whitelist]
        else:
            target_files = all_files

        with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
            for file in target_files:
                abs_path = self.get_file_path(file)
                if os.path.exists(abs_path):
                    # arcname ç¡®ä¿å­˜å…¥å‹ç¼©åŒ…çš„æ˜¯æ–‡ä»¶åï¼Œè€Œéç»å¯¹è·¯å¾„
                    zf.write(abs_path, arcname=file)

        return zip_buffer.getvalue()
==================== END FILE: core/services/report_manager.py ====================


==================== START FILE: core/services/visual_service.py ====================
# core/services/visual_service.py

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from datetime import datetime, date

from core.repository.db_client import DBClient
from core.repository.sku_repo import SkuRepository
from core.services.profit_base import ProfitAnalyzerBase
from core.logging_config import get_logger
from config import Config


class VisualService:
    """
    [æ ¸å¿ƒå¼•æ“] å¯è§†åŒ–æ•°æ®èšåˆæœåŠ¡
    V2.0 Fix: Add 'Order Count' support for Shipping Types.
    Logic: Regular_Orders = Total_Orders - Unique_Orders_With_Issues
    """

    def __init__(self):
        self.db = DBClient()
        self.sku_repo = SkuRepository()
        self.logger = get_logger(self.__class__.__name__)

        self.PLATFORM_FEE_COLS = list(ProfitAnalyzerBase.PLATFORM_FEE_GROUP)
        self.FEE_COLS = ProfitAnalyzerBase.FEE_COLUMNS

        self.ACTION_GROUPS = {
            'CA': 'Cancel', 'RE': 'Return', 'CC': 'Case',
            'CR': 'Request', 'PD': 'Dispute'
        }

        self.STORE_MAP_UI_TO_DB = {
            "88": "esparts88",
            "esplus": "espartsplus"
        }

        self.KEY_SKUS = {"NU1C8E51C", "NU1C8E51K"}

    def _get_date_grain(self, start: date, end: date) -> str:
        delta = (end - start).days
        if delta <= 35:
            return 'D'
        elif delta <= 180:
            return 'W'
        else:
            return 'ME'

    def _vectorized_cogs_calc(self, df: pd.DataFrame, sku_cost_map: Dict[str, float]) -> pd.Series:
        total_cogs = pd.Series(0.0, index=df.index)
        # Quantity å®‰å…¨è½¬æ¢
        base_qty = pd.to_numeric(df['quantity'], errors='coerce').fillna(0.0)

        for i in range(1, 11):
            s_col = f'sku{i}'
            q_col = f'qty{i}'
            if s_col not in df.columns: continue

            sku_series = df[s_col].astype(str).str.strip().str.upper()
            unit_costs = sku_series.map(sku_cost_map).fillna(0.0)
            per_qtys = pd.to_numeric(df[q_col], errors='coerce').fillna(0.0)

            line_cost = unit_costs * per_qtys * base_qty
            total_cogs += line_cost

        return total_cogs

    def load_and_aggregate(self, start_date: date, end_date: date, stores: List[str]) -> Tuple[pd.DataFrame, str]:
        if not stores:
            return pd.DataFrame(), "No stores selected"

        db_stores = [self.STORE_MAP_UI_TO_DB.get(s, s) for s in stores]

        start_str = start_date.strftime('%Y-%m-%d')
        end_str = end_date.strftime('%Y-%m-%d')
        store_str = "', '".join(db_stores)

        sql = f"""
            SELECT * FROM Data_Clean_Log 
            WHERE `order date` >= '{start_str}' 
              AND `order date` <= '{end_str}'
              AND `seller` IN ('{store_str}')
        """

        self.logger.info(f"Visual Query: {sql}")
        df = self.db.read_df(sql)

        if df.empty:
            return pd.DataFrame(), sql

        # --- Preprocessing ---
        df.columns = [c.strip().lower() for c in df.columns]

        # 1. Base Numerics
        df['quantity'] = pd.to_numeric(df.get('quantity', 0), errors='coerce').fillna(0.0)
        df['revenue'] = pd.to_numeric(df.get('revenue', 0), errors='coerce').fillna(0.0)

        # 2. Strict Quantity Logic
        qty_sum_series = pd.Series(0.0, index=df.index)
        for i in range(1, 9):
            col = f'qty{i}'
            if col in df.columns:
                col_vals = pd.to_numeric(df[col], errors='coerce').fillna(0.0)
            else:
                col_vals = 0.0
            qty_sum_series += col_vals

        base_units = df['quantity'] * qty_sum_series

        is_key_row = pd.Series(False, index=df.index)
        for i in range(1, 9):
            s_col = f'sku{i}'
            if s_col in df.columns:
                current_skus = df[s_col].astype(str).str.strip().str.upper()
                is_key_row = is_key_row | current_skus.isin(self.KEY_SKUS)

        df['ui_action'] = df['action'].astype(str).str.strip().str.upper().map(self.ACTION_GROUPS).fillna('Sales')

        adj_factor = np.where(df['ui_action'] == 'Sales', 2.0, -2.0)
        adjustment = np.where(is_key_row, adj_factor * df['quantity'], 0.0)

        df['calc_real_qty'] = base_units + adjustment

        # 3. Fees & Shipping (Abs & Order Counting Prep)
        ship_cols_map = {
            'shipping label-earning data': 'calc_ship_regular',
            'shipping label-underpay': 'calc_ship_under',
            'shipping label-overpay': 'calc_ship_over',
            'shipping label-return': 'calc_ship_return'
        }
        for db_col, calc_col in ship_cols_map.items():
            if db_col in df.columns:
                df[calc_col] = pd.to_numeric(df[db_col], errors='coerce').fillna(0.0).abs()
            else:
                df[calc_col] = 0.0

        # [New] Prepare columns for Order Counting (Set OrderID if condition met, else NaN)
        # Condition: Fee > 0
        df['ord_under'] = df['order number'].where(df['calc_ship_under'] > 0.001)
        df['ord_over'] = df['order number'].where(df['calc_ship_over'] > 0.001)
        df['ord_ret'] = df['order number'].where(df['calc_ship_return'] > 0.001)
        # Any Issue (Union of problems) for calculating Regular
        df['ord_issue'] = df['order number'].where(
            (df['calc_ship_under'] > 0.001) |
            (df['calc_ship_over'] > 0.001) |
            (df['calc_ship_return'] > 0.001)
        )

        valid_plat_cols = [c.lower() for c in self.PLATFORM_FEE_COLS if c.lower() in df.columns]
        if valid_plat_cols:
            df['calc_platform_fee'] = df[valid_plat_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0).sum(
                axis=1).abs()
        else:
            df['calc_platform_fee'] = 0.0

        df['dt'] = pd.to_datetime(df['order date'])

        # 4. COGS
        cogs_df = self.sku_repo.get_all_cogs()
        sku_cost_map = dict(zip(
            cogs_df['SKU'].str.strip().str.upper(),
            pd.to_numeric(cogs_df['Cog'], errors='coerce').fillna(0)
        ))
        df['calc_cogs'] = self._vectorized_cogs_calc(df, sku_cost_map).abs()

        # --- Aggregation ---
        grain = self._get_date_grain(start_date, end_date)

        agg_rules = {
            'revenue': 'sum',
            'calc_real_qty': 'sum',
            'order number': 'nunique',
            'calc_cogs': 'sum',
            'calc_platform_fee': 'sum',
            'calc_ship_regular': 'sum',
            'calc_ship_under': 'sum',
            'calc_ship_over': 'sum',
            'calc_ship_return': 'sum',
            # [New] Count Distinct for Shipping Orders
            'ord_under': 'nunique',
            'ord_over': 'nunique',
            'ord_ret': 'nunique',
            'ord_issue': 'nunique'
        }

        grouped = df.groupby([pd.Grouper(key='dt', freq=grain), 'ui_action']).agg(agg_rules).reset_index()

        pivot_df = grouped.pivot(index='dt', columns='ui_action', values=list(agg_rules.keys()))
        pivot_df.columns = [f"{col[1]}_{col[0]}" for col in pivot_df.columns]

        total_agg = df.groupby(pd.Grouper(key='dt', freq=grain)).agg(agg_rules)
        total_agg.columns = [f"Total_{c}" for c in total_agg.columns]

        final_df = pd.concat([pivot_df, total_agg], axis=1).fillna(0)

        # [New] Calculate Regular Shipping Orders (Total - Issues)
        # Note: logic from logistics.py: Regular = Total - (Over + Under + Return set union)
        if 'Total_order number' in final_df.columns and 'Total_ord_issue' in final_df.columns:
            final_df['Total_ord_regular'] = (final_df['Total_order number'] - final_df['Total_ord_issue']).clip(lower=0)
        else:
            final_df['Total_ord_regular'] = 0

        # Rename Mapping
        rename_map = {}
        for col in final_df.columns:
            # Map temp aggregation columns to clean output names
            new = col.replace('revenue', 'Amount') \
                .replace('calc_real_qty', 'Quantity') \
                .replace('order number', 'Order') \
                .replace('calc_cogs', 'COGS') \
                .replace('calc_platform_fee', 'PlatformFee') \
                .replace('calc_ship_regular', 'ShipRegular') \
                .replace('calc_ship_under', 'ShipUnder') \
                .replace('calc_ship_over', 'ShipOver') \
                .replace('calc_ship_return', 'ShipReturn') \
                .replace('ord_under', 'Order_ShipUnder') \
                .replace('ord_over', 'Order_ShipOver') \
                .replace('ord_ret', 'Order_ShipReturn') \
                .replace('ord_regular', 'Order_ShipRegular') \
                .replace('ord_issue', 'Order_ShipIssue')
            rename_map[col] = new

        final_df = final_df.rename(columns=rename_map)
        final_df['DateStr'] = final_df.index.strftime('%Y-%m-%d')

        return final_df.sort_index(), sql
==================== END FILE: core/services/visual_service.py ====================


==================== START FILE: core/services/profit_combo.py ====================
# core/services/profit_combo.py
import os
import pandas as pd
from collections import defaultdict
import tqdm

from core.services.profit_base import ProfitAnalyzerBase
from core.services.diagnostics.combo import ComboDiagnostician

class ComboProfitAnalyzer(ProfitAnalyzerBase):
    """
    [ä¸šåŠ¡æœåŠ¡] Full SKU (Combo) ç»„åˆåˆ©æ¶¦åˆ†æå™¨ (V2.3 Normalized)
    """

    def _aggregate(self, df: pd.DataFrame) -> dict:
        metrics = defaultdict(lambda: defaultdict(float))
        if df.empty: return metrics

        records = df.to_dict('records')
        for row in tqdm.tqdm(records, desc="èšåˆ Combo æ•°æ®"):
            # [æ ¸å¿ƒä¿®å¤] å¼ºåˆ¶å¤§å†™
            raw_full = str(row.get("full sku", ""))
            if not raw_full or raw_full == '0': continue
            full_sku = raw_full.strip().upper()

            qty_sets = int(float(row.get("quantity", 0)))
            action = str(row.get("action", "")).strip().upper()
            revenue = float(row.get("revenue", 0))
            refund = float(row.get("Refund", 0))

            metrics[full_sku]["total_qty"] += qty_sets
            metrics[full_sku]["total_rev"] += revenue

            if action == "CA": metrics[full_sku]["cancel_qty"] += qty_sets; metrics[full_sku]["cancel_rev"] += refund
            elif action == "RE": metrics[full_sku]["return_qty"] += qty_sets; metrics[full_sku]["return_rev"] += refund
            elif action == "CR": metrics[full_sku]["request_qty"] += qty_sets; metrics[full_sku]["request_rev"] += refund
            elif action == "CC": metrics[full_sku]["claim_qty"] += qty_sets; metrics[full_sku]["claim_rev"] += refund
            elif action == "PD": metrics[full_sku]["dispute_qty"] += qty_sets; metrics[full_sku]["dispute_rev"] += refund

            row_cost = 0.0
            for i in range(1, 21):
                s_key, q_key = f"sku{i}", f"qty{i}"
                if s_key not in row: break

                # SKU åŒ¹é…æˆæœ¬ä¹Ÿéœ€è¦å¤§å†™
                raw_sku = str(row[s_key])
                if not raw_sku or raw_sku == '0': continue
                sku = raw_sku.strip().upper()

                try:
                    q_per = float(row[q_key])
                except:
                    q_per = 0

                row_cost += (self.sku_cost_map.get(sku, 0.0) * q_per * qty_sets)

                if sku in ["NU1C8E51C", "NU1C8E51K"]:
                    row_cost += (self.sku_cost_map.get("NU1C8SKT7", 0.0) * 2 * qty_sets)

            metrics[full_sku]["cog_value"] += -row_cost
            self._accumulate_fees(row, metrics, full_sku, weight=1.0)

        return metrics

    def run(self):
        self._load_basics()
        if self.df_cur is None or self.df_cur.empty:
            self.log("âš ï¸ æœ¬æœŸæ— æ•°æ®ï¼Œæ— æ³•åˆ†æ")
            return
        self.log(f"ğŸ“Š å·²åŠ è½½åŸå§‹è®°å½•: {len(self.df_cur)} æ¡")
        self.log("æ­£åœ¨èšåˆæœ¬æœŸæ•°æ®...")
        m_cur = self._calculate_net_profit(self._aggregate(self.df_cur))
        self.log("æ­£åœ¨èšåˆä¸ŠæœŸæ•°æ®(ç”¨äºç¯æ¯”)...")
        m_prev = self._calculate_net_profit(self._aggregate(self.df_prev))

        tables = self.generate_full_report_suite(m_cur, m_prev, key_name="Full SKU")
        self.log("æ­£åœ¨æ‰§è¡Œ AI æ™ºèƒ½è¯Šæ–­...")
        diag = ComboDiagnostician(m_cur, m_prev)
        df_diag = diag.diagnose()
        tables.append(("C1_æ™ºèƒ½è¯Šæ–­è¡¨ (AI Diagnostics)", df_diag))
        explanation_lines = diag.get_tag_definitions()

        filename = f"Profit_Analysis_Combo_{self.file_suffix}.csv"
        save_path = os.path.join(self.output_dir, filename)
        try:
            with open(save_path, "w", encoding="utf-8-sig") as f:
                for name, df in tables:
                    f.write(f"=== {name} ===\n")
                    df.to_csv(f, index=False)
                    f.write("\n\n")
                f.write("\n")
                for line in explanation_lines:
                    f.write(f"{line}\n")
            self.log(f" Combo åˆ©æ¶¦æŠ¥è¡¨å·²ç”Ÿæˆ: {filename}")
        except Exception as e:
            self.log(f" ä¿å­˜å¤±è´¥: {e}")

==================== END FILE: core/services/profit_combo.py ====================


==================== START FILE: core/services/prediction.py ====================
# core/services/prediction.py

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
import tqdm
from collections import defaultdict
from typing import Dict, Any

from core.base import BaseAnalyzer
from core.repository.transaction_repo import TransactionRepository
from core.repository.sku_repo import SkuRepository
from config import Config

# å¼•å…¥ç®—æ³•æ¨¡å‹
from core.algo.models import (
    XGBoostForecaster, SarimaForecaster, HoltWintersForecaster,
    ETSForecaster, CrostonForecaster, WeightedCycleForecaster
)


class PredictionService(BaseAnalyzer):
    """
    [ä¸šåŠ¡æœåŠ¡] AI é”€é‡é¢„æµ‹å¼•æ“ (Prediction Engine) - Enterprise V2.0

    èŒè´£ï¼š
    1. å†å²æ•°æ®æ¸…æ´—ä¸èšåˆ (Data Aggregation)ã€‚
    2. å¤šæ¨¡å‹ç«èµ›æœºåˆ¶ (Tournament Mode) é€‰æ‹©æœ€ä½³ç®—æ³•ã€‚
    3. ç”Ÿæˆæœªæ¥ N æœŸçš„é”€é‡é¢„æµ‹ã€‚

    å…³é”®ä¸šåŠ¡è§„åˆ™ (Business Rules):
    - è€—æŸç‡: æ ¹æ® Action (CA, RE, etc.) æ‰£é™¤æ— æ•ˆé”€é‡ã€‚
    - ç‰¹æ®Š SKU: "NU1C8E51C/K" éœ€å¼ºåˆ¶å…³è”è®¡ç®—åˆ° "NU1C8SKT7"ã€‚
    - æ–°å“ç­–ç•¥: æ•°æ®ç‚¹å°‘äº 6 ä¸ªæœˆæ—¶ï¼Œé™çº§ä½¿ç”¨åŠ æƒå¹³å‡ç®—æ³•ã€‚
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.trans_repo = TransactionRepository()
        self.sku_repo = SkuRepository()

        # åˆå§‹åŒ–ç®—æ³•æ± 
        self.models = [
            XGBoostForecaster(),
            SarimaForecaster(),
            HoltWintersForecaster(),
            ETSForecaster(),
            CrostonForecaster(),
            WeightedCycleForecaster()
        ]

        # ç¼“å­˜é…ç½®ï¼Œå‡å°‘å¾ªç¯è¯»å–
        self.LOSS_RATES = Config.LOSS_RATES

    def _get_loss_rate(self, action: str) -> float:
        """
        [è¾…åŠ©] æ ¹æ®è®¢å•åŠ¨ä½œè·å–è€—æŸç‡
        """
        action = str(action).strip().upper()
        if action == 'CA': return 1.0
        if action == 'RE': return self.LOSS_RATES.get('RETURN', 0.3)
        if action == 'CC': return self.LOSS_RATES.get('CASE', 0.6)
        if action == 'CR': return self.LOSS_RATES.get('REQUEST', 0.5)
        if action == 'PD': return self.LOSS_RATES.get('DISPUTE', 1.0)
        return 0.0

    def _aggregate_monthly_sales(self) -> pd.DataFrame:
        """
        [æ ¸å¿ƒé€»è¾‘] èšåˆè¿‡å» 24 ä¸ªæœˆçš„å†å²é”€é‡
        """
        # 1. ç¡®å®šæ—¶é—´çª—å£
        end_dt = datetime.now().replace(day=1) - timedelta(days=1)
        start_dt = end_dt - relativedelta(months=24)

        self.log(f"ğŸ“¥ æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®: {start_dt.date()} -> {end_dt.date()}")
        df_raw = self.trans_repo.get_transactions_by_date(start_dt.date(), end_dt.date())

        if df_raw.empty:
            self.log("âš ï¸ è­¦å‘Š: æŒ‡å®šèŒƒå›´å†…æ— äº¤æ˜“æ•°æ®ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹ã€‚")
            return pd.DataFrame()

        self.log(f"ğŸ“Š åŸå§‹è®°å½•åŠ è½½å®Œæˆ: {len(df_raw)} æ¡ï¼Œå¼€å§‹èšåˆå¤„ç†...")

        # 2. é€è¡Œèšåˆ (ç”±äºæ¶‰åŠå¤æ‚çš„ 1-20 åˆ—å±•å¼€å’Œç‰¹æ®Š SKU é€»è¾‘ï¼Œä¿æŒè¿­ä»£å¤„ç†æœ€ç¨³å¦¥)
        monthly_data = defaultdict(lambda: defaultdict(int))
        records = df_raw.to_dict('records')

        # é¢„å®šä¹‰ç‰¹æ®Š SKU é›†åˆï¼ŒåŠ é€Ÿåˆ¤æ–­
        SPECIAL_SOURCE_SKUS = {"NU1C8E51C", "NU1C8E51K"}
        SPECIAL_TARGET_SKU = "NU1C8SKT7"

        for row in tqdm.tqdm(records, desc="èšåˆé”€é‡æ•°æ®"):
            # A. åŸºç¡€æ ¡éªŒ
            date_val = row.get("order date")
            if pd.isna(date_val): continue

            # è½¬ä¸ºå¹´æœˆ Key (YYYY-MM)
            month_key = date_val.strftime("%Y-%m")

            # B. è®¡ç®—è€—æŸ
            action = row.get("action", "")
            loss_rate = self._get_loss_rate(action)

            # å¦‚æœè€—æŸæ˜¯ 100% (å¦‚å–æ¶ˆå•)ï¼Œç›´æ¥è·³è¿‡åç»­è®¡ç®—ï¼Œæå‡æ€§èƒ½
            if loss_rate >= 1.0: continue

            effective_ratio = 1.0 - loss_rate

            # C. æå–æ•°é‡åŸºæ•°
            try:
                base_qty = int(float(row.get("quantity", 0)))
            except (ValueError, TypeError):
                base_qty = 0

            if base_qty <= 0: continue

            # D. éå† SKU1 - SKU20
            for i in range(1, 21):
                s_key = f"sku{i}"
                q_key = f"qty{i}"

                # åŠ¨æ€åˆ—æ£€æŸ¥ (å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ Schema)
                if s_key not in row:
                    # å°è¯•æ—§ç‰ˆå­—æ®µåå…¼å®¹
                    if f"P_SKU{i}" in row:
                        s_key, q_key = f"P_SKU{i}", f"P_Quantity{i}"
                    else:
                        break

                raw_sku = str(row.get(s_key))
                # ç©ºå€¼æ£€æŸ¥
                if not raw_sku or raw_sku.lower() in ['nan', 'none', '', '0']: continue

                # æ ¸å¿ƒ: å½’ä¸€åŒ– (å¤§å†™ + å»ç©ºæ ¼)
                sku = raw_sku.strip().upper()

                try:
                    per_qty = float(row.get(q_key, 0))
                except (ValueError, TypeError):
                    per_qty = 0

                if per_qty <= 0: continue

                # E. è®¡ç®—å‡€é”€é‡å¹¶ç´¯åŠ 
                # å…¬å¼: è®¢å•å¥—æ•° * æ¯å¥—ä¸ªæ•° * (1 - è€—æŸç‡)
                net_qty = base_qty * per_qty * effective_ratio
                monthly_data[sku][month_key] += int(net_qty)

                # F. ç‰¹æ®Š SKU å…³è”é€»è¾‘ (Business Rule)
                if sku in SPECIAL_SOURCE_SKUS:
                    # è§„åˆ™: å…³è”åˆ° NU1C8SKT7ï¼Œæ•°é‡ç¿»å€ (base_qty * 2)
                    special_qty = base_qty * 2 * effective_ratio
                    monthly_data[SPECIAL_TARGET_SKU][month_key] += int(special_qty)

        # 3. è½¬æ¢ä¸º DataFrame
        if not monthly_data:
            return pd.DataFrame()

        df = pd.DataFrame.from_dict(monthly_data, orient='index').fillna(0)
        # æŒ‰æ—¶é—´è½´æ’åºåˆ—åï¼Œç¡®ä¿æ—¶é—´åºåˆ—æ­£ç¡®
        df = df[sorted(df.columns)]

        self.log(f" èšåˆå®Œæˆ: å…± {len(df)} ä¸ª SKU è¿›å…¥é¢„æµ‹æ± ã€‚")
        return df

    def _evaluate_models(self, series: pd.Series, horizon: int = 3) -> Dict[str, float]:
        """
        [æ¨¡å‹è¯„ä¼°] å›æµ‹è¿‡å» N æœŸï¼Œè®¡ç®—å„æ¨¡å‹çš„å‡†ç¡®ç‡ (1 - WMAPE)
        """
        scores = {m.name: 0.0 for m in self.models}

        # æ•°æ®ç‚¹å¤ªå°‘æ— æ³•å›æµ‹
        if len(series) < 6: return scores

        y = pd.to_numeric(series, errors='coerce').fillna(0)

        for model in self.models:
            try:
                abs_errors = []
                actuals = []
                # æ»šåŠ¨å›æµ‹ (Rolling Backtest)
                for i in range(horizon, 0, -1):
                    train = y.iloc[:-i]
                    actual = y.iloc[-i]

                    # é¢„æµ‹ä¸‹ä¸€æ­¥
                    pred = model.fit_predict(train, periods=1)
                    if pred < 0: pred = 0

                    abs_errors.append(abs(actual - pred))
                    actuals.append(actual)

                sum_actual = sum(actuals)
                sum_error = sum(abs_errors)

                if sum_actual == 0:
                    # å¦‚æœå®é™…å€¼å…¨æ˜¯0ï¼Œè¯¯å·®å°åˆ™æ»¡åˆ†ï¼Œå¦åˆ™0åˆ†
                    accuracy = 100.0 if sum_error < 1.0 else 0.0
                else:
                    wmape = sum_error / sum_actual
                    accuracy = max(0, 1.0 - wmape) * 100

                scores[model.name] = round(accuracy, 2)

            except Exception:
                # å•ä¸ªæ¨¡å‹æŠ¥é”™ä¸åº”ä¸­æ–­æµç¨‹ï¼Œè¯¥æ¨¡å‹å¾—åˆ†ä¸º 0
                scores[model.name] = 0.0

        return scores

    def run(self):
        """
        ä¸»æ‰§è¡Œæµç¨‹
        """
        self.log("ğŸš€ å¯åŠ¨ AI é”€é‡é¢„æµ‹å¼•æ“ (Tournament Mode)...")

        # 1. è·å–èšåˆåçš„å†å²é”€é‡çŸ©é˜µ
        df_matrix = self._aggregate_monthly_sales()

        if df_matrix.empty:
            self.log(" é”™è¯¯: å†å²æ•°æ®ä¸è¶³æˆ–ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆé¢„æµ‹ã€‚")
            return

        results = []

        # 2. éå†æ¯ä¸ª SKU è¿›è¡Œé¢„æµ‹ (ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦)
        # total=len(df_matrix) è®©å‰ç«¯ task_monitor èƒ½æ­£ç¡®æ˜¾ç¤ºè¿›åº¦æ¡
        for sku, row in tqdm.tqdm(df_matrix.iterrows(), total=len(df_matrix), desc="æ¨¡å‹ç«å¤‡ä¸­"):
            series = row
            # æœ‰æ•ˆæ•°æ®é•¿åº¦ (é0å€¼çš„æ•°é‡)
            valid_len = np.count_nonzero(series.values)

            # ç­–ç•¥åˆ†æ”¯: æ–°å“ vs è€å“
            is_new_product = valid_len < 6

            if is_new_product:
                # [æ–°å“ç­–ç•¥] ç›´æ¥ä½¿ç”¨åŠ æƒå¹³å‡ï¼Œä¸è·‘å¤æ‚æ¨¡å‹ï¼ŒèŠ‚çœèµ„æº
                wc_model = self.models[-1]  # WeightedCycleForecaster
                best_val = wc_model.fit_predict(series)
                best_algo = "NewProduct_Rule"

                # å¡«å……è®°å½•
                preds = {m.name: 0 for m in self.models}
                preds["WeightedCycle"] = round(best_val, 2)
                confs = {m.name: 0 for m in self.models}
                confs["WeightedCycle"] = 100.0

            else:
                # [è€å“ç­–ç•¥] é”¦æ ‡èµ›æ¨¡å¼ (Tournament)
                preds = {}
                # A. è¿è¡Œæ‰€æœ‰æ¨¡å‹è·å–é¢„æµ‹å€¼
                for m in self.models:
                    try:
                        p = m.fit_predict(series)
                        preds[m.name] = round(p, 2) if p >= 0 else 0.0
                    except Exception:
                        preds[m.name] = 0.0

                # B. å›æµ‹è¯„ä¼°ï¼Œé€‰å‡ºå‡†ç¡®ç‡æœ€é«˜çš„æ¨¡å‹
                confs = self._evaluate_models(series)
                best_algo = max(self.models, key=lambda m: confs.get(m.name, 0)).name
                best_val = preds.get(best_algo, 0.0)

            # 3. æ„é€ ç»“æœè¡Œ
            record = {"SKU": sku}
            # è®°å½•æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹å€¼å’Œå¾—åˆ† (ä¾¿äºåç»­åˆ†æ)
            for m_name in [m.name for m in self.models]:
                record[f"{m_name}_Forecast"] = preds.get(m_name, 0)
                record[f"{m_name}_Score"] = confs.get(m_name, 0)

            record["Best_Algo"] = best_algo
            record["BestForecast"] = best_val
            results.append(record)

        # 4. ä¿å­˜ç»“æœ
        if not results:
            self.log("âš ï¸ é¢„æµ‹ç»“æœé›†ä¸ºç©ºã€‚")
            return

        df_res = pd.DataFrame(results)

        # æ·»åŠ æŠ¥è¡¨ Footer è¯´æ˜
        footer = [
            "ğŸ“˜ AI é¢„æµ‹é€»è¾‘è¯´æ˜ (AI Forecasting Logic):",
            "1. é”¦æ ‡èµ›æœºåˆ¶ (Tournament): å¯¹æ¯ä¸ª SKU å¹¶è¡Œè¿è¡Œ XGBoost, SARIMA, ETS ç­‰ 6 ç§ç®—æ³•ã€‚",
            "2. å† å†›é€‰æ‹© (Champion Selection): åŸºäºè¿‡å» 3 ä¸ªæœˆæ•°æ®çš„ WMAPE (åŠ æƒå¹³å‡è¯¯å·®) è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹ã€‚",
            "3. æ–°å“ç­–ç•¥ (New Product): å†å²æ•°æ® < 6 ä¸ªæœˆçš„ SKUï¼Œè‡ªåŠ¨é™çº§ä¸ºåŠ æƒç§»åŠ¨å¹³å‡ç®—æ³•ã€‚",
            f"4. ç”Ÿæˆæ—¶é—´ (Generated At): {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        ]

        filename = "Estimated_Monthly_SKU.csv"
        self.save_csv(df_res, filename, footer)

        self.log(f" é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {filename}")

==================== END FILE: core/services/prediction.py ====================


==================== START FILE: core/services/database_service.py ====================
# core/services/database_service.py

import os
import subprocess
import datetime
from pathlib import Path
from typing import List, Tuple, Callable, Optional

from config import Config
from core.logging_config import get_logger
from core.repository.db_client import DBClient
from core.context import get_current_user


class DatabaseService:
    """
    [åŸºç¡€æœåŠ¡] æ•°æ®åº“ç”Ÿå‘½å‘¨æœŸç®¡ç† (Database Lifecycle Mgmt) - Enterprise V2.0

    èŒè´£:
    1. ç¾éš¾æ¢å¤: å…¨é‡å¤‡ä»½ (Backup) ä¸ æµå¼è¿˜åŸ (Restore)ã€‚
    2. ç¯å¢ƒé‡ç½®: æä¾›å®‰å…¨çš„ä¸šåŠ¡æ•°æ®æ¸…ç©ºåŠŸèƒ½ (Reset)ã€‚
    3. å®¡è®¡è¿½è¸ª: æ‰€æœ‰é«˜å±æ“ä½œå‡è®°å½•è¯¦ç»†æ—¥å¿—ã€‚
    """

    def __init__(self):
        self.host = Config.DB_HOST
        self.user = Config.DB_USER
        self.password = Config.DB_PASS
        self.db_name = Config.DB_NAME

        self.backup_dir = Config.BACKUP_DIR
        self.backup_dir.mkdir(parents=True, exist_ok=True)

        self.logger = get_logger(self.__class__.__name__)
        self.db = DBClient()

    def list_backups(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨å¤‡ä»½æ–‡ä»¶ (æŒ‰æ—¶é—´å€’åº)"""
        files = sorted(
            [f.name for f in self.backup_dir.glob("*.sql")],
            reverse=True
        )
        return files

    def create_backup(self, tag: str = "") -> Tuple[bool, str]:
        """æ‰§è¡Œæ•°æ®åº“å…¨é‡å¤‡ä»½"""
        user = get_current_user() or "System"
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        tag_part = f"_{tag}" if tag else ""
        filename = f"{timestamp}{tag_part}.sql"
        filepath = self.backup_dir / filename

        # ä½¿ç”¨ mysqldump
        cmd = [
            'mysqldump',
            '-h', self.host,
            '-u', self.user,
            f'-p{self.password}',
            self.db_name
        ]

        try:
            with open(filepath, 'w') as f:
                process = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, text=True)

            if process.returncode == 0:
                size_kb = os.path.getsize(filepath) / 1024
                self.logger.info(f"åˆ›å»ºå¤‡ä»½: {filename} ({size_kb:.1f} KB)",
                                 extra={"action": "DB_BACKUP", "user": user})
                return True, f"å¤‡ä»½æˆåŠŸ! æ–‡ä»¶: {filename} ({size_kb:.1f} KB)"
            else:
                if filepath.exists(): os.remove(filepath)
                err_msg = f"å¤‡ä»½å¤±è´¥ (Code {process.returncode}):\n{process.stderr}"
                self.logger.error(err_msg)
                return False, err_msg
        except Exception as e:
            self.logger.error(f"å¤‡ä»½æ‰§è¡Œå‡ºé”™: {e}")
            return False, f"æ‰§è¡Œå‡ºé”™: {str(e)}"

    def restore_backup_with_progress(self, filename: str,
                                     progress_callback: Optional[Callable[[float], None]] = None) -> Tuple[bool, str]:
        """
        [æ ¸å¿ƒ] æ‰§è¡Œæ•°æ®åº“è¿˜åŸ (æ”¯æŒè¿›åº¦æ¡)
        """
        user = get_current_user() or "System"
        filepath = self.backup_dir / filename
        if not filepath.exists():
            return False, "å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨"

        self.logger.info(f"å¼€å§‹è¿˜åŸæ•°æ®åº“: {filename}", extra={"action": "DB_RESTORE_START", "user": user})

        cmd = [
            'mysql',
            '-h', self.host,
            '-u', self.user,
            f'-p{self.password}',
            self.db_name
        ]

        try:
            file_size = os.path.getsize(filepath)
            bytes_read = 0
            chunk_size = 1024 * 1024  # 1MB

            process = subprocess.Popen(
                cmd,
                stdin=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=False
            )

            with open(filepath, 'rb') as f:
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk: break

                    try:
                        process.stdin.write(chunk)
                        process.stdin.flush()
                    except BrokenPipeError:
                        break

                    bytes_read += len(chunk)
                    if progress_callback and file_size > 0:
                        progress = min(1.0, bytes_read / file_size)
                        progress_callback(progress)

            process.stdin.close()
            process.wait()

            if process.returncode == 0:
                if progress_callback: progress_callback(1.0)
                self.logger.info(f"è¿˜åŸæˆåŠŸ: {filename}", extra={"action": "DB_RESTORE_SUCCESS", "user": user})
                return True, f"æ•°æ®åº“å·²æˆåŠŸè¿˜åŸè‡³å¿«ç…§: {filename}"
            else:
                err = process.stderr.read().decode('utf-8', errors='ignore')
                self.logger.error(f"è¿˜åŸå¤±è´¥: {err}")
                return False, f"è¿˜åŸå¤±è´¥: {err}"

        except Exception as e:
            self.logger.error(f"è¿˜åŸå¼‚å¸¸: {e}")
            return False, f"è¿˜åŸè¿‡ç¨‹å¼‚å¸¸: {str(e)}"

    def delete_backup(self, filename: str) -> Tuple[bool, str]:
        """åˆ é™¤å¤‡ä»½æ–‡ä»¶"""
        user = get_current_user() or "System"
        filepath = self.backup_dir / filename

        if not filepath.exists():
            return False, "æ–‡ä»¶ä¸å­˜åœ¨"

        try:
            os.remove(filepath)
            self.logger.info(f"åˆ é™¤å¤‡ä»½: {filename}", extra={"action": "DB_BACKUP_DELETE", "user": user})
            return True, f"å·²åˆ é™¤: {filename}"
        except Exception as e:
            return False, f"åˆ é™¤å¤±è´¥: {str(e)}"

    def reset_business_data(self) -> Tuple[bool, str]:
        """
        [é«˜å±] é‡ç½®ä¸šåŠ¡æ•°æ® (Clear Test Data)
        ä»…æ¸…ç©ºæµæ°´è¡¨ï¼Œä¿ç•™ç”¨æˆ·è¡¨ (User_Account) å’Œ åŸºç¡€èµ„æ–™ (Data_COGS)ã€‚
        """
        user = get_current_user() or "System"

        # å®šä¹‰è¦æ¸…ç©ºçš„ä¸šåŠ¡è¡¨æ¸…å•
        target_tables = [
            "Data_Transaction",
            "Data_Order_Earning",
            "Data_Clean_Log",
            "Data_Clean_Log_Staging"
        ]
        # æ³¨æ„: Data_Inventory å¦‚æœæ¸…ç©ºä¼šå¯¼è‡´åº“å­˜å½’é›¶ï¼Œé€šå¸¸æµ‹è¯•é‡ç½®ä¹Ÿå¸Œæœ›é‡ç½®åº“å­˜ã€‚
        # å¦‚æœä½ æƒ³ä¿ç•™åº“å­˜ç»“æ„ä½†æ¸…é›¶æ•°é‡ï¼Œé‚£æ˜¯ Update æ“ä½œã€‚
        # è¿™é‡Œæ—¢ç„¶æ˜¯ "Clear Test Data"ï¼Œæˆ‘ä»¬å‡è®¾æ˜¯æ¸…ç©ºæ‰€æœ‰å¯¼å…¥çš„æµæ°´å’Œç»“æœã€‚
        # Inventory è¡¨é€šå¸¸ç”± ETL å¡«å……ï¼Œæ‰€ä»¥ä¹Ÿå¯ä»¥ Truncateã€‚
        # ä½† SKU èµ„æ–™ (COGS) æ˜¯åŸºç¡€æ¡£æ¡ˆï¼Œä¸èƒ½åˆ ã€‚

        target_tables.append("Data_Inventory")

        self.logger.warning(f"æ­£åœ¨æ‰§è¡Œä¸šåŠ¡æ•°æ®é‡ç½®ï¼Œç›®æ ‡è¡¨: {target_tables}",
                            extra={"action": "DB_RESET_START", "user": user})

        try:
            for tbl in target_tables:
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œå­˜åœ¨åˆ™æ¸…ç©º
                check_sql = f"SHOW TABLES LIKE '{tbl}'"
                if not self.db.read_df(check_sql).empty:
                    self.db.truncate_table(tbl)

            self.logger.info("ä¸šåŠ¡æ•°æ®å·²å…¨éƒ¨æ¸…ç©ºã€‚", extra={"action": "DB_RESET_SUCCESS", "user": user})
            return True, " ä¸šåŠ¡æ•°æ®å·²æ¸…ç©º (ç”¨æˆ·ä¸SKUèµ„æ–™å·²ä¿ç•™)ã€‚"
        except Exception as e:
            self.logger.error(f"é‡ç½®å¤±è´¥: {e}")
            return False, f"é‡ç½®å¤±è´¥: {str(e)}"

==================== END FILE: core/services/database_service.py ====================


==================== START FILE: core/services/ordering.py ====================
# core/services/ordering.py

import os
import numpy as np
import pandas as pd
from math import ceil, floor, sqrt
from typing import Tuple
from tqdm import tqdm

from config import Config
from core.base import BaseAnalyzer
from core.repository.sku_repo import SkuRepository


class OrderingService(BaseAnalyzer):
    """
    [ä¸šåŠ¡æœåŠ¡] æ™ºèƒ½è¡¥è´§è®¡ç®—å¼•æ“ (Smart Ordering Engine) - Enterprise V2.0

    æ ¸å¿ƒç®—æ³• (Logic):
    1. éœ€æ±‚é¢„æµ‹: åŸºäº AI é¢„æµ‹æ¨¡å—çš„æœˆåº¦æ¶ˆè€—é‡ã€‚
    2. å®‰å…¨åº“å­˜ (SS): åŸºäºç›®æ ‡æœåŠ¡æ°´å¹³ (Z-Score) å’Œ é¢„ä¼°æ³¢åŠ¨ç‡ã€‚
       å…¬å¼: SS = Z * sqrt(LeadTime) * Volatility
    3. è¡¥è´§ç¼ºå£: ç›®æ ‡åº“å­˜ - (ç°æœ‰åº“å­˜ + åœ¨é€”).
    4. MOQ åœ†æ•´: åŸºäºä¸šåŠ¡è§„åˆ™ (èµ·è®¢é‡) å¯¹ç¼ºå£è¿›è¡Œå–æ•´ã€‚

    ä¼˜åŒ–ç‚¹:
    - ç»“æ„åŒ– MOQ è§„åˆ™ï¼Œä¾¿äºç»´æŠ¤ã€‚
    - å¢å¼ºæ•°æ®åŠ è½½çš„å¥å£®æ€§ã€‚
    - å¼•å…¥è¿›åº¦æ¡åé¦ˆã€‚
    """

    # æœåŠ¡æ°´å¹³å¯¹åº”çš„ Z åˆ†æ•° (æ ‡å‡†æ­£æ€åˆ†å¸ƒ)
    Z_SCORES = {
        0.98: 2.05,  # Aç±»: æé«˜æœåŠ¡æ°´å¹³
        0.95: 1.65,  # Bç±»: æ ‡å‡†æœåŠ¡æ°´å¹³
        0.90: 1.28  # Cç±»: åŸºç¡€æœåŠ¡æ°´å¹³
    }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.sku_repo = SkuRepository()

        # ä¸šåŠ¡å‚æ•°
        self.lead_time = float(Config.LEAD_MONTH)
        self.min_safety = float(Config.MIN_SAFETY_MONTH)

    def _load_data_sources(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        åŠ è½½æ‰€æœ‰ä¾èµ–æ•°æ®æºï¼šé¢„æµ‹è¡¨ã€åº“å­˜è¡¨ã€æˆæœ¬è¡¨ã€‚
        """
        self.log("ğŸ“¥ [Ordering] æ­£åœ¨åŠ è½½æ•°æ®æº (Prediction / Inventory / COGS)...")

        # 1. åŠ è½½é¢„æµ‹æ•°æ® (Prediction)
        pred_path = os.path.join(self.output_dir, "Estimated_Monthly_SKU.csv")
        if not os.path.exists(pred_path):
            self.log(f"âš ï¸ æœªæ‰¾åˆ°é¢„æµ‹æ–‡ä»¶: {pred_path}ã€‚è¯·å…ˆè¿è¡Œ [AI é”€é‡é¢„æµ‹]ã€‚")
            df_pred = pd.DataFrame()
        else:
            try:
                df_pred = pd.read_csv(pred_path)
                # æ¸…æ´— & å½’ä¸€åŒ–
                if "SKU" in df_pred.columns:
                    df_pred = df_pred.dropna(subset=["SKU"])
                    df_pred["SKU"] = df_pred["SKU"].astype(str).str.strip().str.upper()
                    # è¿‡æ»¤å¯èƒ½å­˜åœ¨çš„ Footer æ–‡å­—è¡Œ
                    mask_valid = ~df_pred["SKU"].str.contains("è¯´æ˜|ç”Ÿæˆ|:", regex=True, na=False)
                    df_pred = df_pred[mask_valid]
                    # èšåˆå»é‡
                    df_pred = df_pred.groupby("SKU", as_index=False)["BestForecast"].sum()
                    df_pred.rename(columns={"BestForecast": "é¢„æµ‹æœˆæ¶ˆè€—"}, inplace=True)
                else:
                    df_pred = pd.DataFrame()
            except Exception as e:
                self.log(f" è¯»å–é¢„æµ‹æ–‡ä»¶å¤±è´¥: {e}")
                df_pred = pd.DataFrame()

        # 2. åŠ è½½åº“å­˜ (Inventory)
        df_inv = self.sku_repo.get_inventory_latest()
        if not df_inv.empty:
            df_inv["SKU"] = df_inv["SKU"].astype(str).str.strip().str.upper()
            df_inv["Quantity"] = pd.to_numeric(df_inv["Quantity"], errors='coerce').fillna(0)
            df_inv = df_inv.groupby("SKU", as_index=False)["Quantity"].sum()

        # 3. åŠ è½½æˆæœ¬ (COGS)
        df_cogs = self.sku_repo.get_all_cogs()[["SKU", "Cog"]]
        if not df_cogs.empty:
            df_cogs["SKU"] = df_cogs["SKU"].astype(str).str.strip().str.upper()
            df_cogs["Cog"] = pd.to_numeric(df_cogs["Cog"], errors='coerce').fillna(0)
            # COGS è¡¨å¦‚æœæœ‰é‡å¤ SKUï¼Œå–æœ€å¤§æˆæœ¬ (ä¿å®ˆç­–ç•¥)
            df_cogs = df_cogs.groupby("SKU", as_index=False)["Cog"].max()

        return df_pred, df_inv, df_cogs

    def _calc_abc_classification(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ‰§è¡Œ ABC åˆ†çº§ç®—æ³•
        é€»è¾‘: æŒ‰ (é¢„æµ‹é”€é‡ * æˆæœ¬) é™åºæ’åˆ—ï¼Œç´¯è®¡å æ¯” 80%/95% åˆ‡åˆ†ã€‚
        """
        # ç¡®ä¿æ•°å€¼å®‰å…¨
        df["é¢„æµ‹æœˆæ¶ˆè€—"] = pd.to_numeric(df["é¢„æµ‹æœˆæ¶ˆè€—"], errors='coerce').fillna(0)
        df["Cog"] = pd.to_numeric(df["Cog"], errors='coerce').fillna(0)

        # è®¡ç®—é¢„ä¼°é”€å”®é¢ (GMV Potential)
        df["é¢„ä¼°é”€å”®é¢"] = df["é¢„æµ‹æœˆæ¶ˆè€—"] * df["Cog"]

        # æ’åº
        df = df.sort_values("é¢„ä¼°é”€å”®é¢", ascending=False).reset_index(drop=True)

        total_val = df["é¢„ä¼°é”€å”®é¢"].sum()

        # è¾¹ç•Œæƒ…å†µï¼šæ€»é¢ä¸º0 (å¯èƒ½å…¨æ˜¯æ–°å“æˆ–æ— é”€é‡)
        if total_val <= 0:
            df["ABCç­‰çº§"] = "C"
            df["ç›®æ ‡æœåŠ¡æ°´å¹³"] = 0.90
            df["ç´¯è®¡å æ¯”"] = 0.0
            return df

        # è®¡ç®—ç´¯è®¡å æ¯”
        df["ç´¯è®¡å æ¯”"] = df["é¢„ä¼°é”€å”®é¢"].cumsum() / total_val

        # å‘é‡åŒ–åˆ†çº§ (æ¯” apply æ›´å¿«)
        conditions = [
            (df["ç´¯è®¡å æ¯”"] <= 0.80),
            (df["ç´¯è®¡å æ¯”"] <= 0.95)
        ]
        choices_grade = ["A", "B"]
        choices_sl = [0.98, 0.95]

        df["ABCç­‰çº§"] = np.select(conditions, choices_grade, default="C")
        df["ç›®æ ‡æœåŠ¡æ°´å¹³"] = np.select(conditions, choices_sl, default=0.90)

        return df

    def _get_moq_rule(self, sku: str) -> int:
        """
        [ä¸šåŠ¡è§„åˆ™] è·å– SKU çš„æœ€å°èµ·è®¢é‡ (MOQ)
        TODO: æœªæ¥åº”å°†æ­¤è§„åˆ™è¿ç§»è‡³æ•°æ®åº“ Config è¡¨ä¸­ã€‚
        """
        sku_str = str(sku).strip().upper()
        # ç‰¹å®šå‰ç¼€ 1000ï¼Œå…¶ä»– 100
        special_prefixes = ("NE", "NU", "BE", "BU")
        if sku_str.startswith(special_prefixes):
            return 1000
        return 100

    def _calculate_logic_row(self, row: pd.Series) -> pd.Series:
        """
        [æ ¸å¿ƒé€»è¾‘] å•è¡Œè®¡ç®—å»ºè®®è¡¥è´§é‡ (Apply é€»è¾‘)
        åŒ…å«å¤æ‚çš„åœ†æ•´å’Œå¤‡æ³¨ç”Ÿæˆã€‚
        """
        # 1. åŸºç¡€æ•°æ®
        forecast = float(row["é¢„æµ‹æœˆæ¶ˆè€—"])
        current_inv = float(row["Quantity"])
        sl = float(row["ç›®æ ‡æœåŠ¡æ°´å¹³"])

        # 2. å®‰å…¨åº“å­˜è®¡ç®— (Safety Stock)
        # ç®€åŒ–å‡è®¾ï¼šæ³¢åŠ¨ç‡ä¸ºé”€é‡çš„ 50% (Standard Deviation approx)
        # ä¼ä¸šçº§ä¼˜åŒ–æ–¹å‘ï¼šåº”åŸºäºå†å²é”€é‡è®¡ç®—çœŸå®çš„ std()
        volatility = forecast * 0.5
        z_score = self.Z_SCORES.get(sl, 1.28)

        # å…¬å¼: SS = Z * sqrt(LeadTime) * Volatility
        # ä¸”å¿…é¡»æ»¡è¶³æœ€ä½å®‰å…¨åº“å­˜æœˆæ•° (Min Safety Month)
        ss_statistical = z_score * sqrt(self.lead_time) * volatility
        ss_minimum = self.min_safety * forecast

        safety_stock = max(ss_statistical, ss_minimum)

        # 3. è¡¥è´§ç¼ºå£ (Gap)
        cycle_stock = self.lead_time * forecast  # å‘¨æœŸå†…æ¶ˆè€—
        target_stock = cycle_stock + safety_stock

        # æš‚æ—¶æ²¡æœ‰åœ¨é€”æ•°æ® (In-Transit)ï¼Œé¢„ç•™æ¥å£
        in_transit = 0.0
        net_inventory = current_inv + in_transit

        gap = target_stock - net_inventory

        # 4. MOQ åœ†æ•´ç­–ç•¥
        sku = str(row["SKU"])
        moq = self._get_moq_rule(sku)

        # é»˜è®¤å€¼
        suggest_qty = 0
        note = ""

        # Case A: åº“å­˜å……è¶³
        if gap <= 0:
            suggest_qty = 0
            note = "åº“å­˜å……è¶³"

        # Case B: é”€é‡è¿‡ä½ (åŠå¹´é”€é‡éƒ½ä¸å¤Ÿä¸€ä¸ª MOQ)
        elif (forecast * 6) < moq:
            suggest_qty = 0
            note = f"é”€é‡è¿‡ä½ (åŠå¹´ < MOQ {moq})"

        else:
            # Case C: éœ€è¦è¡¥è´§ï¼Œè®¡ç®—å€æ•°
            # é€»è¾‘ï¼šå¦‚æœç¼ºå£è¶…è¿‡ MOQ çš„ 33%ï¼Œå°±å‘ä¸Šå–æ•´å¤šè®¢ä¸€ç‚¹ï¼›å¦åˆ™å‘ä¸‹å–æ•´ï¼ˆæˆ–è€…æš‚ä¸è®¢ï¼‰
            factor = gap / moq
            remainder = factor - int(factor)

            if remainder >= 0.33:
                rounds = ceil(factor)
            else:
                rounds = floor(factor)

            final_qty = rounds * moq

            if final_qty > 0:
                suggest_qty = final_qty
                note = "å»ºè®®è¡¥è´§"
            else:
                suggest_qty = 0
                note = "ç¼ºå£å¾®å° (æœªè¾¾èµ·è®¢é˜ˆå€¼)"

        # è¿”å›æ‰©å±•çš„ Series
        return pd.Series({
            "å®‰å…¨åº“å­˜": round(safety_stock, 1),
            "ç›®æ ‡åº“å­˜": round(target_stock, 1),
            "ç†è®ºåº“å­˜": round(net_inventory, 1),
            "ç¼ºå£": round(gap, 1),
            "å»ºè®®è®¢è´§": int(suggest_qty),
            "å¤‡æ³¨": note
        })

    def run(self):
        """ä¸»æ‰§è¡Œæµ"""
        self.log(f"ğŸš€ å¯åŠ¨æ™ºèƒ½è¡¥è´§è®¡ç®—å¼•æ“ (LeadTime={self.lead_time}, Safety={self.min_safety})...")

        # 1. å‡†å¤‡æ•°æ®
        df_pred, df_inv, df_cogs = self._load_data_sources()

        if df_pred.empty:
            self.log(" æ— æ³•è¿›è¡Œè®¡ç®—ï¼šé¢„æµ‹æ•°æ®ä¸ºç©ºã€‚è¯·æ£€æŸ¥ä¸Šä¸€æ­¥éª¤ã€‚")
            return

        # 2. åˆå¹¶å®½è¡¨ (Left Join ä»¥é¢„æµ‹è¡¨ä¸ºä¸»)
        self.log("ğŸ”— æ­£åœ¨å…³è”åº“å­˜ä¸æˆæœ¬æ•°æ®...")
        df_main = pd.merge(df_pred, df_inv, on="SKU", how="left")
        df_main = pd.merge(df_main, df_cogs, on="SKU", how="left")

        # å¡«å……ç¼ºå¤±å€¼
        df_main.fillna(0, inplace=True)

        # 3. ä¸šåŠ¡è®¡ç®— - ABC åˆ†çº§
        self.log("ğŸ“Š æ‰§è¡Œ ABC ä»·å€¼åˆ†çº§...")
        df_main = self._calc_abc_classification(df_main)

        # 4. ä¸šåŠ¡è®¡ç®— - è¡¥è´§é‡ (é€è¡Œè®¡ç®—)
        self.log("ğŸ§® è®¡ç®—å®‰å…¨åº“å­˜ä¸è¡¥è´§å»ºè®®...")

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        tqdm.pandas(desc="Computing Logic")
        # å°†è®¡ç®—ç»“æœåˆå¹¶å›åŸè¡¨
        logic_results = df_main.progress_apply(self._calculate_logic_row, axis=1)
        df_final = pd.concat([df_main, logic_results], axis=1)

        # 5. æ ¼å¼åŒ–è¾“å‡º
        output_cols = [
            "SKU", "ABCç­‰çº§", "å»ºè®®è®¢è´§", "å¤‡æ³¨",
            "é¢„æµ‹æœˆæ¶ˆè€—", "ç›®æ ‡æœåŠ¡æ°´å¹³", "å®‰å…¨åº“å­˜",
            "ç›®æ ‡åº“å­˜", "ç†è®ºåº“å­˜", "ç¼ºå£", "Cog"
        ]

        # æ’åºï¼šä¼˜å…ˆå¤„ç† A ç±»ï¼Œä¸”å»ºè®®è®¢è´§é‡å¤§çš„
        df_final = df_final.sort_values(["ABCç­‰çº§", "å»ºè®®è®¢è´§"], ascending=[True, False])

        # ç¡®ä¿åˆ—å­˜åœ¨ (é˜²æ­¢é€»è¾‘è¿”å›ç¼ºå¤±)
        for c in output_cols:
            if c not in df_final.columns:
                df_final[c] = 0

        df_out = df_final[output_cols]

        # 6. ä¿å­˜ç»“æœ
        filename = f"Smart_Ordering_Plan_{self.file_suffix}.csv"
        footer = [
            "ğŸ“˜ æ™ºèƒ½è¡¥è´§é€»è¾‘è¯´æ˜ (Logic Description):",
            f"1. å‚æ•°è®¾å®š: LeadTime={self.lead_time}ä¸ªæœˆ, MinSafety={self.min_safety}ä¸ªæœˆã€‚",
            "2. ABCåˆ†çº§: Aç±»(98%æ»¡è¶³ç‡), Bç±»(95%), Cç±»(90%)ã€‚",
            "3. è®¡ç®—å…¬å¼: ç›®æ ‡åº“å­˜ = (é¢„æµ‹æ¶ˆè€— * LeadTime) + å®‰å…¨åº“å­˜ã€‚",
            "4. ç¼ºå£ = ç›®æ ‡åº“å­˜ - (ç°æœ‰åº“å­˜ + åœ¨é€”)ã€‚",
            "5. åœ†æ•´è§„åˆ™: åŸºäº MOQ (èµ·è®¢é‡) è®¡ç®—ï¼Œç¼ºå£è¶…è¿‡ MOQ çš„ 33% å‘ä¸Šå–æ•´ï¼Œå¦åˆ™å‘ä¸‹ã€‚"
        ]

        self.save_csv(df_out, filename, footer)

==================== END FILE: core/services/ordering.py ====================


==================== START FILE: core/services/profit_listing.py ====================
# core/services/profit_listing.py
import os
import pandas as pd
from collections import defaultdict
import tqdm

from core.services.profit_base import ProfitAnalyzerBase
from core.services.diagnostics.listing import ListingDiagnostician

class ListingProfitAnalyzer(ProfitAnalyzerBase):
    """
    [ä¸šåŠ¡æœåŠ¡] Listing (Item ID) çº§åˆ©æ¶¦åˆ†æå™¨ (V2.3 Normalized)
    """

    def _aggregate(self, df: pd.DataFrame) -> dict:
        metrics = defaultdict(lambda: defaultdict(float))
        if df.empty: return metrics

        records = df.to_dict('records')
        for row in tqdm.tqdm(records, desc="èšåˆ Listing æ•°æ®"):
            raw_id = str(row.get("item id", ""))
            # [æ ¸å¿ƒä¿®å¤] ç§»é™¤ .0 åç¼€å¹¶å»ç©ºæ ¼
            item_id = raw_id.strip().replace(".0", "")
            if not item_id or item_id == '0': continue

            if "title" not in metrics[item_id]:
                metrics[item_id]["title"] = str(row.get("item title", "")).strip()

            qty_sets = int(float(row.get("quantity", 0)))
            action = str(row.get("action", "")).strip().upper()
            revenue = float(row.get("revenue", 0))
            refund = float(row.get("Refund", 0))

            metrics[item_id]["total_qty"] += qty_sets
            metrics[item_id]["total_rev"] += revenue

            if action == "CA": metrics[item_id]["cancel_qty"] += qty_sets; metrics[item_id]["cancel_rev"] += refund
            elif action == "RE": metrics[item_id]["return_qty"] += qty_sets; metrics[item_id]["return_rev"] += refund
            elif action == "CR": metrics[item_id]["request_qty"] += qty_sets; metrics[item_id]["request_rev"] += refund
            elif action == "CC": metrics[item_id]["claim_qty"] += qty_sets; metrics[item_id]["claim_rev"] += refund
            elif action == "PD": metrics[item_id]["dispute_qty"] += qty_sets; metrics[item_id]["dispute_rev"] += refund

            row_cost = 0.0
            for i in range(1, 21):
                s_key, q_key = f"sku{i}", f"qty{i}"
                if s_key not in row: break

                # è¿™é‡Œçš„ SKU ä¹Ÿéœ€è¦å½’ä¸€åŒ–ä»¥åŒ¹é…æˆæœ¬è¡¨
                raw_sku = str(row[s_key])
                if not raw_sku or raw_sku == '0': continue
                sku = raw_sku.strip().upper()

                try:
                    q_per = float(row[q_key])
                except:
                    q_per = 0

                unit_cost = self.sku_cost_map.get(sku, 0.0)
                row_cost += (unit_cost * q_per * qty_sets)

                if sku in ["NU1C8E51C", "NU1C8E51K"]:
                    extra_cost = self.sku_cost_map.get("NU1C8SKT7", 0.0)
                    row_cost += (extra_cost * 2 * qty_sets)

            metrics[item_id]["cog_value"] += -row_cost
            self._accumulate_fees(row, metrics, item_id, weight=1.0)

        return metrics

    def run(self):
        self._load_basics()
        if self.df_cur is None or self.df_cur.empty:
            self.log("âš ï¸ æœ¬æœŸæ— æ•°æ®ï¼Œæ— æ³•åˆ†æ")
            return
        self.log(f"ğŸ“Š å·²åŠ è½½åŸå§‹è®°å½•: {len(self.df_cur)} æ¡")
        self.log("æ­£åœ¨èšåˆæœ¬æœŸæ•°æ®...")
        m_cur = self._calculate_net_profit(self._aggregate(self.df_cur))
        self.log("æ­£åœ¨èšåˆä¸ŠæœŸæ•°æ®(ç”¨äºç¯æ¯”)...")
        m_prev = self._calculate_net_profit(self._aggregate(self.df_prev))

        tables = self.generate_full_report_suite(m_cur, m_prev, key_name="Item ID")
        self.log("æ­£åœ¨æ‰§è¡Œ AI æ™ºèƒ½è¯Šæ–­...")
        diag = ListingDiagnostician(m_cur, m_prev)
        df_diag = diag.diagnose()
        tables.append(("C1_æ™ºèƒ½è¯Šæ–­è¡¨ (AI Diagnostics)", df_diag))
        explanation_lines = diag.get_tag_definitions()

        filename = f"Profit_Analysis_Listing_{self.file_suffix}.csv"
        save_path = os.path.join(self.output_dir, filename)
        try:
            with open(save_path, "w", encoding="utf-8-sig") as f:
                for name, df in tables:
                    f.write(f"=== {name} ===\n")
                    df.to_csv(f, index=False)
                    f.write("\n\n")
                f.write("\n")
                for line in explanation_lines:
                    f.write(f"{line}\n")
            self.log(f" Listing åˆ©æ¶¦æŠ¥è¡¨å·²ç”Ÿæˆ: {filename}")
        except Exception as e:
            self.log(f" ä¿å­˜å¤±è´¥: {e}")

==================== END FILE: core/services/profit_listing.py ====================


==================== START FILE: core/services/data_manager.py ====================
# core/services/data_manager.py

import pandas as pd
from sqlalchemy import text
from typing import List, Dict, Any, Tuple
import json
import math

from config import Config
from core.repository.db_client import DBClient
from core.logging_config import get_logger
from core.context import get_current_user


class DataManager:
    """
    [ä¸šåŠ¡æœåŠ¡] æ•°æ®ä¿®æ”¹ç®¡ç†å™¨ (Data Modification Service) - Enterprise V2.3

    Fix Log:
    - V2.3: ä¿®å¤ update_cogs_batch ä¸­å›  Data_COGS å­˜åœ¨é‡å¤ SKU å¯¼è‡´ to_dict('index') æŠ¥é”™çš„é—®é¢˜ã€‚
            å¢åŠ å¼ºåˆ¶å»é‡é€»è¾‘ (drop_duplicates)ã€‚
    """

    def __init__(self):
        self.db = DBClient()
        self.logger = get_logger(self.__class__.__name__)

    # =========================================================================
    # è¾…åŠ©å·¥å…·: å·®å¼‚æ£€æµ‹
    # =========================================================================
    def _is_diff(self, val1: Any, val2: Any) -> bool:
        """
        [å†…éƒ¨å·¥å…·] åˆ¤æ–­ä¸¤ä¸ªå€¼æ˜¯å¦å®è´¨ä¸åŒ
        å¤„ç†æµ®ç‚¹æ•°ç²¾åº¦ã€å­—ç¬¦ä¸²ç©ºæ ¼ã€None/NaN ç­‰æƒ…å†µ
        """
        # 1. ç»Ÿä¸€è½¬å­—ç¬¦ä¸²æ¯”è¾ƒ (æœ€ç¨³å¦¥)
        s1 = str(val1).strip() if val1 is not None else ""
        s2 = str(val2).strip() if val2 is not None else ""

        # æµ®ç‚¹æ•°ç‰¹æ®Šå¤„ç† (å¿½ç•¥ .00 çš„å·®å¼‚)
        try:
            f1 = float(s1)
            f2 = float(s2)
            return not math.isclose(f1, f2, rel_tol=1e-9)
        except:
            # éæ•°å­—ï¼Œç›´æ¥æ¯”å­—ç¬¦ä¸²
            return s1 != s2

    def _format_change_log(self, sku: str, changes: Dict[str, Tuple[Any, Any]]) -> str:
        """
        æ ¼å¼åŒ–å•æ¡å˜æ›´æ—¥å¿—: SKU: Field(Old->New), ...
        """
        details = []
        for field, (old, new) in changes.items():
            details.append(f"{field}({old} -> {new})")
        return f"[{sku}] å˜æ›´: " + ", ".join(details)

    # =========================================================================
    # åº“å­˜ä¿®æ”¹ç›¸å…³ (Inventory)
    # =========================================================================

    def get_inventory_columns(self) -> List[str]:
        """è·å– Data_Inventory çš„æ‰€æœ‰æ—¥æœŸåˆ—"""
        sql = "SELECT * FROM Data_Inventory LIMIT 0"
        df = self.db.read_df(sql)
        cols = [c for c in df.columns if c.lower() != 'sku']
        return sorted(cols, reverse=True)

    def get_all_skus(self) -> List[str]:
        """è·å–æ‰€æœ‰ SKU åˆ—è¡¨"""
        sql = "SELECT DISTINCT SKU FROM Data_Inventory ORDER BY SKU ASC"
        df = self.db.read_df(sql)
        return df['SKU'].tolist()

    def get_inventory_value(self, date_col: str, sku: str) -> int:
        """è·å–æŒ‡å®šæ—¥æœŸå’Œ SKU çš„åº“å­˜æ•°é‡"""
        valid_cols = self.get_inventory_columns()
        if date_col not in valid_cols:
            self.logger.warning(f"éæ³•åˆ—åè¯·æ±‚: {date_col}")
            return 0

        sql = f"SELECT `{date_col}` FROM Data_Inventory WHERE SKU = :sku"
        df = self.db.read_df(sql, {"sku": sku})
        if df.empty:
            return 0
        return int(df.iloc[0, 0])

    def update_inventory_qty(self, date_col: str, sku: str, new_qty: int) -> Tuple[bool, str]:
        """
        æ›´æ–°åº“å­˜æ•°é‡ (è¯¦ç»†å®¡è®¡)
        """
        valid_cols = self.get_inventory_columns()
        if date_col not in valid_cols:
            return False, f"éæ³•åˆ—å: {date_col}"

        user = get_current_user() or "Unknown"

        # 1. è·å–æ—§å€¼ (Before)
        old_val = self.get_inventory_value(date_col, sku)

        # å¦‚æœå€¼æ²¡å˜ï¼Œç›´æ¥è¿”å›ï¼Œä¸è®°å½•æ—¥å¿—
        if old_val == new_qty:
            return True, "å€¼æœªå˜åŒ–ï¼Œæ— éœ€æ›´æ–°ã€‚"

        try:
            # 2. æ‰§è¡Œæ›´æ–°
            sql = f"UPDATE Data_Inventory SET `{date_col}` = :qty WHERE SKU = :sku"
            self.db.execute_stmt(sql, {"qty": new_qty, "sku": sku})

            # 3. è¯¦ç»†æ—¥å¿— (After)
            msg = f"åº“å­˜è°ƒæ•´: SKU[{sku}] æœˆä»½[{date_col}] æ•°é‡ç”± {old_val} æ”¹ä¸º {new_qty}"
            self.logger.info(msg, extra={"action": "UPDATE_INVENTORY", "user": user})

            return True, f" ä¿®æ”¹æˆåŠŸ: {old_val} â {new_qty}"

        except Exception as e:
            err_msg = f"æ›´æ–°å¤±è´¥: {str(e)}"
            self.logger.error(err_msg)
            return False, err_msg

    # =========================================================================
    # èµ„æ–™ä¿®æ”¹ç›¸å…³ (COGS)
    # =========================================================================

    def get_cogs_data(self) -> pd.DataFrame:
        """è·å– Data_COGS å…¨è¡¨"""
        return self.db.read_df("SELECT * FROM Data_COGS")

    def get_distinct_values(self, col: str) -> List[str]:
        """è·å–æŸåˆ—çš„å”¯ä¸€å€¼"""
        valid_cols = ['Category', 'SubCategory', 'Type']
        if col not in valid_cols: return []

        sql = f"SELECT DISTINCT `{col}` FROM Data_COGS WHERE `{col}` IS NOT NULL AND `{col}` != '' ORDER BY `{col}`"
        df = self.db.read_df(sql)
        return df[col].tolist()

    def update_cogs_batch(self, df_changes: pd.DataFrame) -> Tuple[bool, str]:
        """
        æ‰¹é‡æ›´æ–° COGS (æ˜¾å¾®é•œçº§è¯¦ç»†å®¡è®¡)
        """
        user = get_current_user() or "Unknown"

        try:
            # 1. é¢„å¤„ç†æ–°æ•°æ®
            df_changes['Cost'] = pd.to_numeric(df_changes['Cost'], errors='coerce').fillna(0)
            df_changes['Freight'] = pd.to_numeric(df_changes['Freight'], errors='coerce').fillna(0)
            df_changes['Cog'] = (df_changes['Cost'] + df_changes['Freight']).round(2)

            # 2. è·å–æ•°æ®åº“å½“å‰æ•°æ® (Old Snapshot)
            skus_to_update = df_changes['SKU'].tolist()
            if not skus_to_update:
                return True, "æ²¡æœ‰æ•°æ®éœ€è¦æ›´æ–°"

            # æ„é€ æŸ¥è¯¢
            df_old = self.get_cogs_data()

            # [æ ¸å¿ƒä¿®å¤] å¯¹æ—§æ•°æ®è¿›è¡Œå¼ºåˆ¶å»é‡ï¼Œé˜²æ­¢ to_dict('index') æŠ¥é”™
            if not df_old.empty:
                # å½’ä¸€åŒ– SKU
                df_old['SKU'] = df_old['SKU'].astype(str).str.strip().str.upper()
                # æ£€æŸ¥é‡å¤å¹¶å»é‡ (ä¿ç•™ç¬¬ä¸€æ¡)
                if df_old['SKU'].duplicated().any():
                    self.logger.warning("æ£€æµ‹åˆ° Data_COGS å­˜åœ¨é‡å¤ SKUï¼Œæ‰§è¡Œè‡ªåŠ¨å»é‡ä»¥è¿›è¡Œå®¡è®¡å¯¹æ¯”ã€‚")
                    df_old = df_old.drop_duplicates(subset=['SKU'], keep='first')

            # è½¬ä¸ºå­—å…¸ä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾: {sku: {col: val}}
            old_map = df_old.set_index('SKU').to_dict('index')

            updated_count = 0
            change_logs = []  # æ”¶é›†æ‰€æœ‰å˜æ›´æ˜ç»†

            with self.db.get_engine().begin() as conn:
                for _, row in df_changes.iterrows():
                    sku = str(row['SKU']).strip().upper()
                    new_data = row.to_dict()

                    old_data = old_map.get(sku, {})

                    # 3. é€å­—æ®µæ¯”å¯¹å·®å¼‚
                    diffs = {}  # {field: (old, new)}
                    check_cols = ['Category', 'SubCategory', 'Type', 'Cost', 'Freight']

                    for col in check_cols:
                        old_v = old_data.get(col, "")
                        new_v = new_data.get(col, "")

                        if self._is_diff(old_v, new_v):
                            diffs[col] = (old_v, new_v)

                    # å¦‚æœæœ‰å·®å¼‚ï¼Œæ‰æ‰§è¡Œæ›´æ–°å¹¶è®°å½•
                    if diffs:
                        sql = text("""
                                   UPDATE Data_COGS
                                   SET Category=:cat,
                                       SubCategory=:sub,
                                       Type=:type,
                                       Cost=:cost,
                                       Freight=:freight,
                                       Cog=:cog
                                   WHERE SKU = :sku
                                   """)
                        conn.execute(sql, {
                            "cat": row.get('Category', ''), "sub": row.get('SubCategory', ''),
                            "type": row.get('Type', ''), "cost": row['Cost'],
                            "freight": row['Freight'], "cog": row['Cog'], "sku": sku
                        })
                        updated_count += 1

                        # ç”Ÿæˆå•æ¡å˜æ›´æ—¥å¿—
                        change_logs.append(self._format_change_log(sku, diffs))

            # 4. ç”Ÿæˆèšåˆå®¡è®¡æ—¥å¿—
            if updated_count > 0:
                # å¦‚æœå˜æ›´å°‘äº 10 æ¡ï¼Œç›´æ¥æ‹¼æ¥åœ¨æ—¥å¿—é‡Œ
                if len(change_logs) <= 10:
                    detail_msg = " | ".join(change_logs)
                else:
                    # å¦‚æœå˜æ›´å¤ªå¤šï¼Œåªæ˜¾ç¤ºå‰ 5 æ¡ + ç»Ÿè®¡
                    detail_msg = " | ".join(change_logs[:5]) + f" ... (å…± {updated_count} æ¡)"

                full_msg = f"æ‰¹é‡ä¿®æ”¹èµ„æ–™: æ›´æ–° {updated_count} è¡Œã€‚è¯¦æƒ…: {detail_msg}"
                self.logger.info(full_msg, extra={"action": "BATCH_UPDATE_COGS", "user": user})

                return True, f" æˆåŠŸæ›´æ–° {updated_count} æ¡æ•°æ®ï¼Œå·®å¼‚å·²è®°å½•ã€‚"
            else:
                return True, "âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•å®è´¨æ€§æ•°æ®å˜æ›´ã€‚"

        except Exception as e:
            self.logger.error(f"COGS ä¿å­˜å¤±è´¥: {e}")
            return False, f"ä¿å­˜å¤±è´¥: {str(e)}"

    def add_new_sku(self, sku_data: Dict[str, Any]) -> Tuple[bool, str]:
        """
        æ–°å¢ SKU (å®Œæ•´å‚æ•°å®¡è®¡)
        """
        user = get_current_user() or "Unknown"
        sku = str(sku_data.get('SKU', '')).strip().upper()

        if not sku: return False, "SKU ä¸èƒ½ä¸ºç©º"

        check = self.db.read_df("SELECT SKU FROM Data_COGS WHERE SKU = :sku", {"sku": sku})
        if not check.empty: return False, f"SKU {sku} å·²å­˜åœ¨ï¼"

        try:
            cost = float(sku_data.get('Cost', 0))
            freight = float(sku_data.get('Freight', 0))
            sku_data['Cog'] = round(cost + freight, 2)
            sku_data['SKU'] = sku
        except ValueError:
            return False, "Cost æˆ– Freight å¿…é¡»ä¸ºæ•°å­—"

        try:
            with self.db.get_engine().begin() as conn:
                cols = ", ".join(sku_data.keys())
                params_str = ", ".join([f":{k}" for k in sku_data.keys()])
                conn.execute(text(f"INSERT INTO Data_COGS ({cols}) VALUES ({params_str})"), sku_data)

                inv_cols_df = pd.read_sql("SELECT * FROM Data_Inventory LIMIT 0", conn)
                inv_columns = inv_cols_df.columns.tolist()
                inv_vals = [f"'{sku}'" if col.upper() == 'SKU' else "0" for col in inv_columns]

                col_str = ", ".join([f"`{c}`" for c in inv_columns])
                val_str = ", ".join(inv_vals)
                conn.execute(text(f"INSERT INTO Data_Inventory ({col_str}) VALUES ({val_str})"))

            # [å…³é”®] è®°å½•è¯¦ç»†å‚æ•°ï¼Œæ ¼å¼åŒ–ä¸ºæ˜“è¯»å­—ç¬¦ä¸²
            details = ", ".join([f"{k}={v}" for k, v in sku_data.items()])
            self.logger.info(f"æ–°å»ºæ¡£æ¡ˆ: SKU[{sku}] è¯¦æƒ…: {{{details}}}", extra={"action": "CREATE_SKU", "user": user})

            return True, f" SKU {sku} åˆ›å»ºæˆåŠŸï¼"

        except Exception as e:
            self.logger.error(f"SKU åˆ›å»ºå¤±è´¥: {e}")
            return False, f"åˆ›å»ºå¤±è´¥: {str(e)}"

==================== END FILE: core/services/data_manager.py ====================


==================== START FILE: core/services/correction_service.py ====================
# core/services/correction_service.py

import pandas as pd
import difflib
from typing import List, Optional, Tuple, Dict, Any

from config import Config
from core.repository.db_client import DBClient
from core.repository.sku_repo import SkuRepository
from core.logging_config import get_logger
from core.context import get_current_user


class CorrectionService:
    """
    [æ ¸å¿ƒæœåŠ¡] SKU çº é”™ã€è®°å¿†ä¸æ¸…æ´—æœåŠ¡ (Correction & Cleaning Service) - Enterprise V2.1

    èŒè´£:
    1. è®°å¿†åº“ç®¡ç†: è®°å½•ç”¨æˆ·çš„ä¿®æ­£æ“ä½œ (Learn)ã€‚
    2. æ™ºèƒ½æ¨è: æä¾›æ¨¡ç³Šæœç´¢å’Œè‡ªåŠ¨åŒ¹é…å»ºè®® (Suggest)ã€‚
    3. æ•°æ®æ¸…æ´—: æä¾›å¯¹ Transaction è¡¨ä¸­è„æ•°æ®çš„è¯»å†™æ¥å£ (Clean)ã€‚
    4. æµç¨‹ç¼–æ’: å°è£… ETL Parser è°ƒç”¨ï¼Œé¿å… UI å±‚ç›´æ¥ä¾èµ– ETLã€‚
    """

    def __init__(self):
        self.db = DBClient()
        self.logger = get_logger(self.__class__.__name__)

        # è®°å¿†æ–‡ä»¶è·¯å¾„
        self.memory_dir = Config.BASE_DIR / "core" / "memory"
        self.memory_file = self.memory_dir / "sku_correction_memory.csv"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not self.memory_dir.exists():
            self.memory_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–ä»“åº“
        self.sku_repo = SkuRepository()
        self.valid_skus = set(self.sku_repo.get_valid_skus())

        # åŠ è½½è®°å¿†åº“
        self.memory_df = self._load_memory()

    def _load_memory(self) -> pd.DataFrame:
        if self.memory_file.exists():
            try:
                return pd.read_csv(self.memory_file, dtype=str).fillna("")
            except Exception:
                return pd.DataFrame(columns=["CustomLabel", "BadSKU", "BadQty", "CorrectSKU", "CorrectQty"])
        else:
            return pd.DataFrame(columns=["CustomLabel", "BadSKU", "BadQty", "CorrectSKU", "CorrectQty"])

    # =========================================================================
    # 1. è®°å¿†ä¸æ¨è (Knowledge Base)
    # =========================================================================

    def save_correction_memory(self, custom_label: str, bad_sku: str, bad_qty: str,
                               correct_sku: str, correct_qty: str):
        """è®°å½•ç”¨æˆ·çš„ä¿®æ­£æ“ä½œåˆ° CSV (å­¦ä¹ )"""
        new_row = {
            "CustomLabel": str(custom_label).strip(),
            "BadSKU": str(bad_sku).strip().upper(),
            "BadQty": str(bad_qty).strip(),
            "CorrectSKU": str(correct_sku).strip().upper(),
            "CorrectQty": str(correct_qty).strip()
        }

        # è¿½åŠ å¹¶å»é‡ (ä¿ç•™æœ€æ–°çš„)
        self.memory_df = pd.concat([self.memory_df, pd.DataFrame([new_row])], ignore_index=True)
        self.memory_df.drop_duplicates(subset=["CustomLabel", "BadSKU"], keep='last', inplace=True)

        # å†™å…¥æ–‡ä»¶
        try:
            self.memory_df.to_csv(self.memory_file, index=False, encoding='utf-8-sig')
        except Exception as e:
            self.logger.error(f"è®°å¿†åº“ä¿å­˜å¤±è´¥: {e}")

        # åˆ·æ–°å†…å­˜ä¸­çš„æœ‰æ•ˆSKU
        if correct_sku not in self.valid_skus:
            self.valid_skus.add(correct_sku)

    def find_auto_fix(self, custom_label: str, bad_sku: str) -> Tuple[Optional[str], Optional[str]]:
        """ä»è®°å¿†åº“ä¸­å¯»æ‰¾è§£å†³æ–¹æ¡ˆ"""
        if self.memory_df.empty: return None, None

        custom_label = str(custom_label).strip()
        bad_sku = str(bad_sku).strip().upper()

        match = self.memory_df[
            (self.memory_df["CustomLabel"] == custom_label) &
            (self.memory_df["BadSKU"] == bad_sku)
            ]

        if not match.empty:
            row = match.iloc[-1]
            return row["CorrectSKU"], row["CorrectQty"]
        return None, None

    def get_fuzzy_suggestions(self, bad_sku: str, n: int = 5) -> List[str]:
        """æ¨¡ç³Šæœç´¢å»ºè®®"""
        bad_sku = str(bad_sku).upper()
        contains = [s for s in self.valid_skus if bad_sku in s]
        fuzzy = difflib.get_close_matches(bad_sku, self.valid_skus, n=n, cutoff=0.4)
        results = sorted(list(set(contains + fuzzy)))
        return results[:n]

    def is_valid_sku(self, sku: str) -> bool:
        """æ ¡éªŒ SKU æ˜¯å¦å­˜åœ¨"""
        return str(sku).strip().upper() in self.valid_skus

    def validate_quantity(self, val: str) -> bool:
        """éªŒè¯æ•°é‡æ˜¯å¦åˆæ³• (UI è¾…åŠ©)"""
        try:
            v = float(val)
            return v > 0 and v.is_integer()
        except:
            return False

    # =========================================================================
    # 2. æ•°æ®åº“äº¤äº’ä¸æµç¨‹ç¼–æ’ (Database & Orchestration)
    # =========================================================================

    def run_auto_parser(self) -> Dict[str, Any]:
        """
        [ç¼–æ’] è¿è¡Œ ETL Parser è¿›è¡Œè‡ªåŠ¨æ‰«æä¸ä¿®å¤
        æ³¨æ„ï¼šå±€éƒ¨å¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ– (Parser -> CorrectionService -> Parser)
        """
        from core.etl.parser import TransactionParser

        self.logger.info("å¯åŠ¨è‡ªåŠ¨è§£æä¸ä¿®å¤æµç¨‹...")
        parser = TransactionParser()
        return parser.run()

    def get_next_pending_issue(self) -> Optional[pd.Series]:
        """
        è·å–ä¸‹ä¸€æ¡å¾…å¤„ç†çš„å¼‚å¸¸è®°å½•
        P_Flag = 99 è¡¨ç¤ºè§£ææˆ–æ ¡éªŒæœªé€šè¿‡
        """
        sql = "SELECT * FROM Data_Transaction WHERE P_Flag = 99 LIMIT 1"
        df = self.db.read_df(sql)
        if df.empty:
            return None
        return df.iloc[0]

    def apply_fix(self, order_id: str, col_idx: int,
                  custom_label: str, bad_sku: str, bad_qty: str,
                  new_sku: str, new_qty: str) -> bool:
        """
        [åŸå­æ“ä½œ] åº”ç”¨ä¿®å¤ï¼šæ›´æ–°æ•°æ®åº“ + ä¿å­˜è®°å¿†
        """
        user = get_current_user() or "Unknown"

        # 1. ä¿å­˜è®°å¿†
        self.save_correction_memory(custom_label, bad_sku, bad_qty, new_sku, new_qty)

        # 2. æ›´æ–°æ•°æ®åº“
        try:
            sku_col = f"P_SKU{col_idx}"
            qty_col = f"P_Quantity{col_idx}"

            # ä»…æ›´æ–°å€¼ï¼Œä¸æ›´æ–° Flagã€‚
            # Parser ä¼šåœ¨ä¸‹ä¸€æ¬¡è¿è¡Œæ—¶è‡ªåŠ¨æ ¡éªŒé€šè¿‡å¹¶æ›´æ–° Flag=5ã€‚
            sql = f"""
            UPDATE Data_Transaction 
            SET `{sku_col}` = :ns, `{qty_col}` = :nq
            WHERE `Order number` = :oid
            """
            self.db.execute_stmt(sql, {"ns": new_sku, "nq": new_qty, "oid": order_id})

            self.logger.info(f"äººå·¥ä¿®å¤: Order[{order_id}] {bad_sku}->{new_sku}",
                             extra={"action": "MANUAL_FIX_SKU", "user": user})
            return True
        except Exception as e:
            self.logger.error(f"ä¿®å¤æäº¤å¤±è´¥: {e}")
            return False

    def mark_as_skipped(self, order_id: str) -> None:
        """
        è·³è¿‡æŸæ¡è®°å½• (æ ‡è®°ä¸º P_Flag=5ï¼Œå¼ºè¡Œé€šè¿‡ï¼Œé˜²æ­¢å¡æ­»)
        """
        user = get_current_user() or "Unknown"
        sql = "UPDATE Data_Transaction SET P_Flag = 5 WHERE `Order number` = :oid"
        self.db.execute_stmt(sql, {"oid": order_id})
        self.logger.warning(f"è·³è¿‡å¼‚å¸¸è®°å½•: Order[{order_id}]", extra={"action": "SKIP_FIX", "user": user})
==================== END FILE: core/services/correction_service.py ====================


==================== START FILE: core/services/profit_base.py ====================
# core/services/profit_base.py

import os
import pandas as pd
from collections import defaultdict
from abc import abstractmethod
from typing import List, Tuple, Dict

# å¼•å…¥åŸºç±»ä¸ä»“åº“
from core.base import BaseAnalyzer
from core.repository.transaction_repo import TransactionRepository
from core.repository.sku_repo import SkuRepository
from config import Config


class ProfitAnalyzerBase(BaseAnalyzer):
    """
    [æ ¸å¿ƒä¸šåŠ¡åŸºç±»] åˆ©æ¶¦åˆ†æé€šç”¨æ¨¡æ¿ (V2.3 Diagnostic)

    Add: å¢åŠ ä¸ŠæœŸæ•°æ®åŠ è½½é‡ç›‘æ§å’Œ Key åŒ¹é…åº¦è¯Šæ–­æ—¥å¿—ã€‚
    """

    FEE_COLUMNS = [
        "Shipping and handling", "Refund Shipping and handling",
        "Seller collected tax", "eBay collected tax",
        "Refund Seller collected tax", "Refund eBay collected tax",
        "Final Value Fee - fixed", "Final Value Fee - variable",
        "Regulatory operating fee", "International fee",
        "Charity donation", "Deposit processing fee",
        "Refund Final Value Fee - fixed", "Refund Final Value Fee - variable",
        "Refund Regulatory operating fee", "Refund International fee",
        "Refund Charity donation", "Refund Deposit processing fee",
        'Very high "item not as described" fee', 'Refund Very high "item not as described" fee',
        "Below standard performance fee", "Refund Below standard performance fee",
        "Payments dispute fee", "Promoted Listings fee", "Refund Promoted Listings fee",
        "Shipping label-Earning data", "Shipping label-Return"
    ]

    PLATFORM_FEE_GROUP = {
        "Final Value Fee - fixed", "Final Value Fee - variable",
        "Regulatory operating fee", "International fee",
        "Charity donation", "Deposit processing fee",
        "Refund Final Value Fee - fixed", "Refund Final Value Fee - variable",
        "Refund Regulatory operating fee", "Refund International fee",
        "Refund Charity donation", "Refund Deposit processing fee"
    }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.trans_repo = TransactionRepository()
        self.sku_repo = SkuRepository()
        self.sku_cost_map = {}
        self.df_cur = pd.DataFrame()
        self.df_prev = pd.DataFrame()

    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        if df.empty: return df
        exclude_cols = {"action", "order date", "seller", "order number", "item id", "item title", "buyer username",
                        "full sku"}
        df = df.copy()
        for col in df.columns:
            if str(col).lower().startswith("sku"): continue
            if col in exclude_cols: continue
            try:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
            except:
                pass
        return df

    def _load_basics(self):
        """åŠ è½½å¹¶æ¸…æ´—åŸºç¡€æ•°æ®"""
        # 1. åŠ è½½ SKU æˆæœ¬
        self.log("æ­£åœ¨åŠ è½½ SKU æˆæœ¬è¡¨ (Data_COGS)...")
        df_cogs = self.sku_repo.get_all_cogs()
        # ç¡®ä¿ SKU æˆæœ¬æ˜ å°„ Key ä¹Ÿæ˜¯å¤§å†™å»ç©ºæ ¼ï¼Œé˜²æ­¢åŒ¹é…å¤±è´¥
        self.sku_cost_map = {
            str(k).strip().upper(): pd.to_numeric(v, errors='coerce')
            for k, v in zip(df_cogs["SKU"], df_cogs["Cog"])
        }

        # 2. åŠ è½½æœ¬æœŸæ•°æ®
        self.log(f"æ­£åœ¨åŠ è½½å¹¶æ¸…æ´—æœ¬æœŸæ•°æ®: {self.start_date} -> {self.end_date}")
        self.df_cur = self._clean_data(self.trans_repo.get_transactions_by_date(self.start_date, self.end_date))

        # 3. è®¡ç®—ä¸ŠæœŸæ—¶é—´
        delta = self.end_date - self.start_date
        prev_end = self.start_date - pd.Timedelta(days=1)
        prev_start = prev_end - delta

        # 4. åŠ è½½ä¸ŠæœŸæ•°æ® (å¢åŠ è°ƒè¯•æ—¥å¿—)
        self.log(f"æ­£åœ¨åŠ è½½å¹¶æ¸…æ´—ä¸ŠæœŸæ•°æ®: {prev_start} -> {prev_end}")
        self.df_prev = self._clean_data(self.trans_repo.get_transactions_by_date(prev_start, prev_end))

        if self.df_prev.empty:
            self.log("âš ï¸ [è¯Šæ–­] ä¸ŠæœŸæ•°æ®åŠ è½½ä¸ºç©ºï¼è¿™ç›´æ¥å¯¼è‡´ç¯æ¯”æ— æ³•è®¡ç®—ã€‚è¯·æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨è¯¥æ—¥æœŸèŒƒå›´çš„æ•°æ®ã€‚")
        else:
            self.log(f" [è¯Šæ–­] ä¸ŠæœŸæ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(self.df_prev)} è¡Œã€‚")

    def _accumulate_fees(self, row: pd.Series, metrics_dict: dict, key: str, weight: float = 1.0):
        for col in self.FEE_COLUMNS:
            val = float(row.get(col, 0))
            if val != 0: metrics_dict[key][col] += val * weight

    def _calculate_net_profit(self, metrics: dict) -> dict:
        R = Config.LOSS_RATES
        for k, m in metrics.items():
            m["net_qty"] = (
                    m["total_qty"] - m.get("cancel_qty", 0) - m.get("return_qty", 0) * R.get('RETURN', 0.3) - m.get(
                "request_qty", 0) * R.get('REQUEST', 0.5) - m.get("claim_qty", 0) * R.get('CASE', 0.6) - m.get(
                "dispute_qty", 0) * R.get('DISPUTE', 1.0))
            m["net_rev"] = (m["total_rev"] + m.get("cancel_rev", 0) + m.get("return_rev", 0) + m.get("request_rev",
                                                                                                     0) + m.get(
                "claim_rev", 0) + m.get("dispute_rev", 0))
            m["net_shipping"] = m.get("Shipping and handling", 0) + m.get("Refund Shipping and handling", 0)
            m["net_tax"] = (m.get("Seller collected tax", 0) + m.get("eBay collected tax", 0) + m.get(
                "Refund Seller collected tax", 0) + m.get("Refund eBay collected tax", 0))
            m["net_platform_fee"] = sum(v for field, v in m.items() if field in self.PLATFORM_FEE_GROUP)
            m["net_high_return_fee"] = m.get('Very high "item not as described" fee', 0) + m.get(
                'Refund Very high "item not as described" fee', 0)
            m["net_low_rating_fee"] = m.get("Below standard performance fee", 0) + m.get(
                "Refund Below standard performance fee", 0)
            m["net_third_party_fee"] = m.get("Payments dispute fee", 0)
            m["net_ad_fee"] = m.get("Promoted Listings fee", 0) + m.get("Refund Promoted Listings fee", 0)
            m["net_postage_cost"] = m.get("Shipping label-Earning data", 0)
            m["net_return_postage"] = m.get("Shipping label-Return", 0)
            m["profit"] = (m["net_rev"] + m["cog_value"] + m["net_shipping"] + m["net_platform_fee"] + m[
                "net_high_return_fee"] + m["net_low_rating_fee"] + m["net_third_party_fee"] + m["net_ad_fee"] + m[
                               "net_postage_cost"] + m["net_return_postage"])
        return metrics

    # =========================================================================
    # 4. æŠ¥è¡¨ç”Ÿæˆæ ¸å¿ƒ
    # =========================================================================

    def _calc_pct(self, df: pd.DataFrame, base_col: str) -> pd.DataFrame:
        res = df.copy()
        if res.empty: return res

        cols = res.columns[1:]
        for c in cols:
            if c != base_col:
                res[c] = res.apply(lambda r: 0 if r[base_col] == 0 else r[c] / r[base_col], axis=1)

        if base_col in res.columns:
            res[base_col] = 1.0
        return res

    def _format_pct(self, df: pd.DataFrame) -> pd.DataFrame:
        res = df.copy()
        if res.empty: return res

        for c in res.columns[1:]:
            if pd.api.types.is_numeric_dtype(res[c]):
                res[c] = res[c].apply(lambda x: f"{x:.2%}")
        return res

    def _calc_mom(self, df_cur: pd.DataFrame, df_prev: pd.DataFrame, key_col: str) -> pd.DataFrame:
        """
        [ç®—æ³•] è®¡ç®—ç¯æ¯”è¡¨ (A3/B3)
        """
        res = df_cur.copy()

        # 1. æ£€æŸ¥ä¸ŠæœŸæ•°æ®æœ‰æ•ˆæ€§
        if df_prev.empty or key_col not in df_prev.columns:
            for c in res.columns:
                if c != key_col:
                    res[c] = "æ— ä¸ŠæœŸæ•°æ®"
            return res

        # 2. å»ºç«‹æ˜ å°„ (å¼ºåˆ¶è½¬å­—ç¬¦ä¸²å¹¶å¤§å†™ï¼Œç¡®ä¿åŒ¹é…)
        # æ³¨æ„: è¿™é‡Œå‡è®¾ df_cur çš„ key ä¹Ÿå·²ç»æ ‡å‡†åŒ–äº†
        df_prev['temp_key'] = df_prev[key_col].astype(str).str.strip().str.upper()
        prev_map = df_prev.set_index('temp_key')

        for c in res.columns:
            if c == key_col: continue

            for idx in res.index:
                # è·å–å½“å‰è¡Œçš„ Keyï¼ŒåŒæ ·æ ‡å‡†åŒ–
                raw_key = res.at[idx, key_col]
                key = str(raw_key).strip().upper()

                cur_val = res.at[idx, c]

                # åŒ¹é…é€»è¾‘
                if key in prev_map.index and c in prev_map.columns:
                    prev_val = prev_map.at[key, c]

                    if prev_val == 0:
                        res.at[idx, c] = "æ— æ³•è®¡ç®—"
                    else:
                        try:
                            v_cur = float(str(cur_val).replace('%', '')) if isinstance(cur_val,
                                                                                       str) and '%' in cur_val else float(
                                cur_val)
                            v_prev = float(str(prev_val).replace('%', '')) if isinstance(prev_val,
                                                                                         str) and '%' in prev_val else float(
                                prev_val)

                            growth = (v_cur - v_prev) / abs(v_prev)
                            res.at[idx, c] = f"{growth:.2%}"
                        except:
                            res.at[idx, c] = "æ•°æ®é”™ä¹±"
                else:
                    res.at[idx, c] = "æ— ä¸ŠæœŸæ•°æ®"
        return res

    def generate_full_report_suite(self, m_cur: dict, m_prev: dict, key_name: str) -> List[Tuple[str, pd.DataFrame]]:
        map_a = {
            "æ€»é”€é‡": "total_qty", "æ€»å–æ¶ˆæ•°": "cancel_qty", "æ€»é€€è´§æ•°(æ— å¹³å°ä»‹å…¥)": "return_qty",
            "æ€»é€€è´§æ•°(å¹³å°ä»‹å…¥)": "request_qty", "æ€»é€€è´§æ•°(å¹³å°å¼ºåˆ¶é€€æ¬¾)": "claim_qty",
            "å¼ºåˆ¶é€€è´§(ä»…é€€æ¬¾)": "dispute_qty", "å‡€é”€å”®": "net_qty"
        }
        map_b = {
            "æ€»é”€å”®é¢": "total_rev", "æ€»å–æ¶ˆé¢": "cancel_rev", "æ€»é€€è´§é¢(æ— å¹³å°ä»‹å…¥)": "return_rev",
            "æ€»é€€è´§é¢(å¹³å°ä»‹å…¥)": "request_rev", "æ€»é€€è´§é¢(å¹³å°å¼ºåˆ¶é€€æ¬¾)": "claim_rev",
            "å¼ºåˆ¶é€€æ¬¾(ä»…é€€æ¬¾)": "dispute_rev", "å‡€é”€å”®": "net_rev", "å‡€é”€å”®äº§å“æˆæœ¬": "cog_value",
            "å‡€ä¹°å®¶æ”¯ä»˜é‚®è´¹": "net_shipping", "å‡€é”€å”®ç¨": "net_tax", "å‡€å›ºå®šå¹³å°è´¹ç”¨": "net_platform_fee",
            "å‡€é«˜é€€è´§äº§å“ç½šæ¬¾": "net_high_return_fee", "å‡€ä½è´¦æˆ·è¯„çº§ç½šæ¬¾": "net_low_rating_fee",
            "å‡€ç¬¬ä¸‰æ–¹æŠ•è¯‰ç½šæ¬¾": "net_third_party_fee", "å‡€å¹¿å‘Šå¼€é”€": "net_ad_fee",
            "é€€è´§åŒ…é‚®è´¹ç”¨": "net_return_postage", "å‡€é‚®è´¹æ”¯å‡º": "net_postage_cost", "ç›ˆäº": "profit"
        }

        def build_df(mets, mapping):
            data = []
            for k in sorted(mets.keys()):
                row = {key_name: k}
                for label, field in mapping.items():
                    row[label] = round(mets[k].get(field, 0), 2)
                data.append(row)

            if not data:
                cols = [key_name] + list(mapping.keys())
                return pd.DataFrame(columns=cols)

            return pd.DataFrame(data)

        # ç”ŸæˆåŸºç¡€è¡¨
        df_a1 = build_df(m_cur, map_a)
        df_pa1 = build_df(m_prev, map_a)

        # [è¯Šæ–­æ—¥å¿—] æ‰“å° Key çš„é‡åˆæƒ…å†µ
        cur_keys = set(df_a1[key_name].astype(str).str.strip().str.upper())
        prev_keys = set(df_pa1[key_name].astype(str).str.strip().str.upper())
        overlap = cur_keys.intersection(prev_keys)

        if not df_pa1.empty:
            self.log(f"ï¸ [åŒ¹é…è¯Šæ–­] æœ¬æœŸ {key_name} æ•°: {len(cur_keys)}, ä¸ŠæœŸæ•°: {len(prev_keys)}")
            self.log(f"ï¸ [åŒ¹é…è¯Šæ–­] æˆåŠŸåŒ¹é…(é‡åˆ)æ•°: {len(overlap)}ã€‚å¦‚æœæ­¤æ•°å­—è¿‡ä½ï¼Œè¯´æ˜ Key æ ¼å¼ä¸ä¸€è‡´æˆ–äº§å“æ¢ä»£ã€‚")

        # ç”Ÿæˆåç»­è¡¨
        df_a2 = self._format_pct(self._calc_pct(df_a1, "æ€»é”€é‡"))
        df_a3 = self._calc_mom(self._calc_pct(df_a1, "æ€»é”€é‡"), self._calc_pct(df_pa1, "æ€»é”€é‡"), key_name)

        df_b1 = build_df(m_cur, map_b)
        df_pb1 = build_df(m_prev, map_b)

        df_b2 = self._format_pct(self._calc_pct(df_b1, "æ€»é”€å”®é¢"))
        df_b3 = self._calc_mom(self._calc_pct(df_b1, "æ€»é”€å”®é¢"), self._calc_pct(df_pb1, "æ€»é”€å”®é¢"), key_name)

        return [
            ("A1_æ•°é‡è¡¨", df_a1),
            ("A2_æ•°é‡å æ¯”è¡¨", df_a2),
            ("A3_æ•°é‡ç»“æ„ç¯æ¯”è¡¨", df_a3),
            ("B1_é‡‘é¢è¡¨", df_b1),
            ("B2_é‡‘é¢å æ¯”è¡¨", df_b2),
            ("B3_è´¹ç”¨ç»“æ„ç¯æ¯”è¡¨", df_b3)
        ]

    @abstractmethod
    def _aggregate(self, df: pd.DataFrame):
        pass

==================== END FILE: core/services/profit_base.py ====================


==================== START FILE: core/services/sales_analyzer.py ====================
# core/services/sales_analyzer.py
import pandas as pd
import numpy as np
import tqdm
from collections import defaultdict

from core.base import BaseAnalyzer
from core.repository.transaction_repo import TransactionRepository
from core.repository.sku_repo import SkuRepository
from core.rules import BusinessRules
from config import Config


class SalesQtyAnalyzer(BaseAnalyzer):
    """
    [ä¸šåŠ¡æœåŠ¡] SKU é”€é‡ç»Ÿè®¡åˆ†æ (V2.2 Normalized)
    Fix: å¼ºåˆ¶ SKU è½¬å¤§å†™ï¼Œé˜²æ­¢å¤§å°å†™å¯¼è‡´çš„é‡å¤ç»Ÿè®¡ã€‚
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.trans_repo = TransactionRepository()
        self.sku_repo = SkuRepository()

    def _get_row_val(self, row, keys, default=None):
        for k in keys:
            if k in row:
                return row[k]
        return default

    def _process_row(self, row, stats_dict):
        raw_seller = self._get_row_val(row, ["seller", "Seller"], "")
        seller = str(raw_seller).strip().lower()
        action = str(row.get("action", "")).strip().upper()

        try:
            quantity_val = int(float(row.get("quantity", 0)))
        except:
            quantity_val = 0

        sku_list = []
        for i in range(1, 21):
            s_key = f"sku{i}"
            q_key = f"qty{i}"

            if s_key not in row:
                if f"P_SKU{i}" in row:
                    s_key, q_key = f"P_SKU{i}", f"P_Quantity{i}"
                else:
                    break

            sku_val = row.get(s_key)
            if sku_val and str(sku_val).strip() not in ["", "None", "nan", "NaN","0"]:
                try:
                    q = int(float(row.get(q_key, 0)))
                except:
                    q = 0
                # [æ ¸å¿ƒä¿®å¤] å¼ºåˆ¶è½¬å¤§å†™
                sku_clean = str(sku_val).strip().upper()
                sku_list.append((sku_clean, q))
            else:
                break

        if any(sku in ["NU1C8E51C", "NU1C8E51K"] for sku, _ in sku_list):
            sku_list.append(("NU1C8SKT7", 2))

        action_map = {
            "88": ["esparts88"],
            "plus": ["espartsplus"],
            "total": None
        }

        code_map = {
            "Canceled": "CA", "Returned": "RE", "Cased": "CC",
            "Request": "CR", "Dispute": "PD"
        }

        for prefix, target_sellers in action_map.items():
            if target_sellers is not None:
                if seller not in target_sellers:
                    continue

            for sku, qtyp in sku_list:
                total_qty = quantity_val * qtyp
                stats_dict[sku][f"{prefix}_Sold"] += total_qty

                for label, code in code_map.items():
                    if action == code:
                        stats_dict[sku][f"{prefix}_{label}"] += total_qty

    def run(self):
        self.log(f"ğŸš€ å¼€å§‹åˆ†æé”€é‡: {self.start_date} -> {self.end_date}")

        df_log = self.trans_repo.get_transactions_by_date(self.start_date, self.end_date)

        # [æœ€ç»ˆé˜²å¾¡] æ£€æŸ¥è¡Œæ•°
        if df_log is None or df_log.empty:
            self.log("âš ï¸ æœŸé—´æ— æ•°æ® (Repositoryè¿”å›ç©º)")
            return

        self.log(f"ğŸ“Š å·²åŠ è½½åŸå§‹è®°å½•: {len(df_log)} æ¡")

        stats = defaultdict(lambda: defaultdict(int))
        records = df_log.to_dict('records')

        for row in tqdm.tqdm(records, desc="è®¡ç®—é”€é‡"):
            self._process_row(row, stats)

        if not stats:
            self.log("âš ï¸ ç»Ÿè®¡ç»“æœä¸ºç©º")
            return

        df_res = pd.DataFrame.from_dict(stats, orient='index').reset_index()
        df_res.rename(columns={'index': 'SKU'}, inplace=True)
        df_res.fillna(0, inplace=True)

        R = Config.LOSS_RATES
        prefixes = ["88", "plus", "total"]
        metrics = ["Canceled", "Returned", "Cased", "Request", "Dispute"]

        for prefix in prefixes:
            if f"{prefix}_Sold" not in df_res.columns:
                df_res[f"{prefix}_Sold"] = 0
            sold = df_res[f"{prefix}_Sold"]

            for metric in metrics:
                col_name = f"{prefix}_{metric}"
                if col_name not in df_res.columns: df_res[col_name] = 0
                df_res[f"{prefix}_{metric}_%"] = (df_res[col_name] / sold).replace([np.inf, -np.inf], 0).fillna(
                    0).apply(lambda x: f"{x:.2%}")

            df_res[f"{prefix}_Net"] = (
                    df_res[f"{prefix}_Sold"]
                    - df_res[f"{prefix}_Canceled"]
                    - df_res[f"{prefix}_Returned"] * R.get('RETURN', 0.3)
                    - df_res[f"{prefix}_Cased"] * R.get('CASE', 0.6)
                    - df_res[f"{prefix}_Request"] * R.get('REQUEST', 0.5)
                    - df_res[f"{prefix}_Dispute"] * R.get('DISPUTE', 1.0)
            ).astype(int)

        cols = ["SKU"]
        for p in prefixes:
            cols.extend([
                f"{p}_Sold", f"{p}_Canceled", f"{p}_Canceled_%",
                f"{p}_Returned", f"{p}_Returned_%", f"{p}_Cased", f"{p}_Cased_%",
                f"{p}_Request", f"{p}_Request_%", f"{p}_Dispute", f"{p}_Dispute_%",
                f"{p}_Net"
            ])

        for c in cols:
            if c not in df_res.columns: df_res[c] = 0
        df_res = df_res[cols]

        filename = f"SKU_Sold_{self.file_suffix}.csv"

        footer = [
            " ", "å¤‡æ³¨è¯´æ˜ï¼š",
            "1. å–æ¶ˆçš„è®¢å•ä¸ç®—åº“å­˜æ¶ˆè€—",
            f"2. Caseä¸ºå®¢æˆ·æŠ•è¯‰é€€è´§,å¹³å°ä»‹å…¥å¼ºåˆ¶é€€æ¬¾,è€—æŸç‡{int(R.get('CASE', 0.6) * 100)}%",
            f"3. Requestä¸ºå®¢æˆ·ç”³è¯·é€€è´§,å¹³å°ä»‹å…¥,å–å®¶é€€æ¬¾,è€—æŸç‡{int(R.get('REQUEST', 0.5) * 100)}%",
            f"4. Returnä¸ºå®¢æˆ·ç”³è¯·é€€è´§,æ— å¹³å°ä»‹å…¥,å–å®¶é€€æ¬¾, è€—æŸç‡{int(R.get('RETURN', 0.3) * 100)}%",
            f"5. Disputeä¸ºå®¢æˆ·é€šè¿‡é“¶è¡ŒæŠ•è¯‰, å¹³å°å¼ºåˆ¶é€€æ¬¾, è€—æŸç‡{int(R.get('DISPUTE', 1.0) * 100)}%",
        ]

        self.save_csv(df_res, filename, footer)

==================== END FILE: core/services/sales_analyzer.py ====================


==================== START FILE: core/services/inventory_snapshot.py ====================
# core/services/inventory_snapshot.py

import pandas as pd
from datetime import datetime
from core.base import BaseAnalyzer
from core.repository.sku_repo import SkuRepository


class InventorySnapshot(BaseAnalyzer):
    """
    [ä¸šåŠ¡æœåŠ¡] å½“å‰åº“å­˜è´§å€¼å¯¼å‡º (Asset Snapshot V2.3 Normalized)
    Fix: å¼ºåˆ¶å¯¹ SKU è¿›è¡Œå»é‡èšåˆï¼Œé˜²æ­¢é‡å¤è¡Œã€‚
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.sku_repo = SkuRepository()

    def run(self):
        self.log("ğŸš€ å¼€å§‹å¯¼å‡ºå½“å‰åº“å­˜è´§å€¼å¿«ç…§...")

        # 1. è·å–æœ€æ–°åº“å­˜
        df_inv = self.sku_repo.get_inventory_latest()

        if df_inv.empty:
            self.log("âš ï¸ åº“å­˜è¡¨ (Data_Inventory) ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
            return

        # [æ ¸å¿ƒä¿®å¤] å½’ä¸€åŒ– + èšåˆå»é‡
        df_inv["SKU"] = df_inv["SKU"].astype(str).str.strip().str.upper()
        df_inv["Quantity"] = pd.to_numeric(df_inv["Quantity"], errors='coerce').fillna(0)
        # å¦‚æœæ•°æ®åº“æœ‰é‡å¤ SKU (å¦‚ 'abc' å’Œ 'ABC')ï¼Œè¿™é‡Œä¼šåˆå¹¶
        df_inv = df_inv.groupby("SKU", as_index=False)["Quantity"].sum()

        # 2. è·å–æˆæœ¬
        df_cogs = self.sku_repo.get_all_cogs()[["SKU", "Cost", "Cog"]]

        # [æ ¸å¿ƒä¿®å¤] COGS å½’ä¸€åŒ– (å–ç¬¬ä¸€æ¡æœ‰æ•ˆæˆæœ¬)
        df_cogs["SKU"] = df_cogs["SKU"].astype(str).str.strip().str.upper()
        df_cogs["Cost"] = pd.to_numeric(df_cogs["Cost"], errors='coerce').fillna(0)
        df_cogs["Cog"] = pd.to_numeric(df_cogs["Cog"], errors='coerce').fillna(0)
        # å»é‡ï¼Œä¿ç•™ç¬¬ä¸€æ¡
        df_cogs = df_cogs.drop_duplicates(subset=["SKU"], keep='first')

        # 4. å…³è”è®¡ç®— (Left Join)
        df_merged = pd.merge(df_inv, df_cogs, on="SKU", how="left")
        df_merged.fillna(0, inplace=True)

        # 5. è®¡ç®—èµ„äº§ä»·å€¼
        df_merged["è´§å€¼(ä¸å«è¿è´¹)"] = (df_merged["Quantity"] * df_merged["Cost"]).round(2)
        df_merged["è´§å€¼(å«è¿è´¹)"] = (df_merged["Quantity"] * df_merged["Cog"]).round(2)

        # 6. æ•´ç†è¾“å‡º
        final_cols = ["SKU", "Quantity", "è´§å€¼(ä¸å«è¿è´¹)", "è´§å€¼(å«è¿è´¹)"]
        df_out = df_merged[final_cols]

        total_asset = df_out["è´§å€¼(å«è¿è´¹)"].sum()
        self.log(f"ğŸ’° å½“å‰åº“å­˜æ€»èµ„äº§(å«è¿è´¹): ${total_asset:,.2f}")

        # 7. ä¿å­˜
        now_str = datetime.now().strftime("%Y_%m_%d")
        filename = f"Current_Inventory_Snapshot_{now_str}.csv"

        self.save_csv(df_out, filename)

==================== END FILE: core/services/inventory_snapshot.py ====================


==================== START FILE: core/services/diagnostics/sku.py ====================
# core/services/diagnostics/sku.py
import pandas as pd
from .base import BaseDiagnostician


class SkuDiagnostician(BaseDiagnostician):
    """
    [ç­–ç•¥æœåŠ¡] SKU ä¾›åº”é“¾å¥åº·åº¦è¯Šæ–­ä¸“å®¶

    æ ¸å¿ƒå…³æ³¨ï¼šåº“å­˜å‘¨è½¬(Turnover)ã€èµ„é‡‘æ•ˆç‡(Cash Flow)ã€ä¾›åº”é“¾é£é™©(Supply Chain Risk)ã€‚
    é€»è¾‘å¤åˆ»ï¼šAnalysis_Performance_SKU.py
    """

    def __init__(self, metrics_cur: dict, metrics_prev: dict, inventory_map: dict):
        super().__init__(metrics_cur, metrics_prev)
        self.inventory_map = inventory_map  # SKU ç‹¬æœ‰çš„åº“å­˜æ•°æ®

    def _prepare_features(self) -> pd.DataFrame:
        data = []
        for sku, cur in self.m_cur.items():
            # 1. åŸºç¡€è´¢åŠ¡æŒ‡æ ‡
            sales = cur.get("net_qty", 0)
            rev = cur.get("total_rev", 0)
            profit = cur.get("profit", 0)
            margin = profit / rev if rev > 0 else 0

            # 2. é£é™©æŒ‡æ ‡ (é€€è´§ä¸å¹¿å‘Š)
            # é€€è´§ç‡ = (Return + Request + Claim) / Total Orders
            # ä¸¥æ ¼ä¾æ®
            bad_qty = (cur.get("return_qty", 0) + cur.get("request_qty", 0) + cur.get("claim_qty", 0))
            total_qty = cur.get("total_qty", 0)
            ret_rate = bad_qty / total_qty if total_qty > 0 else 0

            ad_cost = cur.get("net_ad_fee", 0)
            acos = ad_cost / rev if rev > 0 else 0

            # 3. è¶‹åŠ¿æŒ‡æ ‡ (ç¯æ¯”å¢é•¿)
            prev_sales = self.m_prev.get(sku, {}).get("net_qty", 0)
            if prev_sales > 0:
                growth = (sales - prev_sales) / prev_sales
            else:
                growth = 1.0 if sales > 0 else 0.0

            # 4. ä¾›åº”é“¾æŒ‡æ ‡ (DOS - Days of Supply)
            curr_inv = self.inventory_map.get(sku, 0)
            # å‡è®¾æœˆé”€å‡æ‘Šåˆ°30å¤©ï¼Œè‹¥é”€é‡ä¸º0ï¼Œå‘¨è½¬å¤©æ•°è®¾ä¸ºæå€¼999
            daily_sales = sales / 30 if sales > 0 else 0
            dos = curr_inv / daily_sales if daily_sales > 0 else 999

            data.append({
                "SKU": sku, "Sales": sales, "Margin": margin,
                "ACOS": acos, "ReturnRate": ret_rate,
                "Growth": growth, "DOS": dos, "Inventory": curr_inv
            })

        return pd.DataFrame(data)

    def diagnose(self) -> pd.DataFrame:
        df = self._prepare_features()
        if df.empty: return df

        # åŠ¨æ€åˆ†ä½æ•°è®¡ç®— (BCGçŸ©é˜µæ ¸å¿ƒ)
        q_sales_high = df["Sales"].quantile(0.8)
        q_sales_low = df["Sales"].quantile(0.2)
        q_margin_high = df["Margin"].quantile(0.8)
        q_margin_low = df["Margin"].quantile(0.2)

        results = []
        for _, row in df.iterrows():
            tags, sugs = [], []

            # --- BCG äº§å“çŸ©é˜µ ---
            #
            if row["Sales"] > q_sales_high:
                if row["Margin"] > q_margin_high:
                    tags.append("ğŸŒŸæ˜æ˜Ÿ(Star)")
                    sugs.append("æ ¸å¿ƒèµ„äº§: ç¡®ä¿åº“å­˜ä¼˜å…ˆä¾›åº”ï¼Œå»ºç«‹é˜²å¾¡å£å’ã€‚")
                else:
                    tags.append("ğŸ®ç°é‡‘ç‰›(Cow)")
                    sugs.append("èµ„é‡‘æ”¯æŸ±: ä¸¥æ§æˆæœ¬ï¼Œç»´æŒç°æœ‰è§„æ¨¡ï¼Œä½œä¸ºå¼•æµå…¥å£ã€‚")
            elif row["Sales"] < q_sales_low:
                if row["Margin"] < q_margin_low:
                    tags.append("ğŸ•ç˜¦ç‹—(Dog)")
                    sugs.append("è´Ÿèµ„äº§: å»ºè®®ç«‹å³åœæ­¢è¡¥è´§ï¼Œè¿›è¡Œæ¸…ä»“æˆ–æ·˜æ±°ã€‚")
                else:
                    tags.append("ğŸ’æ½œåŠ›(Question)")
                    sugs.append("è§‚å¯ŸæœŸ: ä¼˜åŒ–Listingè´¨é‡ï¼Œå°è¯•ç²¾å‡†å¹¿å‘ŠæŠ•æ”¾æµ‹è¯•ã€‚")
            else:
                tags.append("ğŸ”¹å¹³åº¸(Average)")
                sugs.append("ç»´æŒç°çŠ¶: å®šæœŸå¤ç›˜ï¼Œå¯»æ‰¾çªç ´æœºä¼šã€‚")

            # --- è¶‹åŠ¿ç›‘æ§ ---
            #
            if row["Growth"] > 0.2:
                tags.append("ğŸš€é£™å‡")
            elif row["Growth"] < -0.2:
                tags.append("ğŸ“‰è¡°é€€")

            # --- é£é™©é¢„è­¦ ---
            #
            if row["ReturnRate"] > 0.1:
                tags.append("âš ï¸é«˜é€€è´§")
                sugs.append("å“è´¨çº¢çº¿: ç«‹å³æ£€æŸ¥äº§å“è´¨é‡æˆ–Listingæè¿°å‡†ç¡®æ€§ã€‚")

            if row["ACOS"] > 0.4:
                tags.append("ğŸ”¥å¹¿å‘Šé»‘æ´")
                sugs.append("ROIè¿‡ä½: ç¼©å‡å¹¿å‘Šé¢„ç®—ï¼Œä¼˜åŒ–å…³é”®è¯åŒ¹é…ã€‚")

            # --- åº“å­˜å¥åº·åº¦ (DOS) ---
            #
            if row["DOS"] < 7:
                tags.append("ğŸš¨æ–­è´§é¢„è­¦")
                sugs.append("ç´§æ€¥è¡¥è´§: åº“å­˜ä¸è¶³æ”¯æ’‘ä¸€å‘¨é”€é‡ï¼Œéœ€ç©ºè¿è¡¥è´§ã€‚")
            elif row["DOS"] > 180:
                tags.append("ğŸ“¦æ»é”€ç§¯å‹")
                sugs.append("å»åº“å­˜: åº“é¾„è¿‡é•¿ï¼Œå»ºè®®ç«™å¤–Dealæˆ–é™ä»·æ¸…ä»“ã€‚")

            results.append({
                "SKU": row["SKU"],
                "è¯Šæ–­æ ‡ç­¾": " | ".join(tags),
                "AIè¿è¥å»ºè®®": " ".join(sugs),
                "é”€é‡": int(row["Sales"]),
                "åˆ©æ¶¦ç‡": f"{row['Margin']:.2%}",
                "ACOS": f"{row['ACOS']:.2%}",
                "å‘¨è½¬å¤©æ•°": round(row["DOS"], 1),
                "é€€è´§ç‡": f"{row['ReturnRate']:.2%}",
                "å½“å‰åº“å­˜": int(row["Inventory"])
            })

        return pd.DataFrame(results)

    @staticmethod
    def get_tag_definitions() -> list:
        return [
            "ğŸ“˜ SKU ä¾›åº”é“¾è¯Šæ–­ä½“ç³»è¯´æ˜ (Supply Chain Diagnostics):",
            "=================================================================================",
            "1. ã€BCG çŸ©é˜µå®šä½ã€‘ åŸºäºæœ¬æœŸæ•°æ®åˆ†ä½æ•°(Quantile)åŠ¨æ€åˆ’åˆ†:",
            "   - ğŸŒŸ æ˜æ˜Ÿ (Star)     [é”€é‡Top20% + åˆ©æ¶¦Top20%] : ä¼ä¸šå¢é•¿å¼•æ“ï¼Œéœ€èµ„æºå€¾æ–œã€‚",
            "   - ğŸ® ç°é‡‘ç‰› (Cow)    [é”€é‡Top20% + åˆ©æ¶¦éTop]  : å¸‚åœºå æœ‰ç‡é«˜ï¼Œæä¾›ç¨³å®šç°é‡‘æµã€‚",
            "   - ğŸ’ æ½œåŠ› (Question) [é”€é‡Bottom20% + åˆ©æ¶¦Top] : é«˜æ¯›åˆ©ä½†åœ¨å­µåŒ–æœŸï¼Œéœ€æµé‡æ‰¶æŒã€‚",
            "   - ğŸ• ç˜¦ç‹— (Dog)      [é”€é‡Bottom20% + åˆ©æ¶¦Bottom] : æ»é”€ä¸”äºæŸï¼Œåº”å‰¥ç¦»èµ„äº§ã€‚",
            "",
            "2. ã€åº“å­˜å¥åº·åº¦ (DOS)ã€‘ Days of Supply:",
            "   - ğŸš¨ æ–­è´§é¢„è­¦ (DOS < 7å¤©): æé«˜ç¼ºè´§é£é™©ï¼Œå¯èƒ½å¯¼è‡´Listingé™æƒã€‚",
            "   - ğŸ“¦ æ»é”€ç§¯å‹ (DOS > 180å¤©): èµ„é‡‘å‘¨è½¬æ•ˆç‡æä½ï¼Œéœ€è®¡æè·Œä»·å‡†å¤‡ã€‚",
            "",
            "3. ã€é£é™©æ§åˆ¶ã€‘:",
            "   - ğŸ”¥ å¹¿å‘Šé»‘æ´ (ACOS > 40%): å¹¿å‘ŠèŠ±è´¹è¶…è¿‡è­¦æˆ’çº¿ï¼Œå³ä½¿å‡ºå•ä¹Ÿå¯èƒ½äºæŸã€‚",
            "   - âš ï¸ é«˜é€€è´§ (Rate > 10%): äº§å“å­˜åœ¨è´¨é‡æˆ–æè¿°ç¼ºé™·ï¼Œç”šè‡³å¯¼è‡´å°å·é£é™©ã€‚",
            "================================================================================="
        ]

==================== END FILE: core/services/diagnostics/sku.py ====================


==================== START FILE: core/services/diagnostics/crm.py ====================
# core/services/diagnostics/crm.py

import pandas as pd
from .base import BaseDiagnostician


class CustomerDiagnostician(BaseDiagnostician):
    """
    [ç­–ç•¥æœåŠ¡] å®¢æˆ·ä»·å€¼ä¸é£é™©è¯Šæ–­ä¸“å®¶ (CRM Professional)

    V2.0 Fix:
    1. æ˜¾å¼å®ç° __init__ ä»¥åŒ¹é… BaseDiagnostician æ¥å£ã€‚
    2. å°†ä¼ å…¥çš„ metrics_cur (DataFrame) æ­£ç¡®èµ‹å€¼ç»™ self.m_curã€‚
    """

    def __init__(self, metrics_cur: pd.DataFrame, metrics_prev: pd.DataFrame = None, **kwargs):
        """
        åˆå§‹åŒ–è¯Šæ–­å™¨
        :param metrics_cur: æœ¬æœŸ RFM æ•°æ® (DataFrame)
        :param metrics_prev: ä¸ŠæœŸæ•°æ® (CRM æš‚æœªä½¿ç”¨ï¼Œè®¾ä¸º None)
        """
        # å¿…é¡»æ˜¾å¼è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œæˆ–è€…æ‰‹åŠ¨èµ‹å€¼
        # è¿™é‡Œæˆ‘ä»¬æ‰‹åŠ¨èµ‹å€¼ä»¥ç¡®ä¿ç±»å‹å®‰å…¨ï¼Œå› ä¸ºåŸºç±»å¯èƒ½æœŸæœ›çš„æ˜¯ dictï¼Œä½† CRM ä¼ çš„æ˜¯ DataFrame
        self.m_cur = metrics_cur
        self.m_prev = metrics_prev
        self.kwargs = kwargs

    def diagnose(self) -> pd.DataFrame:
        # åœ¨ CRM ä¸­ï¼Œm_cur æ˜¯ä¸€ä¸ª DataFrameï¼Œä¸æ˜¯ dict
        df = self.m_cur.copy()
        if df.empty: return df

        # è®¡ç®—åŠ¨æ€é˜ˆå€¼ (Top 20%)
        q_net_m_high = df["Net_Monetary"].quantile(0.8)
        q_freq_high = df["Frequency"].quantile(0.8)

        results = []

        for _, row in df.iterrows():
            tags, sugs = [], []

            # æå–æ ¸å¿ƒæŒ‡æ ‡
            user = row["buyer username"]
            freq = row["Frequency"]
            net_monetary = row["Net_Monetary"]  # æ ¸å¿ƒæŒ‡æ ‡ï¼šå‡€å€¼
            gross_monetary = row["Gross_Monetary"]
            recency = row["Recency"]
            ret_rate = row["ReturnRate"]
            dispute = row["DisputeCount"]
            aov = row["AOV"]

            is_target_user = False

            # åŸºç¡€å®‰å…¨åˆ¤å®š: é€€è´§ç‡ < 20% ä¸” æ— çº çº·
            is_safe_user = (ret_rate < 0.2) and (dispute == 0)

            # =========================================================
            # 1. åŠ£è´¨/é£é™©ç»„ (Risk Group)
            # =========================================================

            # è§„åˆ™A: çº çº·é»‘åå•
            if dispute > 0:
                tags.append("çº çº·å‘èµ·è€…")
                sugs.append("é«˜å±é¢„è­¦: æ›¾å‘èµ·Payment Disputeï¼Œå»ºè®®æ‹‰é»‘ã€‚")
                is_target_user = True

            # è§„åˆ™B: æƒ¯æ€§é€€è´§ (ä¹°å¾—å¤šé€€å¾—å¤š)
            if freq >= 2 and ret_rate > 0.3:
                tags.append("âš ï¸æƒ¯æ€§é€€è´§")
                sugs.append(f"åˆ©æ¶¦æ€æ‰‹: é€€è´§ç‡{ret_rate:.0%}ï¼Œå®é™…å‡€å€¼ä»…{net_monetary:.2f}ã€‚")
                is_target_user = True

            # è§„åˆ™C: è™šå‡å¤§æˆ· (Grosså¾ˆé«˜ï¼ŒNetå¾ˆä½)
            if gross_monetary > 1000 and (net_monetary / gross_monetary) < 0.2:
                tags.append("ğŸ¤¡è™šå‡å¤§æˆ·")
                sugs.append("æ— æ•ˆäº¤æ˜“: äº§ç”Ÿå¤§é‡æµæ°´ä½†æ— å®é™…åˆ©æ¶¦ï¼Œæµªè´¹è¿è´¹ã€‚")
                is_target_user = True

            # =========================================================
            # 2. ä¼˜è´¨/ä»·å€¼ç»„ (Value Group)
            # =========================================================

            if is_safe_user:
                # è§„åˆ™D: è¶…çº§é²¸é±¼ (Real Whale)
                if net_monetary > df["Net_Monetary"].quantile(0.9):
                    tags.append("ğŸ³è¶…çº§é²¸é±¼")
                    sugs.append("VVIP: å‡€è´¡çŒ®æé«˜ï¼Œéœ€é˜²æ­¢æµå¤±ã€‚")
                    is_target_user = True

                # è§„åˆ™E: å¿ è¯šé“ç²‰ (Loyalist)
                elif freq >= 4 and recency < 60:
                    tags.append("â¤ï¸å¿ è¯šé“ç²‰")
                    sugs.append("é«˜ç²˜æ€§: å¤è´­ä¹ æƒ¯è‰¯å¥½ï¼Œæ–°å“æ¨å¹¿é¦–é€‰ã€‚")
                    is_target_user = True

                # è§„åˆ™F: æ½œåŠ›æ‰¹å‘å•† (Wholesaler)
                elif aov > 500 and freq <= 5:
                    tags.append("ğŸ“¦æ½œåŠ›æ‰¹å‘å•†")
                    sugs.append("Bç«¯å¼€å‘: å•ç¬”å‡€å€¼é«˜ï¼Œå¯èƒ½æ˜¯çº¿ä¸‹åº—æˆ–åŒè¡Œã€‚")
                    is_target_user = True

                # è§„åˆ™G: æ²‰ç¡å¤§å®¢æˆ·
                elif net_monetary > q_net_m_high and recency > 90:
                    tags.append("ğŸ’¤æ²‰ç¡å¤§å®¢æˆ·")
                    sugs.append("å¬å›: ä¼˜è´¨å®¢æˆ·æµå¤±é¢„è­¦ï¼Œå»ºè®®é‚®ä»¶è§¦è¾¾ã€‚")
                    is_target_user = True

            if not is_target_user:
                continue

            results.append({
                "ä¹°å®¶ç”¨æˆ·å": user,
                "å®¢æˆ·æ ‡ç­¾": " | ".join(tags),
                "è¿è¥å»ºè®®": " ".join(sugs),
                "å‡€æ¶ˆè´¹é¢(Net LTV)": round(net_monetary, 2),
                "æ€»æµæ°´(Gross)": round(gross_monetary, 2),
                "è®¢å•æ•°(1Y)": freq,
                "é€€è´§ç‡(1Y)": f"{ret_rate:.1%}",
                "æœ€è¿‘è´­ä¹°(å¤©å‰)": int(recency),
                "å®¢å•ä»·(Net AOV)": round(aov, 2)
            })

        return pd.DataFrame(results)

    @staticmethod
    def get_tag_definitions() -> list:
        return [
            "ğŸ“˜ å®¢æˆ·ä»·å€¼ä¸é£é™©è¯Šæ–­è¯´æ˜ (CRM V2.0):",
            "=================================================================================",
            "ã€æ ¸å¿ƒé€»è¾‘ã€‘ æ•°æ®åŸºäºã€è¿‡å»365å¤©ã€‘ã€‚æ‰€æœ‰ä»·å€¼åˆ¤æ–­åŸºäºã€å‡€æ¶ˆè´¹é¢ (Net LTV)ã€‘ï¼Œå³æ‰£é™¤é€€æ¬¾åçš„çœŸå®æ”¶å…¥ã€‚",
            "",
            "1. ğŸš« é£é™©æ§åˆ¶:",
            "   -  çº çº·å‘èµ·è€…: æœ‰ Payment Dispute è®°å½•ã€‚",
            "   - âš ï¸ æƒ¯æ€§é€€è´§: è´­ä¹°>=2æ¬¡ ä¸” é€€è´§ç‡>30%ã€‚",
            "   - ğŸ¤¡ è™šå‡å¤§æˆ·: æ€»æµæ°´é«˜ï¼Œä½†å‡€å€¼æä½ (é€€æ¬¾å æ¯”>80%)ï¼Œå±äºæ— æ•ˆç¹è£ã€‚",
            "",
            "2. ğŸ’ ä»·å€¼æŒ–æ˜ (å‰æ: é€€è´§ç‡<20%):",
            "   - ğŸ³ è¶…çº§é²¸é±¼: å‡€æ¶ˆè´¹é¢å¤„äºå…¨åº— Top 10%ã€‚",
            "   - â¤ï¸ å¿ è¯šé“ç²‰: å¹´è´­ä¹°>=4æ¬¡ ä¸” æœ€è¿‘60å¤©æœ‰äº¤æ˜“ã€‚",
            "   - ğŸ“¦ æ½œåŠ›æ‰¹å‘å•†: è´­ä¹°é¢‘æ¬¡ä½ï¼Œä½†å•ç¬”å‡€å€¼ > $500ã€‚",
            "   - ğŸ’¤ æ²‰ç¡å¤§å®¢æˆ·: å†å²å‡€å€¼é«˜(Top 20%) ä½† >90å¤©æœªå›è´­ã€‚",
            "================================================================================="
        ]

==================== END FILE: core/services/diagnostics/crm.py ====================


==================== START FILE: core/services/diagnostics/logistics.py ====================
# core/services/diagnostics/logistics.py
import pandas as pd
from .base import BaseDiagnostician


class LogisticsDiagnostician(BaseDiagnostician):
    """
    [ç­–ç•¥æœåŠ¡] ç‰©æµæ•ˆç›Šè¯Šæ–­ä¸“å®¶ (Logistics Efficiency)

    æ ¸å¿ƒå…³æ³¨ï¼šç‰©æµè´¹æ•ˆæ¯”ã€å¼‚å¸¸è´¹ç”¨ç®¡æ§ã€æ¸ é“åˆ©æ¶¦ã€‚
    """

    def diagnose(self) -> pd.DataFrame:
        df = self.m_cur.copy()
        if df.empty: return df

        results = []
        for _, row in df.iterrows():
            tags, sugs = [], []

            # æå–æ ¸å¿ƒæŒ‡æ ‡
            combo = row["Combo"]
            total_shipping_cost = row["æ€»ç‰©æµæˆæœ¬"]
            total_rev = row["æ€»é”€å”®é¢"]

            # ç»†åˆ†è´¹ç”¨
            base_cost = row["åŸå§‹é‚®è´¹"]
            penalty = row["é‚®è´¹ç½šæ¬¾"]
            overpay = row["è¶…æ”¯é‚®è´¹"]
            return_ship = row["åŒ…é‚®é€€è´§é‚®è´¹"]

            # æ”¶å…¥ç«¯ (å‡è®¾ Order Earning ä¸­çš„ Shipping label-Earning data æ˜¯å¹³å°æ‰£é™¤çš„ï¼Œ
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦é€»è¾‘è½¬æ¢ï¼šå¦‚æœæœ‰ Shipping Paid by Buyer æ•°æ®æœ€å¥½ï¼Œ
            # æš‚æ—¶ç”¨ 'ç‰©æµè´¹ç‡' åˆ¤å®š)

            ship_ratio = total_shipping_cost / total_rev if total_rev > 0 else 0

            # =========================================================
            # 1. å¼‚å¸¸ç®¡æ§ (Cost Control)
            # =========================================================

            # è§„åˆ™A: ç½šæ¬¾é»‘æ´
            # ç½šæ¬¾é‡‘é¢ > åŸå§‹é‚®è´¹çš„ 20%
            if base_cost > 0 and (penalty / base_cost) > 0.2:
                tags.append("ğŸ’¸ç½šæ¬¾é»‘æ´")
                sugs.append("å°ºå¯¸/é‡é‡å¼‚å¸¸: å®é™…å‘è´§è§„æ ¼ä¸ç”³æŠ¥ä¸¥é‡ä¸ç¬¦ï¼Œè¯·å¤æ ¸ä»“åº“SOPã€‚")

            # è§„åˆ™B: è¶…æ”¯é¢„è­¦ (Underpaid)
            if overpay > 5:  # ç»å¯¹å€¼å¤§äº5åˆ€
                tags.append("âš–ï¸é‡é‡è¶…æ”¯")
                sugs.append("æ¸ é“é”™é…: å¯èƒ½ä½¿ç”¨äº†ä¸é€‚åˆè¯¥é‡é‡æ®µçš„ç‰©æµæœåŠ¡ã€‚")

            # è§„åˆ™C: é€€è´§è¿è´¹æ€æ‰‹
            if return_ship > (total_shipping_cost * 0.3):
                tags.append("â†©ï¸é€€è´§è¿è´¹æ€æ‰‹")
                sugs.append("é€€è´§æŸè€—é«˜: å¤§é‡ç‰©æµè´¹æµªè´¹åœ¨é€€è´§é¢å•ä¸Šã€‚")

            # =========================================================
            # 2. æ•ˆç›Šåˆ†æ (Efficiency)
            # =========================================================

            # è§„åˆ™D: æè‡´ç‰©æµ (High Efficiency)
            # ç‰©æµè´¹ç‡ < 10% (å¯¹äºå¤§ä»¶/é‡è´§å¾ˆéš¾å¾—)
            if ship_ratio < 0.10 and total_shipping_cost > 0:
                tags.append("æè‡´ç‰©æµ")
                sugs.append("ä¼˜ç§€æ¨¡å‹: è¯¥ç»„åˆç‰©æµæˆæœ¬æ§åˆ¶æä½³ï¼Œå…·æœ‰å®šä»·ä¼˜åŠ¿ã€‚")

            # è§„åˆ™E: åˆ©æ¶¦ä¾µèš€ (High Cost)
            # ç‰©æµè´¹ç‡ > 30%
            if ship_ratio > 0.30:
                tags.append("ğŸ©¸åˆ©æ¶¦ä¾µèš€")
                sugs.append("è¿è´¹è¿‡é«˜: è¿è´¹å å”®ä»·30%ä»¥ä¸Šï¼Œå»ºè®®æä»·æˆ–æ›´æ¢æµ·å¤–ä»“/ç‰©æµå•†ã€‚")

            if not tags:
                tags.append("ğŸ”¹æ­£å¸¸")

            results.append({
                "Comboç»„åˆ": combo,
                "ç‰©æµè¯Šæ–­": " | ".join(tags),
                "ä¼˜åŒ–å»ºè®®": " ".join(sugs),
                "æ€»å•é‡": int(row["åŸå§‹å•æ•°"]),
                "ç‰©æµè´¹ç‡": f"{ship_ratio:.1%}",
                "æ€»è¿è´¹": round(total_shipping_cost, 2),
                "ç½šæ¬¾å æ¯”": f"{(penalty / base_cost):.1%}" if base_cost > 0 else "0.0%"
            })

        # æŒ‰ æ€»è¿è´¹ é™åºï¼Œä¼˜å…ˆçœ‹èŠ±é’±æœ€å¤šçš„
        return pd.DataFrame(results).sort_values("æ€»è¿è´¹", ascending=False)

    @staticmethod
    def get_tag_definitions() -> list:
        return [
            "ğŸ“˜ ç‰©æµæ•ˆç›Šè¯Šæ–­è¯´æ˜ (Logistics Diagnostics):",
            "=================================================================================",
            "1. ã€å¼‚å¸¸ç®¡æ§ (Cost Control)ã€‘:",
            "   - ğŸ’¸ ç½šæ¬¾é»‘æ´: ç½šæ¬¾é‡‘é¢è¶…è¿‡åŸºç¡€è¿è´¹çš„20%ã€‚é€šå¸¸å› å°ºå¯¸æµ‹é‡é”™è¯¯æˆ–é‡é‡å°‘æŠ¥å¯¼è‡´ã€‚",
            "   - âš–ï¸ é‡é‡è¶…æ”¯: å­˜åœ¨ Underpaid è¡¥ç¼´è®°å½•ï¼Œéœ€æ ¡å‡†ç”µå­ç§¤æˆ–æ›´æ–°äº§å“å‚æ•°ã€‚",
            "   - â†©ï¸ é€€è´§è¿è´¹æ€æ‰‹: é€€è´§äº§ç”Ÿçš„é¢å•è´¹ç”¨å æ€»è¿è´¹30%ä»¥ä¸Šã€‚",
            "",
            "2. ã€è´¹æ•ˆæ¯”åˆ†æ (Efficiency)ã€‘:",
            "   -  æè‡´ç‰©æµ: ç‰©æµè´¹ç‡ < 10%ã€‚è¯¥äº§å“å…·æœ‰æå¼ºçš„ç‰©æµæˆæœ¬ä¼˜åŠ¿ï¼Œé€‚åˆåšçˆ†æ¬¾ã€‚",
            "   - ğŸ©¸ åˆ©æ¶¦ä¾µèš€: ç‰©æµè´¹ç‡ > 30%ã€‚è¿è´¹åƒæ‰äº†å¤§éƒ¨åˆ†åˆ©æ¶¦ï¼Œå»ºè®®æä»·æˆ–ä¼˜åŒ–åŒ…è£…ä½“ç§¯ã€‚",
            "================================================================================="
        ]

==================== END FILE: core/services/diagnostics/logistics.py ====================


==================== START FILE: core/services/diagnostics/__init__.py ====================


==================== END FILE: core/services/diagnostics/__init__.py ====================


==================== START FILE: core/services/diagnostics/listing.py ====================
# core/services/diagnostics/listing.py
import pandas as pd
from .base import BaseDiagnostician


class ListingDiagnostician(BaseDiagnostician):
    """
    [ç­–ç•¥æœåŠ¡] Listing (Item ID) é”€å”®è¡¨ç°è¯Šæ–­ä¸“å®¶

    æ ¸å¿ƒå…³æ³¨ï¼šé“¾æ¥æƒé‡(Weight)ã€æµé‡ä»·å€¼(Traffic Value)ã€ç”Ÿå‘½å‘¨æœŸ(Lifecycle)ã€‚
    é€»è¾‘å¤åˆ»ï¼šAnalysis_Performance_Listing.py
    """

    def diagnose(self) -> pd.DataFrame:
        data = []
        for iid, cur in self.m_cur.items():
            sales = cur.get("net_qty", 0)
            rev = cur.get("total_rev", 0)
            profit = cur.get("profit", 0)
            margin = profit / rev if rev > 0 else 0

            ad_cost = cur.get("net_ad_fee", 0)
            acos = ad_cost / rev if rev > 0 else 0

            # ç¯æ¯”å¢é•¿
            prev_sales = self.m_prev.get(iid, {}).get("net_qty", 0)
            if prev_sales > 0:
                growth = (sales - prev_sales) / prev_sales
            else:
                growth = 1.0 if sales > 0 else 0.0

            # é€€è´§ç‡ (Listing ç»´åº¦çš„é€€è´§æ›´åæ˜ æè¿°ä¸ç¬¦é—®é¢˜)
            total_qty = cur.get("total_qty", 0)
            bad_qty = cur.get("return_qty", 0) + cur.get("claim_qty", 0)
            ret_rate = bad_qty / total_qty if total_qty > 0 else 0

            data.append({
                "item id": iid,
                "item title": cur.get("title", ""),
                "Sales": sales, "Margin": margin,
                "ACOS": acos, "Growth": growth, "ReturnRate": ret_rate
            })

        df = pd.DataFrame(data)
        if df.empty: return df

        q_sales_high = df["Sales"].quantile(0.8)
        q_sales_low = df["Sales"].quantile(0.2)
        q_margin_high = df["Margin"].quantile(0.8)
        q_margin_low = df["Margin"].quantile(0.2)

        results = []
        for _, row in df.iterrows():
            tags, sugs = [], []

            #
            if row["Sales"] > q_sales_high:
                if row["Margin"] > q_margin_high:
                    tags.append("ğŸŒŸç‹ç‰ŒListing")
                    sugs.append("æ ¸å¿ƒèµ„äº§: é“¾æ¥æƒé‡æé«˜ï¼Œé˜²å®ˆç«å“è·Ÿå–ã€‚")
                else:
                    tags.append("ğŸ®å¼•æµListing")
                    sugs.append("æµé‡å…¥å£: åˆ©ç”¨å…³è”é”€å”®(Variation/Bundle)å¸¦åŠ¨é«˜åˆ©æ¬¾ã€‚")
            elif row["Sales"] < q_sales_low:
                if row["Margin"] < q_margin_low:
                    tags.append("ğŸ•åƒåœ¾Listing")
                    sugs.append("æ²‰æ²¡æˆæœ¬: å»ºè®®ä¸‹æ¶é‡åšï¼Œé‡Šæ”¾åˆŠç™»é¢åº¦ã€‚")
                else:
                    tags.append("ğŸ’é•¿å°¾Listing")
                    sugs.append("ç²¾å‡†æµé‡: ä¼˜åŒ–é•¿å°¾å…³é”®è¯ï¼Œç»´æŒé«˜ROIã€‚")
            else:
                tags.append("ğŸ”¹è…°éƒ¨Listing")
                sugs.append("æ½œåŠ›æŒ–æ˜: åˆ†æè½¬åŒ–ç‡ï¼Œå°è¯•å‚åŠ å¹³å°æ´»åŠ¨ã€‚")

            if row["Growth"] > 0.2:
                tags.append("ğŸš€æƒé‡ä¸Šå‡")
            elif row["Growth"] < -0.2:
                tags.append("ğŸ“‰æƒé‡ä¸‹æ»‘")

            if row["ReturnRate"] > 0.1: tags.append("âš ï¸é«˜é€€è´§é£é™©")
            if row["ACOS"] > 0.4: tags.append("ğŸ”¥å¹¿å‘ŠäºæŸ")

            results.append({
                "Item ID": row["item id"],
                "Item Title": row["item title"],
                "è¯Šæ–­æ ‡ç­¾": " | ".join(tags),
                "AIè¿è¥å»ºè®®": " ".join(sugs),
                "é”€é‡": int(row["Sales"]),
                "åˆ©æ¶¦ç‡": f"{row['Margin']:.2%}",
                "å¢é•¿ç‡": f"{row['Growth']:.2%}",
                "ACOS": f"{row['ACOS']:.2%}"
            })
        return pd.DataFrame(results)

    @staticmethod
    def get_tag_definitions() -> list:
        return [
            "ğŸ“˜ Listing é”€å”®è¡¨ç°è¯Šæ–­è¯´æ˜ (Sales Performance):",
            "=================================================================================",
            "1. ã€é“¾æ¥åˆ†å±‚ç­–ç•¥ã€‘:",
            "   - ğŸŒŸ ç‹ç‰Œ (Ace):      é«˜æµé‡è½¬åŒ– + é«˜æº¢ä»·èƒ½åŠ› -> å“ç‰ŒæŠ¤åŸæ²³ã€‚",
            "   - ğŸ® å¼•æµ (Traffic):  é«˜æµé‡è½¬åŒ– + ä½æº¢ä»· -> ç”¨äºæŠ¢å ç±»ç›®æ’åã€‚",
            "   - ğŸ’ é•¿å°¾ (Long Tail): ä½æµé‡ + é«˜æº¢ä»· -> æ»¡è¶³ç‰¹å®šå°ä¼—éœ€æ±‚ã€‚",
            "   - ğŸ• åƒåœ¾ (Trash):    æ— æµé‡ + æ— åˆ©æ¶¦ -> å ç”¨è¿è¥ç²¾åŠ›çš„è´Ÿå€ºã€‚",
            "",
            "2. ã€è¿è¥å…³é”®æŒ‡æ ‡ã€‘:",
            "   - ğŸš€ æƒé‡ä¸Šå‡: é”€é‡ç¯æ¯”æ¶¨å¹… > 20%ï¼Œå»ºè®®åŠ å¤§æ¨å¹¿åŠ›åº¦ä¹˜èƒœè¿½å‡»ã€‚",
            "   - ğŸ”¥ å¹¿å‘ŠäºæŸ: ACOS > 40%ï¼Œéœ€æ£€æŸ¥æ˜¯å¦å…³é”®è¯å®½æ³›åŒ¹é…å¯¼è‡´æ— æ•ˆç‚¹å‡»ã€‚",
            "================================================================================="
        ]

==================== END FILE: core/services/diagnostics/listing.py ====================


==================== START FILE: core/services/diagnostics/combo.py ====================
# core/services/diagnostics/combo.py
import pandas as pd
from .base import BaseDiagnostician


class ComboDiagnostician(BaseDiagnostician):
    """
    [ç­–ç•¥æœåŠ¡] Combo (Full SKU) æ†ç»‘ç­–ç•¥è¯Šæ–­ä¸“å®¶

    æ ¸å¿ƒå…³æ³¨ï¼šæ†ç»‘æœ‰æ•ˆæ€§(Bundling Efficiency)ã€è¿åé£é™©(Risk Association)ã€‚
    é€»è¾‘å¤åˆ»ï¼šAnalysis_Performance_Combo.py
    """

    def diagnose(self) -> pd.DataFrame:
        data = []
        for sku, cur in self.m_cur.items():
            sales = cur.get("net_qty", 0)
            rev = cur.get("total_rev", 0)
            profit = cur.get("profit", 0)
            margin = profit / rev if rev > 0 else 0

            prev_sales = self.m_prev.get(sku, {}).get("net_qty", 0)
            growth = (sales - prev_sales) / prev_sales if prev_sales > 0 else (1.0 if sales > 0 else 0.0)

            # è¿åé£é™©æ£€æŸ¥
            # å¦‚æœ Combo ä¸­æœ‰ä¸€ä¸ªå­ä»¶è´¨é‡å·®ï¼Œä¼šå¯¼è‡´æ•´ä¸ªå¤§é‡‘é¢è®¢å•é€€è´§
            bad_qty = cur.get("return_qty", 0) + cur.get("claim_qty", 0)
            total_qty = cur.get("total_qty", 0)
            ret_rate = bad_qty / total_qty if total_qty > 0 else 0

            data.append({"full sku": sku, "Sales": sales, "Margin": margin, "Growth": growth, "ReturnRate": ret_rate})

        df = pd.DataFrame(data)
        if df.empty: return df

        q_sales_high = df["Sales"].quantile(0.8)
        q_sales_low = df["Sales"].quantile(0.2)
        q_margin_high = df["Margin"].quantile(0.8)

        results = []
        for _, row in df.iterrows():
            tags, sugs = [], []

            #
            if row["Sales"] > q_sales_high:
                if row["Margin"] > q_margin_high:
                    tags.append("ğŸ‘‘é»„é‡‘ç»„åˆ")
                    sugs.append("ç­–ç•¥æˆåŠŸ: æ†ç»‘é”€å”®æˆåŠŸæå‡äº†å®¢å•ä»·(AOV)ä¸åˆ©æ¶¦ã€‚")
                else:
                    tags.append("ğŸ“¦å¼•æµåŒ…")
                    sugs.append("èµ°é‡å·¥å…·: é€šè¿‡ä½ä»·æ‰“åŒ…æŠ¢å å¸‚åœºä»½é¢ã€‚")
            elif row["Sales"] < q_sales_low:
                tags.append("æ— æ•ˆæ†ç»‘")
                sugs.append("ç­–ç•¥å¤±è´¥: å®¢æˆ·ä¸ä¹°è´¦ï¼Œå»ºè®®è§£ç»‘æˆ–æ›´æ¢ç»„åˆæ–¹å¼ã€‚")
            else:
                tags.append("ğŸ”¹æ™®é€šç»„åˆ")

            if row["Growth"] > 0.2: tags.append("ğŸ“ˆçƒ­åº¦ä¸Šå‡")

            #
            if row["ReturnRate"] > 0.1:
                tags.append("âš ï¸è¿åé£é™©")
                sugs.append("ä¸¥é‡è­¦å‘Š: é€€è´§ç‡è¿‡é«˜ï¼Œè¯·æ’æŸ¥Comboä¸­æ˜¯å¦å­˜åœ¨ä½è´¨é‡å­SKUã€‚")

            results.append({
                "Full SKU": row["full sku"],
                "è¯Šæ–­æ ‡ç­¾": " | ".join(tags),
                "AIè¿è¥å»ºè®®": " ".join(sugs),
                "é”€é‡": int(row["Sales"]),
                "åˆ©æ¶¦ç‡": f"{row['Margin']:.2%}",
                "é€€è´§ç‡": f"{row['ReturnRate']:.2%}"
            })
        return pd.DataFrame(results)

    @staticmethod
    def get_tag_definitions() -> list:
        return [
            "ğŸ“˜ Combo æ†ç»‘ç­–ç•¥è¯Šæ–­è¯´æ˜ (Bundling Strategy):",
            "=================================================================================",
            "1. ã€ç»„åˆæ•ˆç›Šåˆ†æã€‘:",
            "   - ğŸ‘‘ é»„é‡‘ç»„åˆ: 1+1>2ï¼ŒæˆåŠŸè®©å®¢æˆ·ä¸ºé«˜å®¢å•ä»·ä¹°å•ï¼Œä¸”åˆ©æ¶¦ä¸°åšã€‚",
            "   -  æ— æ•ˆæ†ç»‘: 1+1<2ï¼Œå®¢æˆ·æ›´å€¾å‘äºå•ç‹¬è´­ä¹°å­äº§å“ï¼Œæ‰“åŒ…åè€Œé™ä½äº†è½¬åŒ–ã€‚",
            "",
            "2. ã€è¿åé£é™© (Cannibalization)ã€‘:",
            "   - âš ï¸ è¿åé£é™©: å› ç»„åˆä¸­æŸä¸€ä¸ªå»‰ä»·é…ä»¶è´¨é‡å·®ï¼Œå¯¼è‡´å®¢æˆ·é€€æ‰æ•´ä¸ªé«˜ä»·åŒ…è£¹ã€‚",
            "                 (ä¾‹: èµ å“èºä¸ç”Ÿé”ˆï¼Œå¯¼è‡´å‡ ç™¾åˆ€çš„è½®æ¯‚è¢«é€€è´§)",
            "================================================================================="
        ]

==================== END FILE: core/services/diagnostics/combo.py ====================


==================== START FILE: core/services/diagnostics/base.py ====================
# core/services/diagnostics/base.py
from abc import ABC, abstractmethod
import pandas as pd


class BaseDiagnostician(ABC):
    """
    [æ¥å£å®šä¹‰] è¯Šæ–­ä¸“å®¶åŸºç±»

    ä¼ä¸šçº§è§„èŒƒï¼šæ‰€æœ‰å…·ä½“çš„è¯Šæ–­ä¸šåŠ¡ï¼ˆSKU/Listing/Comboï¼‰å¿…é¡»éµå¾ªæ­¤å¥‘çº¦ã€‚
    å¼ºåˆ¶è¦æ±‚å®ç° diagnose() è®¡ç®—é€»è¾‘å’Œ get_tag_definitions() è§£é‡Šæ–‡æ¡ˆã€‚
    """

    def __init__(self, metrics_cur: dict, metrics_prev: dict, **kwargs):
        """
        Args:
            metrics_cur: æœ¬æœŸè´¢åŠ¡æŒ‡æ ‡å­—å…¸
            metrics_prev: ä¸ŠæœŸè´¢åŠ¡æŒ‡æ ‡å­—å…¸ (ç”¨äºç¯æ¯”)
            kwargs: æ‰©å±•å‚æ•° (å¦‚åº“å­˜æ˜ å°„è¡¨)
        """
        self.m_cur = metrics_cur
        self.m_prev = metrics_prev
        self.kwargs = kwargs

    @abstractmethod
    def diagnose(self) -> pd.DataFrame:
        """æ‰§è¡Œè¯Šæ–­è®¡ç®—ï¼Œè¿”å›åŒ…å«æ ‡ç­¾å’Œå»ºè®®çš„ DataFrame"""
        pass

    @staticmethod
    @abstractmethod
    def get_tag_definitions() -> list:
        """è¿”å›æ ‡ç­¾çš„è¯¦ç»†ä¸šåŠ¡å®šä¹‰è¯´æ˜ (ç”¨äºæŠ¥è¡¨ Footer)"""
        pass

==================== END FILE: core/services/diagnostics/base.py ====================


==================== START FILE: .pytest_cache/README.md ====================
# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.

==================== END FILE: .pytest_cache/README.md ====================


==================== START FILE: tests/test_core_units.py ====================
# tests/test_core_units.py
import sys
import os
from pathlib import Path
import pytest

# [Setup] å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥ core æ¨¡å—
current_dir = Path(__file__).resolve().parent
project_root = current_dir.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

from core.security import SecurityUtils
from core.rules import BusinessRules
from core.services.correction_service import CorrectionService
from config import Config


# =============================================================================
# 1. å®‰å…¨ç»„ä»¶æµ‹è¯• (Security)
# =============================================================================
def test_password_hashing_flow():
    """éªŒè¯å¯†ç å“ˆå¸Œ -> æ ¡éªŒçš„å®Œæ•´é—­ç¯"""
    raw_pwd = "MySecretPassword123!"

    # 1. å“ˆå¸Œç”Ÿæˆ
    hashed = SecurityUtils.hash_password(raw_pwd)
    assert hashed != raw_pwd
    assert "pbkdf2_sha256" in hashed

    # 2. æ­£ç¡®å¯†ç æ ¡éªŒ
    assert SecurityUtils.verify_password(raw_pwd, hashed) is True

    # 3. é”™è¯¯å¯†ç æ ¡éªŒ
    assert SecurityUtils.verify_password("WrongPassword", hashed) is False


def test_token_generation():
    """éªŒè¯ Token ç”Ÿæˆçš„å”¯ä¸€æ€§å’Œéç©º"""
    t1 = SecurityUtils.generate_token()
    t2 = SecurityUtils.generate_token()
    assert t1 != t2
    assert len(t1) > 10


# =============================================================================
# 2. ä¸šåŠ¡è§„åˆ™æµ‹è¯• (Business Rules)
# =============================================================================
def test_loss_rate_logic():
    """éªŒè¯æ ¸å¿ƒè€—æŸç‡è®¡ç®—è§„åˆ™ (Config.LOSS_RATES)"""
    # å‡è®¾åŸå§‹æ•°é‡ä¸º 100
    qty = 100

    # Case 1: Cancel (CA) -> è€—æŸ 100% -> å‡€å€¼ 0
    assert BusinessRules.get_net_quantity(qty, "CA") == 0.0

    # Case 2: Return (RE) -> è€—æŸ 30% -> å‡€å€¼ 70
    # åŠ¨æ€è¯»å– Configï¼Œé˜²æ­¢ç¡¬ç¼–ç æµ‹è¯•å¤±è´¥
    expected_re = qty * (1 - Config.LOSS_RATES['RETURN'])
    assert BusinessRules.get_net_quantity(qty, "RE") == expected_re

    # Case 3: Dispute (PD) -> è€—æŸ 100% -> å‡€å€¼ 0
    assert BusinessRules.get_net_quantity(qty, "PD") == 0.0

    # Case 4: Normal Sale (ç©º Action æˆ–å…¶ä»–) -> è€—æŸ 0% -> å‡€å€¼ 100
    assert BusinessRules.get_net_quantity(qty, "SOLD") == 100.0


def test_abc_classification():
    """éªŒè¯ ABC åˆ†çº§é€»è¾‘"""
    # 0% - 80% -> A
    assert BusinessRules.get_abc_grade(0.10) == ("A", 0.98)
    assert BusinessRules.get_abc_grade(0.80) == ("A", 0.98)

    # 80% - 95% -> B
    assert BusinessRules.get_abc_grade(0.81) == ("B", 0.95)
    assert BusinessRules.get_abc_grade(0.95) == ("B", 0.95)

    # > 95% -> C
    assert BusinessRules.get_abc_grade(0.96) == ("C", 0.90)


# =============================================================================
# 3. çº é”™æœåŠ¡æµ‹è¯• (Correction Service - Mocked)
# =============================================================================
class MockRepo:
    def get_valid_skus(self):
        return ["APPLE-IPHONE-13", "SAMSUNG-S21", "GOOGLE-PIXEL-6"]


def test_fuzzy_suggestion(monkeypatch):
    """éªŒè¯æ¨¡ç³Šæœç´¢å»ºè®®åŠŸèƒ½ (Mock æ•°æ®åº“ä¾èµ–)"""

    # ä½¿ç”¨ monkeypatch æ¨¡æ‹Ÿ SkuRepositoryï¼Œé¿å…è¿æ¥çœŸå®æ•°æ®åº“
    monkeypatch.setattr("core.services.correction_service.SkuRepository", MockRepo)

    service = CorrectionService()

    # Case A: å®Œå…¨åŒ¹é…
    assert service.is_valid_sku("APPLE-IPHONE-13") is True
    assert service.is_valid_sku("XIAOMI-12") is False

    # Case B: æ¨¡ç³Šæ¨è
    # è¾“å…¥ "APPLE-13" (æ¼äº†ä¸­é—´)ï¼Œåº”è¯¥èƒ½æ¨è "APPLE-IPHONE-13"
    suggestions = service.get_fuzzy_suggestions("APPLE-13")
    assert "APPLE-IPHONE-13" in suggestions

    # è¾“å…¥ "SAMSNG" (æ‹¼å†™é”™è¯¯)
    suggestions = service.get_fuzzy_suggestions("SAMSNG")
    assert "SAMSUNG-S21" in suggestions
==================== END FILE: tests/test_core_units.py ====================


==================== START FILE: tests/__init__.py ====================

==================== END FILE: tests/__init__.py ====================

