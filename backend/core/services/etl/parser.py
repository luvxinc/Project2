# core/services/etl/parser.py
"""
æ–‡ä»¶è¯´æ˜: äº¤æ˜“æ•°æ®è§£æå™¨ (Transaction Parser)
ä¸»è¦åŠŸèƒ½:
1. ç»“æ„åŒ–è§£æ: ä» 'Custom label' å­—æ®µä¸­æå– SKU å’Œ Quantityã€‚
2. æ¨¡å¼è¯†åˆ«: æ”¯æŒ Single (å•å“), Dual (åŒå“), Complex (å¤šå“/ç‰¹æ®Šç¬¦) æ ¼å¼ã€‚
3. æ•°æ®æ ¡éªŒ: éªŒè¯æå–å‡ºçš„ SKU æ˜¯å¦åœ¨ç³»ç»Ÿèµ„æ–™åº“ (Data_COGS) ä¸­å­˜åœ¨ã€‚
4. [Fix] é€»è¾‘ä¿®å¤: å¼ºåˆ¶é‡æ–°æ ¡éªŒ P_Flag=99 çš„è¡Œï¼Œè§£å†³â€œåƒµå°¸é”™è¯¯â€é—®é¢˜ã€‚
"""

import pandas as pd
import numpy as np
import re
import tqdm
from typing import Dict, Any
from sqlalchemy.types import Text

from core.components.db.client import DBClient
from core.services.correction import CorrectionService
from core.sys.logger import get_logger

# æŠ‘åˆ¶ Pandas çš„ FutureWarning
pd.set_option('future.no_silent_downcasting', True)


class TransactionParser:

    def __init__(self):
        self.db = DBClient()
        self.logger = get_logger("TransactionParser")
        self.corrector = CorrectionService()

        # ç¼“å­˜ä¿®å¤å­—å…¸ (BadSKU -> {sku, qty})ï¼Œå‡å°‘å¾ªç¯å†…çš„æŸ¥æ‰¾å¼€é”€
        self.fix_map = self._build_fast_fix_map()

        # å®šä¹‰è¾“å‡ºåˆ—ç»“æ„
        self.parse_cols = ['P_Flag', 'P_Key', 'P_Type', 'P_Check', 'Skufix_Check']
        for i in range(1, 11):
            self.parse_cols.extend([f'P_SKU{i}', f'P_Quantity{i}'])

    def _build_fast_fix_map(self) -> Dict[str, dict]:
        """[ä¼˜åŒ–] æ„å»ºå†…å­˜çº§å¿«é€Ÿä¿®å¤æŸ¥æ‰¾è¡¨"""
        memory_dict = {}
        if not self.corrector.memory_df.empty:
            for _, row in self.corrector.memory_df.iterrows():
                bad = str(row.get('BadSKU', '')).strip().upper()
                if bad:
                    memory_dict[bad] = {
                        'sku': str(row.get('CorrectSKU', '')).strip().upper(),
                        'qty': str(row.get('CorrectQty', '')).strip()
                    }
        return memory_dict

    def run(self, date_range: tuple = None) -> Dict[str, Any]:
        """
        [ä¸»å…¥å£] æ‰§è¡Œè§£ææµç¨‹ (V2.3 - æ—¥æœŸåŒºé—´å¢é‡å¤„ç†)
        
        Args:
            date_range: (date_min, date_max) æ—¥æœŸèŒƒå›´ï¼Œå¦‚ ('2025-01-01', '2025-01-31')
                        å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨ Processed_T=0 ä½œä¸ºç­›é€‰æ¡ä»¶
        """
        self.logger.info("ğŸš€ [Parser] å¼€å§‹è§£æäº¤æ˜“æ•°æ®...")
        
        # 1. æ ¹æ®æ—¥æœŸèŒƒå›´è¯»å–æ•°æ®
        if date_range and date_range[0] and date_range[1]:
            date_min, date_max = date_range
            self.logger.info(f"ğŸ“… æ—¥æœŸèŒƒå›´: {date_min} ~ {date_max}")
            df_all = self.db.read_df(f"""
                SELECT * FROM Data_Transaction 
                WHERE `Transaction creation date` BETWEEN '{date_min}' AND '{date_max}'
            """)
        else:
            # å…œåº•ï¼šè¯»å–å¾…å¤„ç†çš„æ•°æ®
            df_all = self.db.read_df("""
                SELECT * FROM Data_Transaction 
                WHERE COALESCE(Processed_T, 0) = 0
            """)
        
        if df_all.empty:
            self.logger.info("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ•°æ®")
            return {"status": "empty", "auto_fixed": [], "pending_count": 0}

        self.logger.info(f"è¯»å–åˆ° {len(df_all)} æ¡è®°å½•")

        # 2. ç­›é€‰å¾…å¤„ç†è®°å½• (Processed_T=0 æˆ– NULL)
        mask_pending = df_all['Processed_T'].isna() | (df_all['Processed_T'].astype(str) == '0')
        df_pending = df_all[mask_pending].copy()
        
        if df_pending.empty:
            self.logger.info("æ²¡æœ‰å¾…å¤„ç†çš„äº¤æ˜“æ•°æ®ï¼Œè·³è¿‡è§£æã€‚")
            return {"status": "empty", "auto_fixed": [], "pending_count": 0, "df_all": df_all}

        self.logger.info(f"å‘ç° {len(df_pending)} æ¡å¾…å¤„ç†è®°å½• (æ€» {len(df_all)} æ¡)")
        df_pending = df_pending.reset_index(drop=True)

        # 3. åˆå§‹åŒ–åˆ—ç»“æ„
        df_pending = self._init_columns(df_pending)

        # 4. æ­£åˆ™è§£æ (é«˜æ€§èƒ½å‘é‡åŒ–)
        df_pending = self._apply_regex_patterns(df_pending)

        # 5. å¤æ‚è§£æ (è¿­ä»£å¤„ç†å…œåº•)
        df_pending = self._process_complex_rows(df_pending)

        # 6. æ ¡éªŒä¸è‡ªåŠ¨ä¿®å¤
        validation_result = self._validate_and_autofix(df_pending)
        df_parsed = validation_result["df"]

        # 7. åœ¨å†…å­˜ä¸­åˆå¹¶ï¼šç”¨è§£æç»“æœæ›´æ–°åŸè¡¨
        self.logger.info("ğŸ’¾ åˆå¹¶è§£æç»“æœ...")
        
        update_cols = ['P_Flag', 'P_Key', 'P_Type', 'P_Check', 'Skufix_Check']
        for i in range(1, 11):
            update_cols.extend([f'P_SKU{i}', f'P_Quantity{i}'])
        
        for col in update_cols:
            if col not in df_all.columns:
                df_all[col] = None
        
        df_all = df_all.set_index('row_hash')
        df_parsed = df_parsed.set_index('row_hash')
        
        for col in update_cols:
            if col in df_parsed.columns:
                df_all.loc[df_parsed.index, col] = df_parsed[col]
        
        # [V2.3] åœ¨å†…å­˜ä¸­æ ‡è®°ä¸ºå·²å¤„ç† (å¾…å†™å…¥æ—¶ä¸€èµ·æ›´æ–°)
        df_all.loc[df_parsed.index, 'Processed_T'] = 1
        
        df_all = df_all.reset_index()

        # [V2.3] ä¸å†™æ•°æ®åº“ï¼Œè¿”å›å¤„ç†åçš„ DataFrame ä¾› Transformer ä½¿ç”¨
        self.logger.info(f"âœ… è§£æå®Œæˆ: {len(df_parsed)} æ¡è®°å½•")
        
        return {
            "status": "success",
            "auto_fixed": validation_result["fixed_logs"],
            "pending_count": validation_result["failed_count"],
            "df_trans": df_all,  # è¿”å›å¤„ç†åçš„ DataFrame
            "date_range": date_range,
        }

    def _init_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆå§‹åŒ–å¿…è¦çš„åˆ—"""
        # æ ‡å‡†åŒ–åˆ—å
        col_map = {c.lower().replace(" ", ""): c for c in df.columns}
        if 'customlabel' in col_map:
            df.rename(columns={col_map['customlabel']: 'Custom label'}, inplace=True)
        elif 'Custom label' not in df.columns:
            df['Custom label'] = ''

        # åˆå§‹åŒ– P_* åˆ—
        for col in self.parse_cols:
            if col not in df.columns:
                df[col] = None

        if 'Item title' not in df.columns:
            df['Item title'] = ''

        # ç¡®ä¿æ ‡è®°åˆ—ä¸ºæ•´æ•°ç±»å‹ (fillna 0)
        df['P_Flag'] = df['P_Flag'].fillna(0).astype(int)
        return df

    def _apply_regex_patterns(self, df: pd.DataFrame) -> pd.DataFrame:
        """[Stage 1] å‘é‡åŒ–æ­£åˆ™åŒ¹é…"""
        self.logger.info(" æ‰§è¡Œæ­£åˆ™è§£æ (Vectorized)...")

        s_label = df['Custom label'].astype(str).str.strip()
        mask_todo = (df['P_Flag'] == 0)

        # Pattern 1: Single (e.g., "ABCD-1234.1")
        # å…è®¸å­—ç¬¦: A-Z, 0-9, -, /, _
        pat1 = r'^(?:[A-Za-z]{1}[A-Za-z0-9]{0,2}\.)?(?P<SKU>[A-Za-z0-9\-_/]{7,})\.(?P<Quantity>\d{1,3})(?P<QuantityKey>\+2K)?(?:\.[A-Za-z0-9_]*){0,2}$'
        ext1 = s_label[mask_todo].str.extract(pat1)
        idx1 = ext1[ext1['SKU'].notna()].index

        if not idx1.empty:
            df.loc[idx1, 'P_Flag'] = 1
            df.loc[idx1, 'P_Type'] = 'single'
            df.loc[idx1, 'P_SKU1'] = ext1.loc[idx1, 'SKU']
            df.loc[idx1, 'P_Quantity1'] = ext1.loc[idx1, 'Quantity']
            df.loc[idx1, 'P_Key'] = ext1.loc[idx1, 'QuantityKey'].apply(lambda x: 2 if pd.notna(x) else 0)
            df.loc[idx1, 'Skufix_Check'] = 1

        # Pattern 2: Dual (e.g., "PART1.1+PART2.1")
        # [Fix] ç¨å¾®æ”¾å®½å‰ç¼€åŒ¹é…ï¼Œé˜²æ­¢ ZD. è¿™ç§è¢«æ¼æ‰ ([A-Za-z0-2] -> [A-Za-z0-9])
        mask_todo = (df['P_Flag'] == 0)
        pat2 = r'^(?:[A-Za-z]{1}[A-Za-z0-9]{0,2}\.)?(?P<S1>[A-Za-z0-9/\-_]{7,})\.(?P<Q1>\d{1,3})(?P<K1>\+2K)?[\+\.](?P<S2>[A-Za-z0-9/\-_]{7,})\.(?P<Q2>\d{1,3})(?P<K2>\+2K)?(?:\.[A-Za-z0-9_]*){0,2}$'
        ext2 = s_label[mask_todo].str.extract(pat2)
        idx2 = ext2[ext2['S1'].notna() & ext2['S2'].notna()].index

        if not idx2.empty:
            df.loc[idx2, 'P_Flag'] = 2
            df.loc[idx2, 'P_Type'] = 'dual'
            df.loc[idx2, 'P_SKU1'] = ext2.loc[idx2, 'S1']
            df.loc[idx2, 'P_Quantity1'] = ext2.loc[idx2, 'Q1']
            df.loc[idx2, 'P_SKU2'] = ext2.loc[idx2, 'S2']
            df.loc[idx2, 'P_Quantity2'] = ext2.loc[idx2, 'Q2']
            k1 = ext2.loc[idx2, 'K1'].notna().astype(int) * 2
            k2 = ext2.loc[idx2, 'K2'].notna().astype(int) * 2
            df.loc[idx2, 'P_Key'] = k1 + k2
            df.loc[idx2, 'Skufix_Check'] = 1

        return df

    def _process_complex_rows(self, df: pd.DataFrame) -> pd.DataFrame:
        """[Stage 2] è¿­ä»£è§£æå¤æ‚è¡Œ"""
        mask_complex = (df['P_Flag'] == 0)
        count = mask_complex.sum()
        if count == 0: return df

        self.logger.info(f"ğŸ¢ å¤„ç† {count} æ¡å¤æ‚è®°å½•...")
        junk_chars = {'--', '-', 'N/A', 'NULL', 'NONE', '', 'NAN'}
        # [Fix] æ”¾å®½å‰ç¼€åŒ¹é…
        prefix_pattern = re.compile(r'^(?:[A-Za-z]{1}[A-Za-z0-9]{0,2}\.)?(?P<main>.+?)(?:\.[A-Za-z0-9_]*)?$')

        # [Fix] è½¬æ¢ä¸ºåˆ—è¡¨é¿å…è¿­ä»£æ—¶ç´¢å¼•é—®é¢˜
        complex_indices = df[mask_complex].index.tolist()
        
        for idx in complex_indices:
            # [Fix] å®‰å…¨è·å–å€¼
            label_val = df.loc[idx, 'Custom label']
            if isinstance(label_val, pd.Series):
                label_val = label_val.iloc[0] if len(label_val) > 0 else ""
            raw_label = str(label_val).strip()
            
            match = prefix_pattern.match(raw_label)
            main_part = match.group('main') if match else raw_label

            parts = main_part.split('+')
            p_key = 0
            p_skus = []
            p_qtys = []
            valid_parse = False

            for seg in parts:
                seg = seg.strip()
                if not seg or seg.upper() in junk_chars: continue
                if seg.upper() == '2K':
                    p_key += 2
                    continue
                if '+2K' in seg:
                    p_key += 2
                    seg = seg.replace('+2K', '')

                arr = seg.split('.')
                code = arr[0].upper().strip()
                qty = arr[1] if len(arr) > 1 else '1'

                if code in junk_chars: continue

                # å¿«é€ŸæŸ¥è¡¨ä¿®æ­£ (Fast Fix)
                if code not in self.corrector.valid_skus and code in self.fix_map:
                    code = self.fix_map[code]['sku']

                p_skus.append(code)
                p_qtys.append(qty)
                valid_parse = True

            if valid_parse:
                limit = min(len(p_skus), 10)
                for i in range(limit):
                    df.loc[idx, f'P_SKU{i + 1}'] = p_skus[i]
                    df.loc[idx, f'P_Quantity{i + 1}'] = p_qtys[i]

                df.loc[idx, 'P_Flag'] = 5
                df.loc[idx, 'P_Key'] = p_key
                df.loc[idx, 'P_Check'] = 1
                df.loc[idx, 'Skufix_Check'] = 1

        return df

    def _validate_and_autofix(self, df: pd.DataFrame) -> Dict[str, Any]:
        """[Stage 3] æ ¡éªŒä¸è‡ªåŠ¨ä¿®å¤"""
        self.logger.info("ğŸ” æ‰§è¡Œ SKU æ ¡éªŒä¸è‡ªåŠ¨ä¿®å¤...")

        # [Critical Fix]
        # ä¹‹å‰é€»è¾‘: mask_check = (df['P_Flag'] > 0) & (df['P_Flag'] != 99)
        # é—®é¢˜: 99 è¡¨ç¤ºä¸Šæ¬¡æ ¡éªŒå¤±è´¥ã€‚å¦‚æœç”¨æˆ·å» DB åŠ äº† SKUï¼Œä¸‹æ¬¡è·‘å¿…é¡»é‡æ£€ 99 çš„è¡Œï¼Œå¦åˆ™æ°¸è¿œæ˜¯ 99ã€‚
        # ä¿®æ­£: åªè¦æœ‰è§£æç»“æœ (P_Flag > 0)ï¼Œå°±å¿…é¡»é‡æ–°æ ¡éªŒã€‚
        mask_check = (df['P_Flag'] > 0)

        fix_logs = []
        count_failed = 0

        # [Fix] ä½¿ç”¨ iterrows éå†ï¼Œé¿å… df.at è¿”å› Series çš„é—®é¢˜
        check_indices = df[mask_check].index.tolist()
        
        for idx in check_indices:
            is_row_valid = True
            
            # å®‰å…¨è·å–æ ‡é‡å€¼
            custom_label = str(df.loc[idx, 'Custom label']) if 'Custom label' in df.columns else ""
            order_num = str(df.loc[idx, 'Order number']) if 'Order number' in df.columns else ""

            for i in range(1, 11):
                sku_col = f'P_SKU{i}'
                qty_col = f'P_Quantity{i}'
                
                # å®‰å…¨è·å– SKU å€¼
                sku_val = df.loc[idx, sku_col] if sku_col in df.columns else None
                
                # [Fix] ç¡®ä¿æ˜¯æ ‡é‡å€¼
                if isinstance(sku_val, pd.Series):
                    sku_val = sku_val.iloc[0] if len(sku_val) > 0 else None

                if pd.isna(sku_val) or str(sku_val).strip() == "": 
                    continue
                    
                sku = str(sku_val).strip().upper()

                # 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                if self.corrector.is_valid_sku(sku):
                    continue

                # 2. å°è¯•è‡ªåŠ¨ä¿®å¤ (Memory Recall)
                fixed_sku, fixed_qty = self.corrector.find_auto_fix(custom_label, sku)

                if fixed_sku:
                    # å®‰å…¨è·å–æ•°é‡å€¼
                    qty_val = df.loc[idx, qty_col] if qty_col in df.columns else None
                    if isinstance(qty_val, pd.Series):
                        qty_val = qty_val.iloc[0] if len(qty_val) > 0 else ""
                    old_qty_val = str(qty_val).strip() if pd.notna(qty_val) else ""
                    
                    df.loc[idx, sku_col] = fixed_sku
                    new_qty_val = old_qty_val
                    if fixed_qty and str(fixed_qty).strip():
                        df.loc[idx, qty_col] = fixed_qty
                        new_qty_val = str(fixed_qty).strip()
                    fix_logs.append({
                        "order": order_num,
                        "old": sku,
                        "new": fixed_sku,
                        "old_qty": old_qty_val,
                        "new_qty": new_qty_val,
                        "custom_label": custom_label
                    })
                else:
                    is_row_valid = False

            if not is_row_valid:
                df.loc[idx, 'P_Flag'] = 99  # æ ‡è®°ä¸ºå¼‚å¸¸
                count_failed += 1
            else:
                # [Fix] å¦‚æœæ ¡éªŒé€šè¿‡äº†ï¼Œä¸”ä¹‹å‰æ˜¯ 99ï¼Œè¦æ¢å¤æˆæ­£å¸¸çŠ¶æ€ (å¦‚ 5)
                current_flag = df.loc[idx, 'P_Flag']
                if isinstance(current_flag, pd.Series):
                    current_flag = current_flag.iloc[0] if len(current_flag) > 0 else 0
                if current_flag == 99:
                    df.loc[idx, 'P_Flag'] = 5

        self.logger.info(f"æ ¡éªŒç»“æœ: è‡ªåŠ¨ä¿®å¤ {len(fix_logs)} é¡¹ï¼Œå‰©ä½™ {count_failed} è¡Œå¼‚å¸¸ã€‚")
        return {"fixed_logs": fix_logs, "failed_count": count_failed, "df": df}