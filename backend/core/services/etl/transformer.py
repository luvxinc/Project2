# core/services/etl/transformer.py
"""
æ–‡ä»¶è¯´æ˜: äº¤æ˜“æ•°æ®è½¬æ¢å¼•æ“ (Transformer) - Date Normalized
ä¸»è¦åŠŸèƒ½:
1. å°† Raw Data è½¬æ¢ä¸º Clean Dataã€‚
2. ä¸šåŠ¡é€»è¾‘è®¡ç®— (Action/Seller/Fee Proration)ã€‚
3. [Fix] å¼ºåˆ¶æ—¥æœŸæ ¼å¼åŒ–: æ— è®ºåŸå§‹æ ¼å¼å¦‚ä½•ï¼Œå…¥åº“å‰ç»Ÿä¸€è½¬ä¸º 'YYYY-MM-DD'ã€‚
4. å››ç»´å»é‡å¹¶å¢é‡å†™å…¥ã€‚
"""

import pandas as pd
import numpy as np
from sqlalchemy import text
from sqlalchemy.types import Text
from typing import Callable, Optional
from dateutil import parser  # [New] å¼•å…¥å¼ºåŠ›è§£æå™¨

from core.components.db.client import DBClient
from core.sys.logger import get_logger


class TransactionTransformer:

    def __init__(self):
        self.db = DBClient()
        self.logger = get_logger("TransactionTransformer")

        self.output_cols = [
            'order date', 'seller', 'order number', 'item id', 'item title', 'full sku', 'quantity',
            'revenue', 'Shipping and handling', 'Seller collected tax', 'eBay collected tax',
            'Final Value Fee - fixed', 'Final Value Fee - variable', 'Regulatory operating fee',
            'International fee', 'Promoted Listings fee', 'Payments dispute fee',
            'action', 'Refund',
            'Shipping label-Earning data', 'Shipping label-Regular',
            'Shipping label-underpay', 'Shipping label-overpay', 'Shipping label-Return',
            'buyer username', 'ship to city', 'ship to country'
        ]
        for i in range(1, 11):
            self.output_cols.extend([f'sku{i}', f'qty{i}', f'qtyp{i}'])

    def _safe_float(self, series: pd.Series) -> pd.Series:
        if series.empty: return series
        clean = series.astype(str).str.replace(r'[$,\s]', '', regex=True)
        return pd.to_numeric(clean, errors='coerce').fillna(0.0)

    def _normalize_date(self, date_val) -> str:
        """
        [New] å¼ºåŠ›æ—¥æœŸè§£æ
        Input: 'Jun 30, 2025', '2025-06-30', '30-Jun-25'
        Output: '2025-06-30'
        """
        if pd.isna(date_val) or str(date_val).strip() == "":
            return None

        s = str(date_val).strip()
        try:
            # ä¼˜å…ˆå°è¯• pandas è‡ªåŠ¨æ¨æ–­
            dt = pd.to_datetime(s, errors='raise')
            return dt.strftime('%Y-%m-%d')
        except:
            try:
                # å°è¯• dateutil (æ›´æ™ºèƒ½)
                dt = parser.parse(s)
                return dt.strftime('%Y-%m-%d')
            except:
                # è§£æå¤±è´¥ï¼Œè¿”å›åŸå€¼ä»¥ä¾¿æ’æŸ¥
                return s

    def run(self, progress_callback: Optional[Callable[[float, str], None]] = None,
            return_ratios: dict = None,
            df_trans_input: pd.DataFrame = None,
            date_range: tuple = None) -> dict:
        """
        [ä¸»å…¥å£] æ‰§è¡Œ ETL è½¬æ¢æµç¨‹ (V2.3 - æ—¥æœŸåŒºé—´å¢é‡å¤„ç†)
        
        Args:
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            return_ratios: FIFO å›åº“æ¯”ä¾‹é…ç½® {'RE': 0.6, 'CR': 0.5, 'CC': 0.3}
            df_trans_input: ä» Parser ä¼ å…¥çš„ DataFrame (å¯é€‰)
            date_range: (date_min, date_max) æ—¥æœŸèŒƒå›´ (å¯é€‰)
        """

        def report(p, msg):
            if progress_callback: progress_callback(p, msg)
            self.logger.info(msg)

        try:
            report(0.05, "ğŸš€ [Transformer] å¯åŠ¨è½¬æ¢å¼•æ“ (å¢é‡æ¨¡å¼ V2.3)...")

            # =================================================================
            # [V2.3] æ—¥æœŸåŒºé—´å¢é‡å¤„ç†
            # 1. ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ df_transï¼Œå¦åˆ™æ ¹æ®æ—¥æœŸèŒƒå›´è¯»å–
            # 2. åªå¤„ç†è¯¥æ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
            # 3. åœ¨å†…å­˜ä¸­å®Œæˆæ‰€æœ‰å¤„ç†
            # 4. æœ€åä¸€æ¬¡æ€§å†™å…¥æ•°æ®åº“
            # =================================================================
            
            # Step 1: è·å– Transaction æ•°æ®
            if df_trans_input is not None:
                df_trans = df_trans_input.copy()
                self.logger.info(f"ä½¿ç”¨ä¼ å…¥çš„ DataFrame: {len(df_trans)} æ¡")
            elif date_range and date_range[0] and date_range[1]:
                date_min, date_max = date_range
                self.logger.info(f"ğŸ“… æ—¥æœŸèŒƒå›´: {date_min} ~ {date_max}")
                df_trans = self.db.read_df(f"""
                    SELECT * FROM Data_Transaction 
                    WHERE `Transaction creation date` BETWEEN '{date_min}' AND '{date_max}'
                """)
            else:
                # å…œåº•ï¼šè¯»å–å¾…å¤„ç†çš„æ•°æ®
                df_trans = self.db.read_df("""
                    SELECT * FROM Data_Transaction 
                    WHERE COALESCE(Processed_T, 0) = 0
                """)
            
            if df_trans.empty:
                return {"status": "empty", "message": "Transaction æ•°æ®ä¸ºç©º"}
            
            # Step 2: è·å–è®¢å•å·åˆ—è¡¨
            df_trans.columns = df_trans.columns.str.strip().str.lower()
            pending_orders = df_trans['order number'].dropna().unique().tolist()
            self.logger.info(f"å‘ç° {len(pending_orders)} ä¸ªè®¢å•")
            
            order_placeholders = ', '.join([f"'{o}'" for o in pending_orders])
            
            # Step 3: è·å– Earning æ•°æ®
            df_earn = self.db.read_df(f"""
                SELECT * FROM Data_Order_Earning 
                WHERE `Order number` IN ({order_placeholders})
            """)
            
            # ä¿å­˜éœ€è¦æ›´æ–°æ ‡è®°çš„ row_hash
            trans_hashes_to_update = df_trans[df_trans['processed_t'].isna() | (df_trans['processed_t'].astype(str) == '0')]['row_hash'].tolist() if 'row_hash' in df_trans.columns and 'processed_t' in df_trans.columns else []
            
            earn_hashes_to_update = self.db.read_df(f"""
                SELECT row_hash FROM Data_Order_Earning 
                WHERE `Order number` IN ({order_placeholders})
                  AND COALESCE(Processed_E, 0) = 0
            """)['row_hash'].tolist() if not df_earn.empty else []
            
            self.logger.info(f"å¾…å¤„ç†: Transaction={len(df_trans)}, Earning={len(df_earn)}")
            self.logger.info(f"å¾…æ ‡è®°: Trans_hash={len(trans_hashes_to_update)}, Earn_hash={len(earn_hashes_to_update)}")

            df_earn.columns = df_earn.columns.str.strip().str.lower() if not df_earn.empty else df_earn.columns

            # --- æ•°å€¼æ¸…æ´— ---
            report(0.15, "ğŸ§¹ æ‰§è¡Œæ•°å€¼æ¸…æ´—...")
            num_cols = [
                'item subtotal', 'quantity', 'gross transaction amount',
                'shipping and handling', 'seller collected tax', 'ebay collected tax',
                'final value fee - fixed', 'final value fee - variable', 'regulatory operating fee',
                'international fee', 'promoted listings fee', 'payments dispute fee', 'refund'
            ]
            for c in num_cols:
                if c in df_trans.columns:
                    df_trans[c] = self._safe_float(df_trans[c])

            # Earning è¡¨å¤„ç†
            if not df_earn.empty and 'shipping labels' in df_earn.columns:
                df_earn['shipping labels'] = self._safe_float(df_earn['shipping labels'])
                earn_map = df_earn.groupby('order number')['shipping labels'].sum().reset_index()
                earn_map.rename(columns={'shipping labels': 'Shipping label-Earning data'}, inplace=True)
            else:
                earn_map = pd.DataFrame(columns=['order number', 'Shipping label-Earning data'])

            # --- ä¸šåŠ¡é€»è¾‘ ---
            report(0.30, "ğŸ§  è®¡ç®—ä¸šåŠ¡è§„åˆ™...")
            for col in ['type', 'reference id', 'seller', 'item id']:
                if col not in df_trans.columns: df_trans[col] = ''

            # Action Logic - è¯†åˆ«é€€è´§/å–æ¶ˆç±»å‹
            df_trans['type_lower'] = df_trans['type'].astype(str).str.lower()
            df_trans['ref_lower'] = df_trans['reference id'].astype(str).str.lower()
            df_trans['action_code'] = 'NN'  # é»˜è®¤

            # è¯†åˆ«å„ç±»é€€è´§/å–æ¶ˆ
            mask_pd = df_trans['type_lower'] == 'payment dispute'
            df_trans.loc[mask_pd, 'action_code'] = 'PD'

            mask_claim = df_trans['type_lower'] == 'claim'
            mask_case = mask_claim & df_trans['ref_lower'].str.contains('case', case=False)
            df_trans.loc[mask_case, 'action_code'] = 'CC'

            mask_req = mask_claim & df_trans['ref_lower'].str.contains('request', case=False)
            df_trans.loc[mask_req, 'action_code'] = 'CR'

            mask_refund = df_trans['type_lower'] == 'refund'
            mask_ret = mask_refund & df_trans['ref_lower'].str.contains('return', case=False)
            df_trans.loc[mask_ret, 'action_code'] = 'RE'

            mask_cancel = mask_refund & df_trans['ref_lower'].str.contains('cancel', case=False)
            df_trans.loc[mask_cancel, 'action_code'] = 'CA'

            # æå–é€€è´§/å–æ¶ˆè®°å½• (ç¨åå•ç‹¬å¤„ç†)
            mask_return_action = df_trans['action_code'].isin(['CA', 'RE', 'CR', 'CC'])
            df_returns_raw = df_trans[mask_return_action][['order number', 'action_code', 'transaction creation date']].copy()
            df_returns_raw = df_returns_raw.drop_duplicates('order number')  # æ¯ä¸ªè®¢å•ä¸€æ¡é€€è´§è®°å½•

            # Seller Logic
            df_trans['seller_clean'] = df_trans['seller'].astype(str).str.strip().str.replace(r'[\'\"]', '', regex=True)
            df_trans['is_prio'] = df_trans['seller_clean'].str.lower().str.contains('esparts').astype(int)
            seller_map = \
            df_trans.sort_values(['is_prio', 'seller_clean'], ascending=[False, True]).drop_duplicates('order number')[
                ['order number', 'seller_clean']]
            seller_map.rename(columns={'seller_clean': 'seller'}, inplace=True)

            # --- ç‰©æµè´¹ç”¨æå– ---
            report(0.50, "ğŸšš æå–éšæ€§ç‰©æµæˆæœ¬...")
            mask_ship = df_trans['type_lower'] == 'shipping label'
            df_ship = df_trans[mask_ship].copy()
            if 'description' not in df_ship.columns: df_ship['description'] = ''

            df_ship['desc_lower'] = df_ship['description'].astype(str).str.lower()
            df_ship['amt'] = df_ship['gross transaction amount']

            df_ship['underpay'] = np.where(df_ship['desc_lower'].str.contains('underpaid'), df_ship['amt'], 0.0)
            df_ship['overpay'] = np.where(df_ship['desc_lower'].str.contains('overpaid'), df_ship['amt'], 0.0)
            df_ship['return'] = np.where(df_ship['desc_lower'].str.contains('return shipping'), df_ship['amt'], 0.0)
            df_ship['regular'] = np.where(~df_ship['desc_lower'].str.contains('underpaid|overpaid|return|voided|bulk'),
                                          df_ship['amt'], 0.0)

            ship_agg = df_ship.groupby('order number')[['underpay', 'overpay', 'return', 'regular']].sum().reset_index()
            ship_agg.columns = ['order number', 'Shipping label-underpay', 'Shipping label-overpay',
                                'Shipping label-Return', 'Shipping label-Regular']

            # --- ä¸»è¡¨æ„å»º (æ‰€æœ‰ Order éƒ½æ˜¯ NN) ---
            report(0.70, "ğŸ§® è®¢å•çº§è´¹ç”¨åˆ†æ‘Š...")
            mask_order = (df_trans['type_lower'] == 'order') & (df_trans['item id'].notna())
            df_main = df_trans[mask_order].copy()
            df_main['action'] = 'NN'  # æ‰€æœ‰ Order è®°å½•éƒ½æ˜¯ NN

            for c in ['seller']:
                if c in df_main.columns: df_main.drop(columns=[c], inplace=True)

            df_main = df_main.merge(seller_map, on='order number', how='left')
            df_main = df_main.merge(earn_map, on='order number', how='left')
            df_main = df_main.merge(ship_agg, on='order number', how='left')
            df_main.fillna(0, inplace=True)
            
            # === ç”Ÿæˆé€€è´§/å–æ¶ˆè®°å½• (CA/RE/CR/CC) ===
            if not df_returns_raw.empty:
                self.logger.info(f"æ£€æµ‹åˆ° {len(df_returns_raw)} æ¡é€€è´§/å–æ¶ˆè®¢å•")
                return_records = []
                for _, ret_row in df_returns_raw.iterrows():
                    order_num = ret_row['order number']
                    action_code = ret_row['action_code']
                    ret_date = ret_row['transaction creation date']
                    
                    # æ‰¾å¯¹åº”çš„ NN è®°å½•
                    nn_rows = df_main[df_main['order number'] == order_num]
                    if nn_rows.empty:
                        continue
                    
                    # å¤åˆ¶æ¯ä¸ª item çš„ NN è®°å½•ï¼Œæ”¹ action
                    for _, nn_row in nn_rows.iterrows():
                        ret_record = nn_row.to_dict()
                        ret_record['action'] = action_code
                        if pd.notna(ret_date):
                            ret_record['transaction creation date'] = ret_date
                        return_records.append(ret_record)
                
                if return_records:
                    df_returns_final = pd.DataFrame(return_records)
                    df_main = pd.concat([df_main, df_returns_final], ignore_index=True)
                    self.logger.info(f"å·²æ·»åŠ  {len(return_records)} æ¡é€€è´§/å–æ¶ˆè®°å½•")

            # åˆ†æ‘Š
            order_totals = df_main.groupby('order number')['item subtotal'].transform('sum')
            df_main['ratio'] = np.where(order_totals != 0, df_main['item subtotal'] / order_totals, 0.0)

            for col in ['Shipping label-Earning data', 'Shipping label-underpay', 'Shipping label-overpay',
                        'Shipping label-Return', 'Shipping label-Regular']:
                if col in df_main.columns: df_main[col] = df_main[col] * df_main['ratio']

            col_mapping = {
                'transaction creation date': 'order date',
                'item subtotal': 'revenue',
                'shipping and handling': 'Shipping and handling',
                'seller collected tax': 'Seller collected tax',
                'ebay collected tax': 'eBay collected tax',
                'final value fee - fixed': 'Final Value Fee - fixed',
                'final value fee - variable': 'Final Value Fee - variable',
                'regulatory operating fee': 'Regulatory operating fee',
                'international fee': 'International fee',
                'promoted listings fee': 'Promoted Listings fee'
            }
            df_main.rename(columns=col_mapping, inplace=True)

            # SKU å±•å¹³
            sku_parts = []
            for i in range(1, 11):
                s_col, q_col = f'p_sku{i}', f'p_quantity{i}'
                target_s, target_q, target_qp = f'sku{i}', f'qty{i}', f'qtyp{i}'

                if s_col not in df_main.columns:
                    df_main[target_s] = '';
                    df_main[target_q] = 0;
                    df_main[target_qp] = 0;
                    continue

                df_main[target_s] = df_main[s_col]
                df_main[target_q] = self._safe_float(df_main[q_col])
                df_main[target_qp] = df_main[target_q] * df_main['quantity']

                mask = df_main[target_s].notna() & (df_main[target_s] != '')
                part = df_main.loc[mask, target_s].astype(str) + "." + df_main.loc[mask, target_q].astype(int).astype(
                    str)
                sku_parts.append(part)

            if sku_parts:
                df_parts = pd.concat(sku_parts, axis=1)
                df_main['full sku'] = df_parts.apply(lambda x: "+".join(x.dropna()), axis=1)
            else:
                df_main['full sku'] = ''

            # [Mod] å¼ºåˆ¶æ—¥æœŸæ ‡å‡†åŒ– (YYYY-MM-DD)
            df_main['order date'] = df_main['order date'].apply(self._normalize_date)

            # --- å…¥åº“ ---
            report(0.90, f"ğŸ’¾ å››ç»´å»é‡å¹¶åŒæ­¥ ({len(df_main)} è¡Œ)...")

            df_final = pd.DataFrame()
            for c in self.output_cols:
                if c in df_main.columns:
                    df_final[c] = df_main[c]
                else:
                    df_final[c] = 0 if 'fee' in c.lower() or 'label' in c.lower() else ''

            staging = "Data_Clean_Log_Staging"
            target = "Data_Clean_Log"
            
            # ç»Ÿè®¡: æœ¬æ¬¡æ•°æ®
            data_count = len(df_final)
            dedup_count = 0

            with self.db.atomic_transaction() as conn:
                df_final.to_sql(staging, conn, if_exists='replace', index=False,
                                dtype={c: Text() for c in df_final.columns})
                
                # [Fix] ç¡®ä¿ Staging è¡¨ collation ä¸ç›®æ ‡è¡¨ä¸€è‡´ï¼Œé¿å… JOIN æ—¶æŠ¥é”™
                conn.execute(text(f"ALTER TABLE `{staging}` CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci"))

                conn.execute(text(f"CREATE INDEX idx_order ON `{staging}` (`order number`(30))"))
                conn.execute(text(f"CREATE INDEX idx_item ON `{staging}` (`item id`(30))"))
                conn.execute(text(f"CREATE INDEX idx_date ON `{staging}` (`order date`(10))"))

                exists = conn.execute(text(f"SHOW TABLES LIKE '{target}'")).first()
                if not exists:
                    conn.execute(text(f"RENAME TABLE `{staging}` TO `{target}`"))
                else:
                    # ç»Ÿè®¡ staging è¡¨ä¸­æœ‰å¤šå°‘æ¡è®°å½•åœ¨ target è¡¨ä¸­å·²å­˜åœ¨ï¼ˆå³é‡å¤çš„ï¼Œéœ€è¦è¦†ç›–çš„ï¼‰
                    # è¿™äº›è®°å½•ä¸ç®—"æ–°ä¸Šä¼ "ï¼Œåªæ˜¯è¦†ç›–æ—§æ•°æ®
                    dedup_result = conn.execute(text(f"""
                        SELECT COUNT(*) as cnt FROM `{staging}` T2
                        WHERE EXISTS (
                            SELECT 1 FROM `{target}` T1
                            WHERE T1.`order number` = T2.`order number`
                            AND T1.`seller` = T2.`seller`
                            AND COALESCE(T1.`item id`, '') = COALESCE(T2.`item id`, '')
                            AND COALESCE(T1.`action`, '') = COALESCE(T2.`action`, '')
                        )
                    """)).first()
                    dedup_count = dedup_result[0] if dedup_result else 0
                    
                    del_sql = f"""
                        DELETE T1 FROM `{target}` T1 
                        INNER JOIN `{staging}` T2 
                        ON T1.`order number` = T2.`order number`
                        AND T1.`seller` = T2.`seller`
                        AND COALESCE(T1.`item id`, '') = COALESCE(T2.`item id`, '')
                        AND COALESCE(T1.`action`, '') = COALESCE(T2.`action`, '')
                    """
                    conn.execute(text(del_sql))

                    cols = ", ".join([f"`{c}`" for c in self.output_cols])
                    ins_sql = f"INSERT INTO `{target}` ({cols}) SELECT {cols} FROM `{staging}`"
                    conn.execute(text(ins_sql))
                    conn.execute(text(f"DROP TABLE `{staging}`"))

                # [V2.3] å†™å…¥ Transaction è¡¨ (df_trans å·²åŒ…å« Processed_T=1 æ ‡è®°)
                if df_trans_input is not None:
                    from sqlalchemy.types import Text as SAText
                    
                    # è·å–åŸå§‹è¡¨çš„åˆ—å
                    orig_cols = self.db.read_df("SELECT * FROM Data_Transaction LIMIT 0").columns.tolist()
                    
                    # ä½¿ç”¨æ—¥æœŸèŒƒå›´åˆ é™¤æ—§æ•°æ®
                    if date_range and date_range[0] and date_range[1]:
                        conn.execute(text(f"""
                            DELETE FROM `Data_Transaction` 
                            WHERE `Transaction creation date` BETWEEN '{date_range[0]}' AND '{date_range[1]}'
                        """))
                        self.logger.info(f"å·²åˆ é™¤ Transaction æ—¥æœŸèŒƒå›´ {date_range[0]} ~ {date_range[1]} çš„æ—§æ•°æ®")
                    
                    # æ¢å¤åˆ—åï¼šå»ºç«‹å°å†™åˆ°åŸå§‹çš„æ˜ å°„
                    col_map = {c.lower(): c for c in orig_cols}
                    df_trans.columns = [col_map.get(c.lower(), c) for c in df_trans.columns]
                    
                    # åªä¿ç•™åŸè¡¨å­˜åœ¨çš„åˆ—
                    cols_to_keep = [c for c in df_trans.columns if c in orig_cols]
                    df_trans = df_trans[cols_to_keep]
                    
                    # æ’å…¥æ–°æ•°æ®
                    dtype_map = {c: SAText() for c in df_trans.columns}
                    df_trans.to_sql('Data_Transaction', conn, if_exists='append', index=False, chunksize=2000, dtype=dtype_map)
                    self.logger.info(f"å·²å†™å…¥ {len(df_trans)} æ¡ Transaction è®°å½•")
                
                # [V2.3] æ›´æ–° Earning è¡¨çš„æ ‡è®° (ä»éœ€ UPDATEï¼Œå› ä¸ºæ²¡æœ‰ä¼ å…¥ df_earn)
                if earn_hashes_to_update:
                    hash_df = pd.DataFrame({'row_hash': earn_hashes_to_update})
                    hash_df.to_sql('_tmp_earn_hashes', conn, if_exists='replace', index=False)
                    conn.execute(text("""
                        UPDATE `Data_Order_Earning` E
                        INNER JOIN `_tmp_earn_hashes` H ON E.`row_hash` = H.`row_hash`
                        SET E.`Processed_E` = 1
                    """))
                    conn.execute(text("DROP TABLE IF EXISTS `_tmp_earn_hashes`"))
                    self.logger.info(f"å·²æ ‡è®° {len(earn_hashes_to_update)} æ¡ Earning è®°å½•ä¸ºå·²å¤„ç†")

            report(0.90, f"ğŸ“¦ åŒæ­¥ FIFO åº“å­˜ ({len(df_final)} æ¡)...")
            
            # [V2.3] åˆ›å»º FIFO åŒæ­¥è¿›åº¦å›è°ƒ (æ˜ å°„åˆ° 90%-99%)
            def fifo_progress(current, total, msg):
                if total > 0:
                    pct = 0.90 + (current / total) * 0.09  # 90% -> 99%
                    if progress_callback:
                        progress_callback(pct, f"ğŸ“¦ FIFO åŒæ­¥: {current}/{total} ({msg})")
            
            # [V2.0] è°ƒç”¨ FIFO åŒæ­¥æœåŠ¡
            fifo_stats = self._sync_fifo(df_final, return_ratios, progress_callback=fifo_progress)
            
            report(1.0, f"âœ… ETL å®Œæˆ (FIFO: {fifo_stats['out_count']} å‡ºåº“, {fifo_stats['in_count']} å›åº“)")
            
            # è¿”å›ç»Ÿè®¡ä¿¡æ¯
            return {
                'status': 'success',
                'data_count': data_count,
                'dedup_count': dedup_count,
                'actual_upload': data_count - dedup_count,
                'fifo_stats': fifo_stats,
            }

        except Exception as e:
            self.logger.error(f"Transformer Error: {e}")
            raise e

    def _sync_fifo(self, df: pd.DataFrame, return_ratios: dict = None, progress_callback=None) -> dict:
        """
        [V2.0] åŒæ­¥é”€å”®æ•°æ®åˆ° FIFO ç³»ç»Ÿ
        """
        try:
            from core.services.fifo.sales_sync import SalesFifoSyncService
            
            fifo_service = SalesFifoSyncService(return_ratios=return_ratios)
            return fifo_service.sync_from_sales(df, progress_callback=progress_callback)
        except Exception as e:
            self.logger.error(f"FIFO åŒæ­¥å¤±è´¥: {e}")
            return {"out_count": 0, "in_count": 0, "skip_count": 0, "error_count": 1}