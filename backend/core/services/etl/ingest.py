# core/services/etl/ingest.py
"""
æ–‡ä»¶è¯´æ˜Ž: æ•°æ®æ‘„å…¥æœåŠ¡ (Ingest Service)
ä¸»è¦åŠŸèƒ½:
1. è¯»å– Transaction å’Œ Earning çš„ CSV æ–‡ä»¶ã€‚
2. æ™ºèƒ½è¯†åˆ« Seller (é€šè¿‡æ–‡ä»¶å†…å®¹æˆ–æ–‡ä»¶å) å’Œ Header è¡Œã€‚
3. **æ•°æ®æ¸…æ´—**: å…³é”®ä¿®å¤ - å¯¹æ‰€æœ‰å­—ç¬¦ä¸²åˆ—æ‰§è¡Œ strip()ï¼ŒåŽ»é™¤éšå½¢ç©ºæ ¼ã€‚
4. **æ•°æ®å…¥åº“**: ä½¿ç”¨ Hash åŽ»é‡ + APPEND æ¨¡å¼ï¼Œä¿ç•™åŽ†å²æ•°æ®ã€‚

[V2.0 å˜æ›´ - 2026-01-12]
- ç§»é™¤ truncate_raw_tablesï¼Œæ”¹ä¸ºä¿ç•™åŽ†å²æ•°æ®
- ä½¿ç”¨ row_hash è¿›è¡Œæ•´è¡ŒåŽ»é‡
- æ”¹ä¸º APPEND æ¨¡å¼ï¼Œåªè¿½åŠ æ–°æ•°æ®
"""

import pandas as pd
import numpy as np
import csv
import io
import hashlib
from typing import List, Tuple, Optional, Any
from pathlib import Path
from sqlalchemy.types import Text

from core.components.db.client import DBClient
from core.sys.logger import get_logger
from core.services.etl.repository import ETLRepository
from dateutil import parser as dateutil_parser

# æŠ‘åˆ¶ Pandas è­¦å‘Š
pd.set_option('future.no_silent_downcasting', True)


def normalize_date_value(date_val) -> str:
    """
    [V2.1] æ—¥æœŸæ ¼å¼åŒ–
    Input: 'Jun 30, 2025', '2025-06-30', '30-Jun-25', 'Jan-02-2025 03:45:12 PM PST'
    Output: '2025-06-30'
    """
    if pd.isna(date_val) or str(date_val).strip() == "":
        return ""
    
    s = str(date_val).strip()
    try:
        dt = pd.to_datetime(s, errors='raise')
        return dt.strftime('%Y-%m-%d')
    except:
        try:
            dt = dateutil_parser.parse(s, fuzzy=True)
            return dt.strftime('%Y-%m-%d')
        except:
            return s  # è§£æžå¤±è´¥ï¼Œè¿”å›žåŽŸå€¼


def normalize_date_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    [V2.1] å¯¹ DataFrame ä¸­æ‰€æœ‰æ—¥æœŸåˆ—è¿›è¡Œæ ¼å¼åŒ–
    è¯†åˆ«åˆ—ååŒ…å« 'date' çš„åˆ—ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    """
    for col in df.columns:
        if 'date' in str(col).lower():
            df[col] = df[col].apply(normalize_date_value)
    return df


# [V2.1] Data_Order_Earning ç”¨ä¸šåŠ¡é”® hash (é‚®è´¹ä¼šå»¶è¿Ÿæ›´æ–°ï¼Œéœ€è¦è¦†ç›–)
EARNING_HASH_KEY_COLUMNS = [
    'order creation date',
    'order number',
    'item id',
    'item title',
    'buyer name',
    'custom label',
    'seller',
]


def compute_row_hash_full(row: pd.Series) -> str:
    """
    [Data_Transaction ä¸“ç”¨] æ•´è¡Œ hash
    - ä»»ä½•åˆ—å˜åŒ– â†’ hash å˜åŒ– â†’ æ’å…¥æ–°è¡Œ
    """
    values = row.drop('row_hash', errors='ignore')
    content = '|'.join(str(v).strip() for v in values.values)
    return hashlib.md5(content.encode('utf-8')).hexdigest()


def compute_row_hash_key(row: pd.Series) -> str:
    """
    [Data_Order_Earning ä¸“ç”¨] ä¸šåŠ¡é”® hash
    - åªç”¨ä¸å˜åˆ—è®¡ç®— hash
    - é‚®è´¹ç­‰åˆ—å˜åŒ– â†’ hash ä¸å˜ â†’ è¦†ç›–æ—§æ•°æ®
    """
    row_lower = {str(k).lower(): v for k, v in row.items()}
    
    key_values = []
    for col in EARNING_HASH_KEY_COLUMNS:
        val = row_lower.get(col, '')
        key_values.append(str(val).strip())
    
    content = '|'.join(key_values)
    return hashlib.md5(content.encode('utf-8')).hexdigest()


class IngestService:

    def __init__(self):
        self.logger = get_logger("IngestService")
        self.repo = ETLRepository()

    def run_ingest_pipeline(self, transaction_files: List[Any], earning_files: List[Any]) -> dict:
        """
        [ä¸»å…¥å£] æ‰§è¡Œå®Œæ•´çš„æ‘„å…¥æµç¨‹
        
        [V2.3] è¿”å›žç»“æž„ä½“ï¼ŒåŒ…å«æ—¥æœŸèŒƒå›´ä¾›åŽç»­å¤„ç†ä½¿ç”¨
        
        Returns:
            {
                'status': 'success' | 'empty' | 'error',
                'message': str,
                'date_range': (date_min, date_max),  # æœ¬æ¬¡ä¸Šä¼ çš„æ—¥æœŸèŒƒå›´
                'trans_count': int,
                'earn_count': int,
            }
        """
        if not transaction_files and not earning_files:
            return {"status": "empty", "message": "æ²¡æœ‰æ”¶åˆ°ä»»ä½•æ–‡ä»¶"}

        self.logger.info("æ­£åœ¨å¤„ç†æ–‡ä»¶ (Hash åŽ»é‡æ¨¡å¼)...")

        date_min, date_max = None, None
        trans_count, earn_count = 0, 0
        msg = []

        # å¤„ç† Transaction
        if transaction_files:
            result = self._process_files(transaction_files, "Data_Transaction", "Order number")
            trans_count = result['new_count']
            msg.append(f"Transaction: æ–°å¢ž {result['new_count']} æ¡, è·³è¿‡ {result['skip_count']} æ¡")
            if result['date_min'] and result['date_max']:
                date_min = result['date_min']
                date_max = result['date_max']

        # å¤„ç† Earning
        if earning_files:
            result = self._process_files(earning_files, "Data_Order_Earning", "Order number")
            earn_count = result['new_count']
            msg.append(f"Earning: æ–°å¢ž {result['new_count']} æ¡, è¦†ç›– {result['skip_count']} æ¡")
            # Earning çš„æ—¥æœŸèŒƒå›´ä¹Ÿåˆå¹¶
            if result['date_min']:
                date_min = min(date_min, result['date_min']) if date_min else result['date_min']
            if result['date_max']:
                date_max = max(date_max, result['date_max']) if date_max else result['date_max']

        final_msg = " | ".join(msg)
        self.logger.info(f"âœ… æ•°æ®æ‘„å…¥å®Œæˆ: {final_msg}")
        self.logger.info(f"ðŸ“… æ—¥æœŸèŒƒå›´: {date_min} ~ {date_max}")
        
        return {
            "status": "success",
            "message": final_msg,
            "date_range": (date_min, date_max),
            "trans_count": trans_count,
            "earn_count": earn_count,
        }

    def _process_files(self, files: List[Any], table_name: str, key_col: str) -> dict:
        """
        é€šç”¨æ–‡ä»¶å¤„ç†é€»è¾‘
        
        [V2.3] è¿”å›žå­—å…¸ï¼ŒåŒ…å«æ—¥æœŸèŒƒå›´
        
        Returns:
            {'new_count': int, 'skip_count': int, 'date_min': str, 'date_max': str}
        """
        all_chunks = []

        for file_obj in files:
            try:
                # 1. æŽ¢æµ‹å…ƒæ•°æ® (Seller, Skiprows)
                seller, skiprows = self._detect_metadata(file_obj, key_col)

                # 2. å…œåº• Seller
                if not seller and hasattr(file_obj, 'name'):
                    seller = self._infer_seller_from_name(file_obj.name)

                if not seller:
                    name = getattr(file_obj, 'name', 'Unknown')
                    self.logger.warning(f"æ— æ³•è¯†åˆ« Sellerï¼Œè·³è¿‡æ–‡ä»¶: {name}")
                    continue

                # 3. è¯»å–å†…å®¹ (é‡ç½®æŒ‡é’ˆ)
                if hasattr(file_obj, 'seek'): file_obj.seek(0)

                df = pd.read_csv(
                    file_obj,
                    skiprows=skiprows,
                    dtype=str,
                    encoding='utf-8-sig',
                    on_bad_lines='skip'
                )

                if df.empty: continue

                # [å…³é”®ä¿®å¤] å…¨å±€åŽ»ç©ºæ ¼ï¼šå¯¹æ‰€æœ‰ object ç±»åž‹åˆ—æ‰§è¡Œ strip
                df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)

                # 4. æ³¨å…¥å…ƒæ•°æ®ä¸Žæ¸…æ´—
                df["Seller"] = seller
                df.columns = [str(c).strip() for c in df.columns]

                # æ ¡éªŒå…³é”®åˆ—
                if key_col not in df.columns:
                    self.logger.warning(f"æ–‡ä»¶ç»“æž„å¼‚å¸¸ (ç¼ºå¤± {key_col})")
                    continue

                # æ¸…ç†ç©ºå€¼
                df = df.replace(['--', '-', 'N/A', 'null', 'nan', 'None'], np.nan)
                df = df.dropna(how='all')

                # [V2.1] æ—¥æœŸåˆ—æ ¼å¼åŒ– (å†™å…¥å‰ç»Ÿä¸€ä¸º YYYY-MM-DD)
                df = normalize_date_columns(df)

                if not df.empty:
                    all_chunks.append(df)

            except Exception as e:
                import traceback
                name = getattr(file_obj, 'name', 'Unknown')
                self.logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥ ({name}): {e}\n{traceback.format_exc()}")

        if not all_chunks:
            return {'new_count': 0, 'skip_count': 0, 'date_min': None, 'date_max': None}

        # 5. åˆå¹¶æ•°æ®
        try:
            final_df = pd.concat(all_chunks, ignore_index=True)
            
            # [V2.3] æå–æ—¥æœŸèŒƒå›´
            date_col = None
            for col in final_df.columns:
                if 'order' in col.lower() and 'date' in col.lower():
                    date_col = col
                    break
                elif 'transaction' in col.lower() and 'date' in col.lower():
                    date_col = col
                    break
            
            date_min, date_max = None, None
            if date_col and date_col in final_df.columns:
                date_series = pd.to_datetime(final_df[date_col], errors='coerce')
                date_min = date_series.min().strftime('%Y-%m-%d') if pd.notna(date_series.min()) else None
                date_max = date_series.max().strftime('%Y-%m-%d') if pd.notna(date_series.max()) else None
            
            # [V2.1] æ ¹æ®è¡¨åä½¿ç”¨ä¸åŒçš„ hash ç­–ç•¥
            is_earning_table = (table_name == "Data_Order_Earning")
            
            if is_earning_table:
                final_df['row_hash'] = final_df.apply(compute_row_hash_key, axis=1)
            else:
                final_df['row_hash'] = final_df.apply(compute_row_hash_full, axis=1)
            
            # èŽ·å–å·²å­˜åœ¨çš„ hash
            try:
                existing_df = DBClient.read_df(f"SELECT row_hash FROM `{table_name}`")
                existing_hashes = set(existing_df['row_hash'].tolist()) if not existing_df.empty else set()
            except Exception:
                existing_hashes = set()
            
            total_count = len(final_df)
            new_hashes = set(final_df['row_hash'].tolist())
            overlap_hashes = existing_hashes & new_hashes
            pure_new_hashes = new_hashes - existing_hashes
            
            if is_earning_table:
                new_count = len(pure_new_hashes)
                update_count = len(overlap_hashes)
                
                if overlap_hashes:
                    overlap_list = list(overlap_hashes)
                    batch_size = 500
                    for i in range(0, len(overlap_list), batch_size):
                        batch = overlap_list[i:i+batch_size]
                        placeholders = ', '.join([f"'{h}'" for h in batch])
                        DBClient.execute_stmt(f"DELETE FROM `{table_name}` WHERE row_hash IN ({placeholders})")
                    self.logger.info(f"{table_name}: åˆ é™¤ {update_count} æ¡æ—§æ•°æ®å‡†å¤‡è¦†ç›–")
                
                final_df['Processed_E'] = 0
                
                dtype_map = {c: Text() for c in final_df.columns}
                final_df.to_sql(table_name, DBClient.get_engine(), if_exists='append', index=False, chunksize=2000, dtype=dtype_map)
                
                self.logger.info(f"{table_name}: æ–°å¢ž {new_count} æ¡, è¦†ç›– {update_count} æ¡")
                return {'new_count': new_count, 'skip_count': update_count, 'date_min': date_min, 'date_max': date_max}
            
            else:
                df_new = final_df[~final_df['row_hash'].isin(existing_hashes)]
                new_count = len(df_new)
                skip_count = total_count - new_count
                
                if df_new.empty:
                    self.logger.info(f"{table_name}: æ‰€æœ‰ {total_count} æ¡æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    return {'new_count': 0, 'skip_count': skip_count, 'date_min': date_min, 'date_max': date_max}
                
                df_new = df_new.copy()
                df_new['Processed_T'] = 0
                
                dtype_map = {c: Text() for c in df_new.columns}
                df_new.to_sql(table_name, DBClient.get_engine(), if_exists='append', index=False, chunksize=2000, dtype=dtype_map)
                
                self.logger.info(f"{table_name}: æ–°å¢ž {new_count} æ¡, è·³è¿‡ {skip_count} æ¡é‡å¤")
                return {'new_count': new_count, 'skip_count': skip_count, 'date_min': date_min, 'date_max': date_max}
            
        except Exception as e:
            import traceback
            self.logger.error(f"æ•°æ®åº“å†™å…¥å¤±è´¥ ({table_name}): {e}\n{traceback.format_exc()}")
            return {'new_count': 0, 'skip_count': 0, 'date_min': None, 'date_max': None}

    def _detect_metadata(self, file_obj: Any, keyword: str) -> Tuple[Optional[str], int]:
        """æ™ºèƒ½å—…æŽ¢ Seller å’Œ Header è¡Œ"""
        seller = None
        skiprows = 0
        target_key = keyword.lower()

        if hasattr(file_obj, 'seek'): file_obj.seek(0)

        try:
            # å…¼å®¹ Path å’Œ BytesIO
            if isinstance(file_obj, (str, Path)):
                f = open(file_obj, 'r', encoding='utf-8-sig', errors='replace')
                should_close = True
            else:
                # BytesIO éœ€è¦åŒ…è£…
                f = io.TextIOWrapper(file_obj, encoding='utf-8-sig', errors='replace')
                should_close = False

            reader = csv.reader(f)
            lines = []
            for _ in range(30):
                try:
                    lines.append(next(reader))
                except StopIteration:
                    break

            if should_close:
                f.close()
            else:
                f.detach()

            for i, row in enumerate(lines):
                if not row: continue
                clean_row = [str(x).lower().replace('"', '').strip() for x in row]

                # æ‰¾ Seller
                if len(clean_row) >= 2 and clean_row[0] == "seller":
                    if clean_row[1]: seller = clean_row[1]

                # æ‰¾ Header
                if target_key in clean_row:
                    skiprows = i

            return seller, skiprows

        except Exception as e:
            self.logger.warning(f"å…ƒæ•°æ®æŽ¢æµ‹å¼‚å¸¸: {e}")
            return None, 0

    def _infer_seller_from_name(self, filename: str) -> Optional[str]:
        fname = filename.lower()
        if "88" in fname:
            return "esparts88"
        elif "plus" in fname:
            return "espartsplus"
        return None